<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Blog &middot; 안녕지구
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/main.css">
  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.ico?v=2">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">


  <!-- Web Font -->
  <!-- <link href="https://fonts.googleapis.com/css?family=Nanum+Gothic" rel="stylesheet"> -->
  <link href="https://fonts.googleapis.com/css?family=Nanum+Gothic|Nanum+Gothic+Coding|Noto+Sans+KR" rel="stylesheet">

  <!-- Highlighter -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/styles/atom-one-dark.min.css" />
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.11.0/highlight.min.js"></script>
  <script>
    hljs.initHighlightingOnLoad();
    hljs.configure({tabReplace: '    '})
  </script>

  <!-- Custom -->
  <link rel="stylesheet" href="/public/css/custom.css">

  <!-- MathJax -->
  

</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <div class="sidebar-personal-info">
      <!-- 
      <div class="sidebar-personal-info-section">
        <a href="http://gravatar.com/">
          <img src="http://www.gravatar.com/avatar/?s=350" title="View on Gravatar" alt="View on Gravatar" />
        </a>
      </div>
      -->
      <div class="sidebar-personal-info-section">
        <p>안녕하세요. 기록하는 개발자입니다.</p>
      </div>
      
      
      
      <div class="sidebar-personal-info-section">
        <p> Follow me: 
        
        
        
        <a href="https://github.com/HiJiGOO">
          <i class="fa fa-github" aria-hidden="true"></i>
        </a>
        
        |
        
        
        
        <a href="mailto:hi.jigoo@gmail.com">
          <i class="fa fa-envelope" aria-hidden="true"></i>
        </a>
        
        
        
        </p>
      </div>
      
    </div>
  </div>

  <nav class="sidebar-nav">
    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="/">
          Home
        </a>

        
      </span>

    
      
      
      

      

      <span class="foldable">
        <a class="sidebar-nav-item " href="/blog/categories/">
          Categories
        </a>

        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/categories/#algorithm">
                <!-- Algorithm -->
                
                Algorithm&nbsp
                (
                  
                  
                  11
                )
              </a>
          
        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/categories/#bixby">
                <!-- Bixby -->
                
                Bixby&nbsp
                (
                  
                  
                  7
                )
              </a>
          
        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/categories/#NLP">
                <!-- NLP -->
                
                NLP&nbsp
                (
                  
                  
                  0
                )
              </a>
          
        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/categories/#reinforcement-learning">
                <!-- Reinforcement Learning -->
                
                Reinforcement Learning&nbsp
                (
                  
                  
                  4
                )
              </a>
          
        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/categories/#daily">
                <!-- Daily -->
                
                Daily&nbsp
                (
                  
                  
                  0
                )
              </a>
          
        
      </span>

    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="/blog/tags/">
          Tags
        </a>

        
      </span>

    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="/about/">
          About
        </a>

        
      </span>

    

    <!-- <span class="sidebar-nav-item">Currently v1.0.0</span> -->
  </nav>

  <div class="sidebar-item">
    <p>
    &copy; 2019 안녕지구. This work is liscensed under <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a>.
    </p>
  </div>


  <div class="sidebar-item">
    <p>
    Powered by <a href="http://jekyllrb.com">jekyll</a>
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home" title="안녕지구">
              <!-- <img class="masthead-logo" src="/public/logo.png"/> -->
              안녕지구
            </a>
            <small>#developer #bompapa</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/reinforcement%20learning/2019/02/15/rl-dynamic-programming/">
        강화학습 정리 - Dynamic Programming
      </a>
    </h1>

    <span class="post-date">15 Feb 2019</span>
     | 
    
    <a href="/blog/tags/#강화학습" class="post-tag">강화학습</a>
    
    <a href="/blog/tags/#rl" class="post-tag">RL</a>
    
    

    <article>
      <h2 id="4-dynamic-programming">4. Dynamic Programming</h2>

<p><strong><a href="https://en.wikipedia.org/wiki/Dynamic_programming">Dynamic programing (DP)</a></strong> 은 알고리즘 범주 중 하나로, MDP 에서 완벽한 environment 의 model 이 주어졌을 때 optimal policy 를 계산할 수 있다. DP 는 reinforcement learning 에서 위의 조건을 만족하지 못하는 경우가 많기 때문에 사용에는 한계가 있지만, 여전히 이론적으로 매우 중요하다. DP 는 앞으로 다룰 내용들을 이해하는데 꼭 필요한 기반이 된다.</p>

<p>이 챕터에서는 주로 environment 가 <strong>finite MDP</strong> 라고 가정할 것이다. 또한 <strong>dynamics</strong> 가 $p(s^\prime,r \mid s,a)$ 으로 주어진다. $s \in \mathscr{S}, a \in \mathscr{A}(s), r \in \mathscr{R}, s^\prime \in \mathscr{S}^+$ ($\mathscr{S}^+$ 는 episodic 문제에서 terminal state 를 포함)</p>

<p>Reinforcement learning 과 DP 의 핵심 아이디어는 좋은 policy 를 찾기 위해서 value function 을 잘 구조화하는 것이다. 이 챕터에서는 DP 가 value function 을 계산하기 위해서 어떻게 사용하는지 알아볼 것이다. 만약 우리가 <strong>Bellman optimality equation</strong> 을 만족하는 $v_*$ 혹은 $q_*$ 를 알고 있으면 <strong>optimal policy</strong> 는 쉽게 구할 수 있다.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
 	v_*(s) &= \underset{a}{\text{max}} \mathbb{E} \left[ R_{t+1} + \gamma v_*(S_{t+1}) \mid S_t = s, A_t = a \right] \\
	&= \underset{a}{\text{max}} \sum_{s^\prime, r}p(s^\prime, r \mid s, a) \left[ r + \gamma v_*(s^\prime) \right] \tag{4.1}
\end{align*} %]]></script>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
	q_*(s,a) &= \mathbb{E} \left[ R_{t+1} + \gamma \underset{a^\prime}{\text{max}}q_*(S_{t+1}, a^\prime) \mid S_t=s, A_t = a \right] \\
	&= \sum_{s^\prime, r}p(s^\prime, r \mid s, a) \left[ r + \gamma \underset{a^\prime}{\text{max}}q_*(s^\prime, a^\prime)) \right] \tag{4.2}
\end{align*} %]]></script>

<p>앞으로 다루겠지만, DP 알고리즘은 Bellman equation 을 value function 의 근사치를 얻기 위한 업데이트 규칙으로 변환됨으로써 얻어진다.</p>

<div class="message">
(4.1) 식에서 $v_*(s)$ 를 구하기 위해서는 $v_*(s^\prime)$ 가 먼저 업데이트 되어야 합니다. 이 챕터에서는 재귀적인 방법으로 계산하는 것이 아니라, 사전에 미리 구해진 $v_*(s^\prime)$ 의 값을 사용합니다. 결국 반복적으로 업데이트 하면서 원하는 value function 에 근사합니다.
</div>

<hr />

<h3 id="policy-evaluation-prediction">Policy Evaluation (Prediction)</h3>

<p>DP 에서 policy $\pi$ 에 대한 state-value function 을 <strong>policy evaluation</strong> 이라고 한다. 이전 장에서 모든 State s 에 대해 아래와 같이 정의 했었다.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
v_\pi(s) &\doteq  \mathbb{E}_\pi\left[G_t  \mid  S_t=s \right]  \\
			&= \mathbb{E}_\pi \left[R_{t+1} + \gamma G_{t+1}  \mid  S_t=s \right] \tag{from (3.9)}\\
			&= \mathbb{E}_\pi \left[R_{t+1} + \gamma v_{\pi}(S_{t+1})  \mid  S_t=s \right] \tag{4.3}\\
			&= \sum_a \pi(a \mid s) \sum_{s^\prime, r} p(s^\prime,r \mid s,a) \left[ r + \gamma v_\pi(s^\prime) \right], \text{for all } s \in \mathscr{S} \tag{4.4}
			
\end{align*} %]]></script>

<p>$\pi(a \mid s)$ 는 policy $\pi$ 를 따랐을 때 state $s$ 에서 action $a$ 를 선택할 확률이다. 기댓값 $\mathbb{E}$ 에는 $\pi$ 에 따라 정해지기 때문에 $\pi$ 가 붙는다.</p>

<p>만약 environment 의 dynamic 를 완벽하게 알고 있다면, 식 (4.4) 는 선형 연립방정식으로 풀 수 있다. 그러나 우리가 해결하려는 문제에는 반복적인 계산으로 방법이 더 적합하다. 연속적인 approximate value function 을 $v_0, v_1, v_2 …, $ 으로 간주했을 떄, $v_\pi$ 에 대한 Bellman equation 을 사용해서 순차적으로 값을 계산할 수 있다. 여기서 각 $v_k$ 는 final state 를 포함한 모든 State $\mathscr{S}^+$ 에 맵핑 된다.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
v_{k+1}(s) &\doteq \mathbb{E}_\pi \left[R_{t+1} + \gamma v_k(S_{t+1})  \mid  S_t=s \right] \\
			&= \sum_a \pi(a \mid s) \sum_{s^\prime, r} p(s^\prime,r \mid s,a) \left[ r + \gamma v_k(s^\prime) \right], \text{for all } s \in \mathscr{S}, \tag{4.5}
			
\end{align*} %]]></script>

<div class="message">
식 (4.4) 에서 (4.5) 로 넘어오는 식이 매우 중요하다고 생각합니다. 식 (4.5) 를 여러번 계산한다고 생각했을 때, $v_{k+1}$ 을 계산할 때 필요한 $v_k$ 는 이미 이전 시점에서 계산 된 해를 가지고 있습니다. $v_k$ 에서 모든 $s \in \mathscr{S}$ 에 대해서 계산하고 다음 $v_{k+1}$ 로 넘어간다고 생각하시면 편할 겁니다.
</div>

<p>시퀀스 {$v_k$} 는 $k$ 가 무한으로 갈 때 $v_\pi$ 로 수렴한다. 이렇게 반복적으로 업데이트 하는 알고리즘을 <strong>iterative policy evaluation</strong> 이라고 한다.</p>

<p>$v_k$ 으로부터 $v_{k+1}$ 에 대한 근사값(approximation) 을 구하기 위해서, 각 state $s$ 에서 동일한 iterative policy evaluation 을 적용한다. 이 작업은 기존 $s$ 의 값을 새로운 값으로 대체하는데, 이 값은 기존 $s$ 에 이어지는 다음 state 들의 값과 expected immediate reward 로부터 계산된다. 이런 업데이트를 <strong>expected update</strong> 라고 한다. 각 iterative policy evaludation 는 새로운 appoximate value function $v_{k+1}$ 를 구하기 위해서 한번 모든 state 의 값을 업데이트 한다. DP 알고리즘에서 완료된 update 를 expected update 라고 한다. 왜냐하면 다음 state 를 샘플링 하는 것이 아니라 가능한 모든 다음 state 를 기반으로 업데이트 하기때문이다.</p>

<p>식 (4.5) 을 기반으로 iterative policy evaluation 을 프로그래밍 하기 위해서는 두개의 배열이 필요하다. 하나는 이전 값을 보관하는 $v_k(s)$ 이고, 또 다른 하나는 새로운 값을 보관하는 $v_{k+1}(s)$ 이다. 물론 하나의 배열을 사용해서 이전 값을 새로운 값으로 덮어씌워도 된다. 만약 그렇게 하면 업데이트되는 순서에 따라서 이미 업데이트 된 값이 이전 값 대신에 사용될 수도 있지만, 그렇다 하더라도 $v_\pi$ 에 수렴하게 된다. 사실 이런 방법이 두개의 배열을 사용하는 것보다 더 빠르게 수렴한다. 왜냐하면 업데이트된 새로운 값을 바로 사용할 수 있기 때문이다. 이런 알고리즘을 <strong>in-place</strong> 알고리즘 이라고 하며, 앞으로 DP 에서는 주로 이 방법을 사용할 것이다.</p>

<p>다음은 <strong>Iterative policy evaluation</strong> 의 <strong>in-place</strong> 버전에 대한 슈도코드다.</p>

<p><img src="/assets/2019-02-15-rl-dynamic-programming/pic1.png" alt="image" /></p>

<hr />

<h3 id="policy-improvement">Policy Improvement</h3>

<p>value function 을 계산하는 이유 중 하나는 더 나은 policy 를 찾는 것이다. 우리가 deterministic policy 를 갖는 value function $v_\pi$ 를 계산했다고 가정해보자. 특정 state $s$ 에서 policy 를 변경해야 하는지 알고 싶을 것이다. 물론 $v_\pi(s)$ 로부터 현재의 policy 를 따르는 것이 얼마나 좋은지는 알고 있다. 그러나 새로운 policy 를 적용하는 것이 더 나은 것일까 아니면 유지하는 것이 더 나을까? 이 질문에 답하기 위한 방법 중 하나는 $s$ 에서 $a$ 를 선택해보는 것이다. 그리고 그 다음에는 기존의 policy $\pi$ 를 따르는 것이다. 식으로 나타내면 다음과 같다.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
q_\pi(s,a) &\doteq \mathbb{E}_\pi \left[R_{t+1} + \gamma v_\pi(S_{t+1})  \mid  S_t=s, A_t=a \right] \tag{4.6}\\
			&= \sum_{s^\prime, r} p(s^\prime, r \mid s, a) \left[ r + \gamma v_\pi(s^\prime) \right]
			
\end{align*} %]]></script>

<p>기준은 $v_\pi(s)$ 보다 크거나 작은지이다. 만약 더 크다면 $s$ 에서 한 번 $a$ 를 선택하고 그 뒤로 $\pi$ 를 따르는 것이 항상 $\pi$ 를 따르는 것보다 더 나을 것이다. 그렇다면 $s$ 에서 항상 $a$ 를 선택하는 것이 더 낫다고 예상할 수 있고, 새로운 policy 가 전체적으로 더 나은 policy 가 될 것이다.</p>

<p>$\pi$ 과 $\pi^\prime$ 를 모든 $ s \in \mathscr{S} $ 에서 deterministic policy 쌍으로 생각하자.</p>

<script type="math/tex; mode=display">q_{\pi}(s, \pi^\prime(s)) \ge v_\pi(s) \tag{4.7}</script>

<p>위의 식이 만족하기 위해서는 $\pi^\prime$ 이 $\pi$ 보다 같거나 더 좋아야 한다. 즉, 모든 $ s \in \mathscr{S} $ 에서 같거나 더 좋은 <strong>expected return</strong> 을 얻어야 한다.</p>

<script type="math/tex; mode=display">v_{\pi^\prime}(s) \ge v_\pi(s) \tag{4.8}</script>

<p>새로운 greedy policy $\pi^\prime$ 는 아래와 같이 구할 수 있다.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\pi^\prime  &\doteq \underset{a}{argmax}Q_\pi(s,a) \\
			&= \underset{a}{argmax} \mathbb{E} \left[ R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t=s, A_t=a \right] \tag{4.9} \\
			&= \underset{a}{argmax} \sum_{s^\prime, r} p(s^\prime, r \mid s, a) \left[ r + \gamma v_\pi(s^\prime) \right]
\end{align*} %]]></script>

<p>Greedy policy 는 one step 을 내다보고 $v_\pi$ 을 고려해서 가장 좋은 action 을 선택한다. 기존 policy 의 value function 에 대해 greedy 한 방법을 사용해서 기존의 policy 를 개선하여 새로운 policy 를 만드는 프로세스를 <strong>policy improvment</strong> 라고 한다. 만약 새로운 policy $\pi^\prime$ 이 기존 policy $\pi$ 와 유사하여 $v_\pi = v_{\pi^\prime}$ 이라면 식 (4.9) 로 부터 다음 식이 성립된다.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
v_{\pi^\prime}(s)  &= \underset{a}{max} \mathbb{E} \left[ R_{t+1} + \gamma v_{\pi^\prime}(S_{t+1}) \mid S_t=s, A_t=a \right] \\
			&= \underset{a}{max} \sum_{s^\prime, r} p(s^\prime, r \mid s, a) \left[ r + \gamma v_{\pi^\prime}(s^\prime) \right]
\end{align*} %]]></script>

<p>그런데 위 식은 $v_{\pi^\prime}$ 을 $v_*$ 으로, 그리고 $\pi$ 을 $\pi^\prime$ 으로 두면 식 (4.1) 과 같다. 즉 Policy improvment 는 기존 policy 가 opimal 인 경우를 제외하고는 더 나은 policy 를 제공한다.</p>

<p>이번 세션에서 지금까지는 <strong>deterministic policy</strong> 의 특별한 경우만 고려했다. 그러나 일반적인 경우에 policy $\pi(a \mid s)$ 는 확률적인 <strong>stochastic policy</strong> 이다. 여기서 더 자세히 다루지는 않지만 이 세션에 있는 아이디어들은 쉽게 stochastic policy 로 확장될 수 있다. stochastic case 인 경우에 오직 하나의 action 만 선택할 필요는 없다. 만약에 최대화하는 여러개의 action 이 있다면, 확률을 나눠서 새로운 policy 에 부여할 수 있다.</p>

<hr />

<h3 id="policy-iteration">Policy Iteration</h3>

<p>한 번 $v_\pi$ 를 사용해서 policy $\pi$ 에서 개선된 policy $\pi^\prime$ 를 구혔다면, 다시 $v_{\pi^\prime}$ 을 사용해서 policy $\pi^{\prime\prime}$ 을 구할 수 있다. 따라서 개선되는 policy 와 value function 의 시퀀스를 얻을 수 있다.</p>

<p align="center">
  <img width="60%" height="60%" src="/assets/2019-02-15-rl-dynamic-programming/pic2.png" />
</p>

<p>$\underset{\longrightarrow}{E}$ 는 policy <strong>evaluation</strong> 을 나타내고 $\underset{\longrightarrow}{I}$ 는 policy <strong>improvement</strong> 를 나타낸다. 각 policy 는 이전 policy 보다 개선됨을 보장한다. <strong>Finite MDP</strong> 는 유한(finite)개의 policy 가 존재하기 때문에, 이 프로세스는 유한(finite)번 iteration 에서 <strong>optimal policy</strong> 와 <strong>optimal value function</strong> 으로 수렴해야 한다. Optimal policy 를 찾는 이런 과정을 <strong>policy iteration</strong> 이라고 한다. 전체 알고리즘은 아래에 나와있다.</p>

<p><img src="/assets/2019-02-15-rl-dynamic-programming/pic3.png" alt="image" /></p>

<hr />

<h3 id="value-iteration">Value Iteration</h3>

<p>Policy iteration 의 한가지 단점은 매 iteration 마다 policy evaluation 을 포함한다는 것이다. Policy evaluation 은 반복적으로 state set 을 여러번 sweep 하는 것을 필요로 하기 때문에, 그 자체적으로 시간이 쇼요되는 연산이다. 그렇다면 policy evaluation 에서 반복하는 작업을 줄일 수 없을까?</p>

<p>사실 policy iteration 이 수렴하는 것을 보장하면서 policy evaluation 과정을 줄일 수 있는 여러가지 방법이 있다. 그 중 중요한 한가지 방법은 오직 한 번만 sweep (각 state 를 한 번 업데이트) 하고 policy evaluation 을 끝내는 것이다. 이 알고리즘을 <strong>value iteration</strong> 이라고 한다. 이 알고리즘은 <strong>policy improvement</strong> 와 간략해진 <strong>policy evaluation</strong> 단계를 조합한 간단한 업데이트로 아래와 같이 작성할 수 있다.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
v_{k+1}(s)  &= \underset{a}{max} \mathbb{E} \left[ R_{t+1} + \gamma v_{k}(S_{t+1}) \mid S_t=s, A_t=a \right] \\
			&= \underset{a}{max} \sum_{s^\prime, r} p(s^\prime, r \mid s, a) \left[ r + \gamma v_k(s^\prime) \right], \text{for all } s \in \mathscr{S} \tag{4.10}
\end{align*} %]]></script>

<p>Value iteration 을 이해할 수 있는 또 다른 방법으로는 <strong>Bellman optimality</strong> 식 (4.1) 을 참고하는 것이다. Value interation 은 간단하게 Bellman optimality 식을 업데이트 방식으로 변경함으로써 얻을 수 있다. 또한 모든 action 중에서 최댓값을 취하는 것을 제외하면 value iteration 업데이트 식이 policy evaluation 업데이트 식 (4.5) 와 같다. 이 둘의 관계를 더 자세하게 보기 위한 또 다른 방법은 이전 장에서 $v_\pi$ 와 $v_*$ 의 backup digram 을 비교하는 것이다. 아래는 value iteration 의 전체 알고리즘이다.</p>

<p><img src="/assets/2019-02-15-rl-dynamic-programming/pic4.png" alt="image" /></p>

<p>Value iteration 은 policy evaluation 의 one sweep 과 policy improvement 의 one sweep 을 효과적으로 조합한다. 각 policy impovement sweep 마다 policy evaluation sweep 을 여러번 해주는 것이 더 빠르게 수렴하기도 한다. 일반적으로 policy iteration 알고리즘은 policy evaluation 업데이트와 value iteration 업데이트로 이뤄진 sweep 의 시퀀스로써 생각할 수 있다. 왜냐하면 식 (4.10) 에 있는 max 연산만이 이 두개의 업데이트에서 차이점이기 때문이다. 단지 policy evaluation 의 sweep 중 일부에 max 가 추가된 것이다. 이 모든 알고리즘들은 <strong>discounted finite MDPs</strong> 에서 <strong>optimal policy</strong> 로 수렴한다.</p>

<hr />

<h3 id="asynchronous-dynamic-programming">Asynchronous Dynamic Programming</h3>

<p>DP 의 유일한 단점은 MDP 의 모든 state set 을 연산한다는 것이다. 만약에 state set 이 매우 크다면 한 번 sweep 하는데 엄청나게 많은 비용이 든다. 예를 들어서 backgammon 게임에서는 $10^{20}$ 의 state 가 존재한다. 1초당 100만번 value iteration 을 할 수 있더라도 한 번 sweep 하는데 천년이 걸린다.</p>

<p><strong>Asynchronous DP</strong> 알고리즘은 state set 의 체계적인(순서가 있는) sweep 으로 구성되어 있지 않은 DP 알고리즘이다. 이 알고리즘은 state 의 value 를 순서와 상관 없이 업데이트한다. 어떤 state 의 값은 다른 state 의 값들이 한 번 업데이트되기 전에 여러번 업데이트 되기도 한다. 그러나 정확하게 수렴하기 위해서는 asynchronous 알고리즘이 모든 state 값을 계속 업데이트해야 한다. 다시 말해서, 어떤 state 도 무시할 수 없다. Asynchronous DP 알고리즘은 업데이트 될 state 를 선택하는데 있어서 매우 큰 유연성을 제공한다.</p>

<p>이와 유사하게 비동기적으로 간략화된(asynchronous trucated) policy iteration 을 구현하기 위해서 policy evaluation 과 value iteration 업데이트를 조합할 수 있다. 여기서 더 자세하게 다루지는 않겠지만, 다양한 종류의 sweepless DP 에서 유연하게 사용할 수 있는 몇가지 다른 업데이트 방법들이 존재한다.</p>

<p>물론 sweep 을 피한다고 항상 더 적은 연산을 하지는 않는다. 단지 policy 를 더 향상시키기 위해서, 알고리즘이 말도 안되게 긴 시간을 사용할 필요는 없다는 것이다. 알고리즘의 진행 속도를 향상시키기 위해서 업데이트할 state 를 선택하는 유연함의 이점을 사용할 수 있다. 어떤 state 는 다른 state 들 처럼 자주 업데이트 될 필요가 없다. 먼약 optimal 을 찾는 것과 관련이 없는 state 가 있다면 업데이트를 건너 뛸 수 있다.</p>

<p>또한, 비동기(asynchronous) 알고리즘을 사용하면 실시간(real-time) 상호작용을 더 쉽게 조합하여 계산할 수 있다. MDP 문제를 풀기 위해서, agent 가 실제로 MDP 를 경험하는 시점에서 iterative DP 알고리즘을 사용할 수 있다. Agent 의 경험은 DP 알고리즘이 어떤 state 를 업데이트할지 결정한다. 동시에, DP 알고리즘으로 부터 얻은 마지막 value 와 policy 정보는 agent 의 <strong>의사결정(decision making)</strong>을 도와준다. 예를 들어서, agent 가 방문하는 state 를 업데이트 할 수 있다. 이렇게 하면 agent 와 가장 관련이 있는 state set 의 집합에 DP 알고리즘의 업데이트에 집중할 수 있다. 이런 방식은 reinforcement learning 에서 반복되는 주제다.</p>

<hr />

<h3 id="generalized-policy-iteration">Generalized Policy Iteration</h3>

<p><strong>Policy iteration</strong> 은 현재 policy 에 맞게 value function 을 만드는 <strong>policy evaluation</strong> 과 현재 value function 에 맞게 greedy 한 policy 를 만드는 <strong>policy improvement</strong> 의 두 가지 프로세스로 이루어진다. Policy iteration 에서 이 두 가지 프로세스가 번갈아가면서 진행되고, 각 프로세스는 다른 프로세스가 시작하기전에 완료되지만 필수는 아니다. 예를 들어서, <strong>Value iteration</strong> 에서는 각 policy improvement 사이에 policy evaluation 가 딱 한 번씩 진행된다. 비동기(asynchronous) DP 에서는 더 미세하게 교차한다. 경우에 따라서 프로세스에서 오직 하나의 state 가 업데이트되어 다른 프로세스로 반환된다. 두 프로세스가 모든 state 를 업데이트 하기만 한다면, 궁극적으로 최종 결과는 <strong>optimal value function</strong> 과 <strong>optimal policy</strong> 로 수렴한다.</p>

<p><strong>Policy evaluation</strong> 과 <strong>Policy improvement</strong> 의 세부사항과는 관계 없이 두 가지 프로세스가 상호작용하는 일반적인 아이디어를 <strong><em>generalized policy iteration</em> (GPI)</strong> 로 표현한다. 거의 모든 reinforcement learning 은 GPI 로 설명된다. Policy 는 항상 value function 에 대해서 향상되고, value function 은 항상 policy 를 따르는 방향으로 나아간다. 만약 evaluation 프로세스와 improvement 프로세스가 안정화가 되고 결과가 바뀌지 않는다면, value function 과 policy 는 최적화(optimal)가 된 것이다. Value function 은 policy 에 대해서 일치할 때 안정화되고, policy 는 value function 에 대해서 greedy 하게 계산 되었을 때 안정화된다. 그러므로, 두 프로세스는 evaluation function(value function) 에 대해서 greedy 한 policy 를 찾은 경우에만 안정화된다. 이것은 <strong>Bellman optimality equation</strong> (4.1) 을 나타내고 있으며 policy 와 value function 이 optimal 하다는 것을 의미한다.</p>

<p align="center">
  <img width="30%" height="30%" src="/assets/2019-02-15-rl-dynamic-programming/pic5.png" />
</p>

<p>GPI 에서 evaluation 과 improvement 프로세스는 경쟁하면서 동시에 협력하는 것으로 볼 수 있다. 서로 반대 방향으로 당긴다는 의미에서 둘은 경쟁한다. 일반적으로, Value function 에 대해서 greedy 하게 policy 를 만드는 것은 변경되는 policy 에 대해서 value function 을 부정확하게 만든다. 그리고 policy 와 일치하는 방향으로 value function 을 만드는 것은 policy 가 더 이상 greedy 하지 않다는 것을 의미한다. 그러나 장기적인 관점에서, 이 두 프로세스는 <strong>optimal value function</strong> 과 <strong>optimal policy</strong> 이라는 공통된 솔루션을 찾기 위해서 상호작용한다.</p>

<p>또한, GPI 에서 evaluation 과 improvement 프로세스의 상호작용을 두 경계나 목표(goal)측면에서 생각할 수도 있다. 예를 들어서, 아래 2차원 공간에서 두 선처럼 말이다. 사실 이것보다 더 복잡하지만, 다이어그램은 실제로 어떤일이 벌어지는지 보여준다.</p>

<p align="center">
  <img width="50%" height="50%" src="/assets/2019-02-15-rl-dynamic-programming/pic6.png" />
</p>

<hr />

<h3 id="efficiency-of-dynamic-programming">Efficiency of Dynamic Programming</h3>

<p>DP 는 매우 큰 문제에서는 실용적이지 않을 수 있다. 그러나 MDPs 를 해결하는 다른 방법들과 비교하면 상대적으로 DP 는 매우 효율적이다. 만약 기술적인 세부사항을 무시한다면, 최악의 경우 optimal policy 를 찾기 위한 시간은 state 와 action 수에 따른 다항식과 같다. $n$ 과 $k$ 를 state 와 action 의 개수라고 지칭하면, DP 식은 $n$ 과 $k$ 의 다항식보다 적은 계산을 한다는 것을 의미한다. DP 식은 policy 의 총 개수가 $k^n$ 일지라도, optimal policy 를 찾는 것을 보장한다. 이런 의미에서, DP 는 모든 policy 를 직접 조사하는 것(direct search) 보다 기하급수적으로 더 빠르다.</p>

<p>때때로, DP 는 차원의 저주(curse of dimentionality) 때문에 실용성에 한계가 있다고 여겨지기도 하는데, 이는 state 의 수가 state 변수의 수에 따라 기하급수적으로 증가하기 때문이다. 비록 큰 state 셋은 문제를 해결하는데 어려움을 야기하지만, 이는 문제 자체의 본질적인 어려움이지 솔루션으로서 DP 의 문제는 아니다. 실제로 DP 는 큰 state 공간에서 문제를 해결하는데 있어서, direct search 나 linear programming 보다 더 적합하다.</p>

<p>실제로 DP 는 수백만의 state 를 갖는 MDPs 문제를 해결하는데 사용되고 있으며 policy iteration 과 value iteration 도 널리 사용되고 있다. 사실 이런 방법들은 각각 가지고 있는 이론적으로 최악인 경우보다 훨씬 빠르게 수렴하며, 이는 특히 value function 이나 policy 의 초기화가 잘 된 경우 나타난다.</p>

<hr />

<h3 id="summary">Summary</h3>

<p>이번 챕터에서는 <strong>Finite MDPs</strong> 를 해결하는 것과 관련이 있는 <strong>dynamic programming</strong> 의 기본 아이디어와 알고리즘에 대해 알아보았다. <strong>Policy evaluation</strong> 은 주어진 policy 의 value function 을 반복적으로 계산하는 것을 나타낸다. 그리고 <strong>policy improvement</strong> 는 주어진 value function 을 고려해서 더 나은 policy 를 계산하는 것을 나타낸다. 이 두가지를 함께 사용해서 DP 에서 가장 유명한 <strong>policy iteration</strong> 과 <strong>value iteration</strong> 을 계산할 수 있다. 이들 중 한가지를 이용해서 MDP 에 대한 완벽한 정보가 주어진 <strong>finite MDPs</strong> 의 optimal policy 와 value function 을 계산할 수 있다.</p>

<p>Classical DP 는 state set 을 sweep 을 통해서 수행하며, 각 state 에 대해서 <em>expected update</em> 한다. State 하나의 업데이트는 가능한 모든 다음 state 의 값(value)과 각 state 가 발생할 수 있는 확률을 기반으로 수행된다. Expected update 는 Bellman equation 과 관련이 있다. 업데이트에서 더 이상 결과값이 변하지 않으면 Bellman equation 을 충족하는 값으로 수렴된 것이다. 4개의 주요한 값($v_\pi, v_*, q_\pi, q_*$) 에 대한 Bellman equation 과 그에 해당하는 expected update 가 있다. 또한 DP 업데이트를 직관적으로 볼 수 있는 <strong>backup diagram</strong> 이 있다.</p>

<p>사실 거의 모든 reinforcement learning 은 <strong>generaized policy iteration(GPI)</strong> 의 관점에서 볼 수 있다. <strong>GPI</strong> 는 <strong>appoximate policy</strong> 와 <strong>appoximate value function</strong> 을 중심으로 돌아가는 두 개의 상호작용 프로세스의 일반적인 아이디어다. 하나의 프로세스(policy evaluation)는 policy 를 주어진 것으로 여기고 value function 을 policy 에 대한 true value function 에 가깝도록 변경해간다. 또 다른 프로세스(policy imporvement)는 value function 을 주어진 것으로 여기고 value function 에 맞도록 policy 를 더 좋은 방향으로 변경해간다. 비록 각 프로세스가 다른 프로세스의 기준을 변경하더라도, 전반적으로 공통 솔루션을 찾기 위해서 함께 동작한다.</p>

<p>모든 것은 다음 state 값의 추정치(estimate)를 기반으로 state 값의 추정치(estimate)를 업데이트 한다. 즉 다른 추정치를 이용해서 추정치를 업데이트하는 것이다. 이런 아이디어를 <strong>bootstrapping</strong> 이라고 한다. 많은 reinforcement learning 에서는 bootstrapping 을 수행한다. 다음 챕터에서는 model 을 요구하지 않고, bootstrap 을 수행하지 않는 방법에 대해서 알아볼 것이다. 그리고 그 다음 챕터에서는 model 은 요구하지 않지만 bootstrap 을 수행하는 방법에 대해 알아볼 것이다.</p>


    <article>
    <div class="post-more">
      
      <a href="/reinforcement%20learning/2019/02/15/rl-dynamic-programming/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/reinforcement%20learning/2019/02/15/rl-dynamic-programming/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/reinforcement%20learning/2019/01/31/rl-finite-markov-decision-rocesses/">
        강화학습 정리 - Finite Markov Decision Processes
      </a>
    </h1>

    <span class="post-date">31 Jan 2019</span>
     | 
    
    <a href="/blog/tags/#강화학습" class="post-tag">강화학습</a>
    
    <a href="/blog/tags/#rl" class="post-tag">RL</a>
    
    

    <article>
      <h2 id="3-finite-markov-decision-processes">3. Finite Markov Decision Processes</h2>

<p>이번 챕터에서는 앞으로 계속 다룰 finite <strong>Markov decision processes (MDPs)</strong> 에 대해서 소개한다. 그런데 이전 챕터에서 다룬 bandits problem 과는 다르게, 상황에 따라서 다른 action 을 선택해야 하는 <em>associative task</em> 이다. MDPs는 <a href="https://en.wikipedia.org/wiki/Sequential_decision_making">순차적 의사 결정(sequential decision making)</a>을 하는 고전적인 공식인데, 여기서 선택된 action은 immediate reward 에만 영향을 미치는 것이 아니라, 나중의 상황(situation)이나 <strong>state</strong> 그리고 <strong>future reward</strong> 에도 영향을 미친다. 그렇기 때문에 MDPs는 <strong>delayed reward</strong> 라는 개념을 가지고 있으며, immediate reward 와 tradeoff 해야 하는 필요성도 수반하고 있다. Badit problem 에서는 각 action $a$ 의 value로 <strong>$q_*(a)$</strong> 를 estimate했지만, MDPs에서는 action $a$ 와 각 state $s$ 의 value로 <strong>$q_*(s,a)$</strong> 를 estimate한다.</p>

<hr />

<h3 id="the-agentenvironment-interface">The Agent–Environment Interface</h3>

<p>MDPs는 상호 작용하며 학습하는 문제를 간단하게 정의한다. 학습과 동시에 의사 결정하는 것을 <strong><em>agent</em></strong> 라고 하며, 이 agent 와 상호작용 하며 agent 의 외부에 존재하는 모든 것을 <strong><em>environment</em></strong> 라고 한다. agen 가 action 을 선택하면 environment 는 새로운 상황(situation)을 제시한다. 또한 environment 는 reward 를 제공하는데, agent 는 이 reward가  최대가 되도록 action 을 선택해 나아가야 한다.</p>

<div class="message">
situation 과 state 단어가 나오는데요, 여기서는 situation 을 state 를 포함한 개념으로 사용합니다. 중요한 키워드는 state 와 reward 입니다. 
</div>

<p><img src="/assets/2019-01-31-rl-finite-mdps/figure3_1.png" alt="image" /></p>

<p>위 그림처럼 각 time step($t$ = 0, 1, 2, 3, …)마다 <strong>agent</strong> 와 <strong>environment</strong> 는 상호작용 한다. time step $t$ 에서 agent 는 environment 로부터 <strong>state</strong> ($ S_t \in \mathscr{S}$ )를 받으며 이를 고려해서 <strong>action</strong> ($ A_t \in \mathscr{A}(s)$ )을 선택한다. 그리고 한 time step 이후에 action 에 대한 결과로 <strong>reward</strong> ($ R_{t+1} \in \mathscr{R} \subset \mathbb{R} $)와 새로운 state ($ S_{t+1} $)를 받는다. 이를 순서대로 나타내면 아래와 같다.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, ... && (3.1)
\end{align*} %]]></script>

<p>이전 time step 에서 state $s$와 action $a$이 주어졌을 때, $s^\prime$ 와 $r$이 나올 확률은 아래와 같이 표현할 수 있다.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
p(s^\prime,r \mid s,a) \doteq \operatorname{Pr}\left\{ S_t=s^\prime, R_t=r  \mid  S_{t-1}=s, A_{t-1}=a \right\} && (3.2)
\end{align*} %]]></script>

<p>함수 $p$는 MDP의 <em>dynamics</em>를 정의한다. 또한, $p$는 확률 분포이기 때문에 아래와 같이 정의할 수 있다.</p>

<script type="math/tex; mode=display">\sum_{s^\prime \in \mathscr{S}}\sum_{r \in \mathscr{R}}p(s^\prime,r \mid s,a) = 1, \text{  for all }s \in \mathscr{S}, a \in \mathscr{A}(s)</script>

<p>$S_t$와 $R_t$의 값은 바로 직전의 $S_{t-1}$와 $A_{t-1}$에 의해서만 결정된다. state 는 이전에 발생한 agent 와 environment 의 상호작용에 대한 모든 정보를 내포하고 있어야 하는데, 여기서는 이 state 를 <strong><em><a href="https://en.wikipedia.org/wiki/Markov_property">Markov property</a></em></strong>라고 한다.</p>

<p>다음과 같이 함수 $p$으로부터 environment에 대해서 알고 싶은 정보를 계산할 수 있다.</p>

<ul>
  <li><strong><em>state-transition probabilities</em></strong></li>
</ul>

<script type="math/tex; mode=display">p(s^\prime \mid s,a) \doteq \operatorname{Pr}\left\{ S_t=s^\prime  \mid  S_{t-1}=s, A_{t-1}=a \right\} = \sum_{r \in \mathscr{R}}p(s^\prime,r \mid s,a)</script>

<ul>
  <li><strong><em>state-action</em></strong>의 <strong>reward 기댓값(expected reward)</strong></li>
</ul>

<script type="math/tex; mode=display">r(s,a) \doteq  \mathbb{E}\left[R_t  \mid  S_{t-1}=s, A_{t-1}=a\right] = \sum_{r \in \mathscr{R}}r\sum_{s^\prime \in \mathscr{S}}p(s^\prime,r \mid s,a)</script>

<ul>
  <li><strong><em>state-action-next-state</em></strong>의 <strong>reward 기댓값(expected reward)</strong></li>
</ul>

<script type="math/tex; mode=display">r(s,a,s^\prime) \doteq  \mathbb{E}\left[R_t  \mid  S_{t-1}=s, A_{t-1}=a, S_t=s^\prime\right] = \sum_{r \in \mathscr{R}}r\frac{p(s^\prime,r \mid s,a)}{p(s^\prime \mid s,a)}</script>

<p>MDP framework 는 다양한 방법으로 다양한 문제에 적용할 수 있다. 예를 들어서, time step 은 꼭 고정된 간격이 아니여도 되고, action 은 로보트 팔에 적용될 volatage 처럼 low-level 컨트롤 이거나 학교에서 점심을 먹을지 말지에 대한 선택이여도 된다. 또한 state는 sensor에 대한 정보나 방(room)에 있는 물체들에 대한 설명일 수 있다.</p>

<p><strong>MDP framework</strong> 는 상호작용으로부터 목표 지향적인(goal-directed) 학습 문제를 추상화한 것이다. 목표 지향적인(goal-directed) 행동을 취하는 어떤 문제에서도, agen와 environment 사이를 오가는 신호를 세가지로 요약할 수 있다.  agent 가 선택하는 신호를 <strong>action</strong> 이라고 하고, 선택이 이루어지는 기본이 되는 신호를 <strong>state</strong> 라고 하며, agent의 목표인 신호를 <strong>reward</strong> 라고 한다. MDP framework 가 모든 decision-learning problem 을 표현하기에는 충분하지 않을 수 있으나, 지금까지 상당히 유용하게 사용되어왔다.</p>

<p>물론 특정 state 와 action 은 작업마다 크게 다르며, 어떻게 표현 되느냐에 따라 성능에 크게 영향을 줄 수 있다.</p>

<hr />

<h3 id="goals-and-rewards">Goals and Rewards</h3>

<p>Reinforcement Learning 에서 agent의 <strong>목표(goal)</strong> 는 environment 로 부터 받는 특별한 신호인 <strong><em>reward</em></strong> 를 공식 하는 것이다. 각 time step 에서 reward 는 단순한 숫자다. 조금 더 간단하게 얘기하면 agent의 목표(goal)는 reward의 총 합을 최대화 하는 것이다. 총 합이라는 것은 즉시 받는(immediate) reward 뿐 아니라, 장기적으로 봤을 때 받을 모든 reward 의 누적값이 라는 의미다.</p>

<blockquote>
  <p>That all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).</p>
</blockquote>

<p>reward는 reinforcement learning 에서 가장 두드러지는 특징 중 하나다.</p>

<p>reward 에 대해서 goal 을 공식화하는 것이 한계가 있어 보이지만, 유용하고 넓은 범위에서 사용 가능하다는 것을 증명해왔다. 예를 들어서, 로봇(agent)이 미로를 탈출하는 방법을 학습 시킬때 time step 마다 <strong>-1</strong> reward를 주었더니 이 agent는 더 빨리 탈출 하는 방법을 학습했다. 또한 재활용 로봇이 빈 캔을 찾고 수집하는 방법을 학습할 수 있도록 캔을 수집할 때마다 <strong>+1</strong> reward를 주어서 학습시키기도 했다.</p>

<p>위의 예에서 보면, agent 는 항상 reward 를 최대화 하기 위해서 학습한다. 만약 agent 가 우리가 원하는 것을 학습하기 원한다면 agent 가 reward 를 최대화 하면서 목표(goal)를 달성할 수 있도록 reward를 제공해야 한다. 따라서 우리가 설정한 reward 는 달성하고자 하는 목표(goal)을 확실하게 가리키도록 해야 한다. Reward 는 agent 와 <strong><em>‘how’</em></strong> 가 아닌 <strong><em>‘what’</em></strong> 을 communication하는 방법이다.</p>

<hr />

<h3 id="returns-and-episodes">Returns and Episodes</h3>

<p>agent 의 목표는 장기적으로 누적은 reward 를 최대화 하는 것이다. 바로 이 누적된 reward 를 $G_t$ 로 아래와 같이 표현할 수 있다. 이 값을 <strong><em>expected return</em></strong> 이라고 한다.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
G_t \doteq R_{t+1} + R_{t+2} + R_{t+3} + \dots + R_T && (3.7)
\end{align*} %]]></script>

<p>위 식에서 $T$를 final time step 이라고 한다. agent-envirionment 사이에서 이뤄지는 상호작용을 <strong>episode</strong> 개념으로 나눌 수 있다. 이 episode 는 마치 게임에서 한 번 플레이 하는 것과 같은데, 여기서 final time step의 개념이 있는 것은 자연스러운 접근이다. 또한, 각 episode 에서 마지막 state 를 <strong>terminal state</strong> 라고 한다. episode 가 게임에서 승패와 같이 특정 결과로 끝나더라도 다음 episode 는 이전의 결과와 상관없이 새로 시작된다. 이런 episode로 이루어진 task를 <em>episodic task</em> 라고 한다. episodic task 에서 terminal state의 존재 여부에 따라서 state set 을 아래와 같이 구분한다.</p>

<ul>
  <li>$\mathscr{S}$  : terminal state 가 포함되지 않는 모든 state set</li>
  <li>$\mathscr{S}^+$: terminal state 이 포함된 모든 모든 state set</li>
</ul>

<p>반면에 하나의 episode 가 끝나지 않는 경우가 있는데. 이를 <strong>continuing task</strong> 라고 한다. 이런 경우 final time step 은 $T=\infty$로 무한이기 때문에, 최대화 하려고 했던 return 식 (3.7)에 문제가 생긴다. 그렇기 때문에 앞으로 다른 return 식을 사용한다. 여기에 <strong>discounting</strong> 라는 컨셉이 추가된다.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
G_t \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2R_{t+3} + \dots =  \sum_{k=0}^\infty \gamma^kR_{t+k+1} && (3.8)
\end{align*} %]]></script>

<ul>
  <li>$\gamma$: discount rate ($ 0 \le \gamma \le 1 $)</li>
</ul>

<p><strong>discount rate</strong> 는 미래(future) reward 의 현재 가치을 정의한다. 미래 시점인 $k$ time step 에서 받을 reward 의 현재 가치는 $\gamma^{k-1}$ 배 만큼 줄어든다. 만약 $\gamma &lt; 1$ 이면 (3.8) 식은 finite value 를 가지게 될 것이고, $\gamma = 0$이라면 agent 는 즉시 받을(immediate) reward $R_{t+1}$ 만 고려해서 action $A_t$를 선택할 것이다. 하지만 이렇게 immediate reward 만 고려해서 action 을 선택한다면 전체 return 값은 감소할 수 있다. $\gamma$ 가 1에 가까울 수록 전체 return 은 future reward 를 더 많이 수용한다. 이는 agent 가 미래에 받을 reward 를 신뢰한다고 볼 수 있다. 식 (3.8)은 아래와 같이 전개될 수 있다.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
G_t &\doteq R_{t+1} + \gamma R_{t+2} + \gamma^2R_{t+3} + \gamma^2R_{t+4} +\dots \\
	&= R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \gamma^2 R_{t+4} + \dots) \\
	&= R_{t+1} + \gamma G_{t+1} & (3.9)
\end{align*} %]]></script>

<p>return 식 (3.8) 에서 만약 reward 가 0이 아니고 $\gamma &lt; 1$ 이라면 식(3.8)은 finite 하다. 예를 들어서 reward 가 1이라고 가정하면 return 은 아래와 같다.</p>

<script type="math/tex; mode=display">G_t = \sum_{k=0}^\infty \gamma^k = \frac{1}{1 - \gamma}</script>

<hr />

<h3 id="unified-notation-for-episodic-and-continuing-tasks">Unified Notation for Episodic and Continuing Tasks</h3>

<p>이전 섹션에서 agent-environment 상호작용이 일련의 분리된 episode 로 나눠지는 <strong>episodic task</strong> 와 그렇지 않은 <strong>continuing task</strong> 에 대해서 알아봤다. 앞으로 이 두 종류의 task 에 대해서 다룰 것이기 때문에, 두 가지를 모두 정확하게 표현할 수 있는 하나의 표기법(notation)을 정의하는 것이 유용하다.</p>

<p>원래는 episodic task 를 더 정확하게 표현하기 위해서 추가적인 notation 이 필요하다. 왜냐하면 여러개의 연속적인 episode 을 표현해야 할 수도 있기 때문이다. 이 episode 와 time step 둘 모두 고려했을 때 episode $i$ 에서 time step $t$ 일 때 state를 $S_{t,i}$ 로 나타낼 수 있다. 다른 것들도 마찬가지로 $A_{t,i}$, $R_{t,i}$, $\pi_{t,i}$, $T_{i}$, 등으로 나타낼 수 있다. 그러나 여기서는 대부분의 경우 episode를 구분할 필요가 없기 때문에 단일 episode 만 고려할 것이기 때문에 $S_{t,i}$ 대신에 $S_t$ 을 사용할 것이다.</p>

<p>episodic task 와 continuing task 는 episode 의 종료 시점을 고려하면 통합 될 수 있는데 그림으로 나타내면 아래와 같다.</p>

<p align="center">
  <img width="80%" height="80%" src="/assets/2019-01-31-rl-finite-mdps/pic1.png" />
</p>

<p>위 그림에서 네모 모양은 episode 의 마지막에 해당하는 special absorbing state 이다. $S_0$ 에서 시작해서 연속적인 reward +1, +1, 0, 0, 0, … 를 받는데, 모두 더하면 $T=3$ 이거나 $T=\infty$ 이더라도 똑같은 reward 를 얻는다. discount rate 를 적용해도 마찬가지다. 식으로 나타내면 아래와 같이 나타낼 수 있다. 앞으로는 아래 식을 계속 사용한다.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
G_t \doteq \sum_{k=t+1}^T \gamma^{k-t-1}R_k && (3.11)
\end{align*} %]]></script>

<div class="message">
식 (3.8)과 (3.11)은 상당히 유사합니다. (3.8)에서 k가 t시점에서 부터 진행된 time step 이라면, (3.11)에서 k는 시작 time step 를 의미합니다. T time step 을 표현하기 위해서라고 생각하시면 될 것 같습니다. (3.8)에서 단순히 무한 을 T로 변환하면 식이 성립하지 않습니다.
</div>

<hr />

<h3 id="policies-and-value-functions">Policies and Value Functions</h3>

<p>거의 모든 reinforcement learning 알고리즘은 value function 을 estimating 하는 것을 포함한다. 이 <strong><em>value function</em></strong> 은 agent 가 주어진 state 에 있는 것이 얼마나 많은 <strong>expected return</strong> 을 받을지 estimate 한다. 그리고 agent 가 주어진 state 에서 선택한 action 이 얼마나 많은 expected return 을 받을지 estimate 하는 것을 <strong><em>action-value function</em></strong> 이라고 한다.</p>

<div class="message">
여기서 expected return 은 agent 가 앞으로 받을 rewards 의 합입니다. 식 (3.7), (3.8), (3.9)에 자세하게 나와있습니다.
</div>

<p>물론, expected reward 는 agent 가 어떤 action 을 선택해 나아가냐에 따라 달려 있다. 그렇기 때문에 value function 은 <strong>정책(policy)</strong> 이라고 하는 action 을 선택하는 방법에 영향을 받는다.</p>

<p><strong>policy</strong>는 state 와 action의 선택 확률을 맵핑한다. agent 가 time $t$ 에서 policy $\pi$ 를 따른다면, $S_t = s$ 가 주어졌을 때 $A_t = a$ 일 확률을 $\pi(a \mid s)$ 로 나타낼 수 있다.</p>

<p><strong>MDPs</strong> 에서 policy $\pi$ 를 따르는 <strong><em>value function</em></strong> 와  <strong>action value function</strong> 을 아래와 같이 정의할 수 있다.</p>

<ul>
  <li><strong><em>state-value function for policy $\pi$</em></strong></li>
</ul>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
v_\pi(s) \doteq  \mathbb{E}_\pi\left[G_t  \mid  S_t=s \right] = \mathbb{E}_\pi\left[ \sum_{k=0}^\infty \gamma^kR_{t+k+1} \mid S_t = s \right], \text{for all } s \in \mathscr{S}, && (3.12)
\end{align*} %]]></script>

<ul>
  <li><strong><em>action-value function for policy $\pi$</em></strong></li>
</ul>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
q_\pi(s,a) \doteq  \mathbb{E}_\pi\left[G_t  \mid  S_t=s, A_t=a \right] = \mathbb{E}_\pi\left[ \sum_{k=0}^\infty \gamma^kR_{t+k+1} \mid S_t = s, A_t = a \right], && (3.13)
\end{align*} %]]></script>

<p>위 두 value function 값은 경험을 통해서 측정할 수 있다. 예를 들어서, agent 가 policy $\pi$ 를 따르는 상황에서 무한히 각 state 에서 value 의 평균을 구한다면 <strong>$v_\pi(s)$</strong>에 <strong>수렴(converge)</strong> 한다. 그리고 마찬가지로 <strong>$q_\pi(s,a)$</strong> 도 수렴한다. 실제 return 을 매우 많이 샘플링해서 평균을 구하는 방법으로, <strong><em>Monte Carlo method</em></strong> 라고 한다.</p>

<p>Reinforcement learning 과 <strong>dynamic programming</strong> 에서 value function 의 근본적인 성질은 식(3.9) 처럼 <strong>재귀적인 관계(recursive relationship)</strong> 를 충족 시킨다는 것이다.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
v_\pi(s) &\doteq  \mathbb{E}_\pi\left[G_t  \mid  S_t=s \right]  \\
			&= \mathbb{E}_\pi \left[R_{t+1} + \gamma G_{t+1}  \mid  S_t=s \right] &  (by (3.9))\\
			&= \sum_a \pi(a \mid s) \sum_{s^\prime}\sum_r p(s^\prime,r \mid s,a) \left[ r + \gamma \mathbb{E}_\pi \left[G_{t+1} \mid S_{t+1} = s^\prime \right] \right] \\
			&= \sum_a \pi(a \mid s) \sum_{s^\prime, r} p(s^\prime,r \mid s,a) \left[ r + \gamma v_\pi(s^\prime) \right], \text{for all } s \in \mathscr{S}, & (3.14)
			
\end{align*} %]]></script>

<p>식 (3.14)는 $v_\pi$에 대한 <strong><em>Bellman equation</em></strong> 이다. 이것은 state 의 value 와 다음 state 의 value 에 대한 관계를 표현한다. 아래는 그림으로 나타낸 것이다.</p>

<p align="center">
  <img width="30%" height="30%" src="/assets/2019-01-31-rl-finite-mdps/pic2.png" />
</p>

<p>하얀 원은 <strong>state</strong> 이고 검정 점은 <strong>action</strong> 을 의미한다. 제일 위에 있는 state $s$ 에서 시작하여, agent는 policy $\pi$ 를 따라서 세 개의 action 중 하나를 선택한다. 그러면 environment 는 dynamics 함수 $p$에 따라서 reward $r$ 과 다음 state $s_\prime$ 를 준다. Bellman Equation(3.14)은 발생될 확률을 가지는 모든 가능성에 대한 평균이다. 처음 state 의 value 는 다음 expected state 의 (discounted) value 와 그에 따른 reward 의 합과 같다.</p>

<p>Reinforcement learning method 에서 심장이라고 할 수 있는 update 혹은 backup 연산을 나타내기 때문에, 위 다이어그램을 <strong>backup diagram</strong> 이라고 한다.</p>

<p><strong>$q_\pi(s, a)$</strong>에 대한 Bellman equation을 backup digram으로 나타내면 아래와 같다.</p>

<p align="center">
  <img width="30%" height="30%" src="/assets/2019-01-31-rl-finite-mdps/pic3.png" />
</p>

<hr />

<h3 id="optimal-policies-and-optimal-value-functions">Optimal Policies and Optimal Value Functions</h3>

<p>Reinforcement learning 문제를 해결한다는 것은 장기적 관점에서 가장 많은 <strong>reward</strong> 를 얻을 수 있는 <strong>policy</strong> 를 찾는 것이다. 만약 모든 state 에서 $\pi$ 의 expected return 이 $\pi^\prime$ 보다 같거나 더 크다면, policy $\pi$ 가 policy $\pi^\prime$ 보다 더 잘 정의된 것이다. 다시 말해서 $ s \in \mathscr{S}$ 에서 $v_\pi(s) \geq v_{\pi^\prime}$ 이라면 $\pi \geq \pi^\prime$ 이다. 이중에서 다른 모든 policy 보다 제일 나은 policy 를 <strong><em>optimal policy</em></strong> 라고 하며, <strong>$\pi_*$</strong> 로 표기한다. 그리고 이 optimal policy 를 따르는 state-value function 을 <strong><em>optimal state-value function</em></strong> 이라고 하며, action-value function 을 <strong><em>optimal action-value function</em></strong> 이라고 한다.</p>

<ul>
  <li><strong><em>optimal state-value function</em></strong></li>
</ul>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
v_*(s) &\doteq  \underset{\pi}{\text{max}} v_\pi(s), \text{	for all } s \in \mathscr{S}
			
\end{align*} %]]></script>

<ul>
  <li><strong><em>optimal action-value function</em></strong></li>
</ul>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
q_*(s,a) &\doteq  \underset{\pi}{\text{max}} q_\pi(s,a), \text{	for all } s \in \mathscr{S} \text{ and } a \in \mathscr{A}(s)
			
\end{align*} %]]></script>

<p>$q_*$ 를 $v_*$ 에 대한 식으로 아래와 같이 정의할 수 있다.</p>

<script type="math/tex; mode=display">\begin{align*}
q_*(s,a) = \mathbb{E} \left[ R_{t+1} + \gamma v_*(S_{t+1}) \mid  S_t=s, A_t=a \right]
\end{align*}</script>

<p>$v_*$는 policy 에 대한 value function 이기 때문에, Bellman Equation (3.14) 의 조건에 대해 일관성을 가져야 한다. 그러나 <strong>optimal value function</strong> 이기 때문에 특정한 policy 와 상관 없는 식을 가져야 한다. 이를 <strong>Bellman optimality equation</strong> 이라고 한다. 직관적으로 말하면, Bellman optimality equation 은 optimal policy 를 따른 state 의 value 와 가장 좋은 action 을 선택했을 때 받는 expected return 은 같다는 사실을 표현한다.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
 	v_*(s) &= \underset{a \in \mathscr{A}(s)}{\text{max}} q_{\pi_*}(s,a) \\
	&= \underset{a}{\text{max}} \mathbb{E}_{\pi_*} \left[ G_t \mid S_t = s, A_t = a \right]\\
	&= \underset{a}{\text{max}} \mathbb{E}_{\pi_*} \left[ R_{t+1} + \gamma G_{t+1} \mid S_t = s, A_t = a \right] & (by (3.9))\\
	&= \underset{a}{\text{max}} \mathbb{E} \left[ R_{t+1} + \gamma v_*(S_{t+1}) \mid S_t = s, A_t = a \right] & (3.18)\\
	&= \underset{a}{\text{max}} \sum_{s^\prime, r}p(s^\prime, r \mid s, a) \left[ r + \gamma v_*(s^\prime) \right] & (3.19)
\end{align*} %]]></script>

<p>위 식에서 마지막 두 방정식은 $v_*$ 에 대한 Bellman optimality equation 의 두가지 형태이다. $q_*$ 에 대한 <strong>Bellman optimality equation</strong> 은 다음과 같다.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
	q_*(s,a) &= \mathbb{E} \left[ R_{t+1} + \gamma \underset{a^\prime}{\text{max}}q_*(S_{t+1}, a^\prime) \mid S_t=s, A_t = a \right] \\
	&= \sum_{s^\prime, r}p(s^\prime, r \mid s, a) \left[ r + \gamma \underset{a^\prime}{\text{max}}q_*(s^\prime, a^\prime)) \right] & (3.20)
\end{align*} %]]></script>

<p>아래 backup diagram 은 $v_*$ 와 $q_*$ 에 대한 Bellman optimality equation 을 시각적으로 보여준다. 이 backup digram 은 agent 가 max 를 선택하는 부분인 호(arc)를 제외하면 앞의 $v_\pi$ 및 $q_\pi$ 와 같다. 왼쪽은 식 (3.19)을 보여주고, 오른쪽은 식 (3.20)을 보여준다.</p>

<p align="center">
  <img width="60%" height="60%" src="/assets/2019-01-31-rl-finite-mdps/figure3_4.png" />
</p>

<p>만약 $v_*$ 를 알고 있다면 optimal policy 를 찾는 건 상대적으로 쉽다. 각 state $s$ 에서 Bellman optimality equation 의 최대 값을 얻을 수 있는 action 이 하나 이상은 있을 것이다. 이 action 에 0이 아닌 확률이 부여된 policy 가 <strong>optimal policy</strong> 이다. 이것을 <strong>one-step search</strong> 로 생각할 수 있는데, one-step search 후에 선택한 action 이 optimal action 이다. <strong>장기적인 관점에서 기대되는 최적의 반환 값(optimal expected long-term return)</strong>인 $v_*$ 은 각 state 에서 사용된다. 그러므로 one-step-ahead search 는 long-term optimal action 을 찾는다고 볼 수 있다.</p>

<p>$q_*$ 가 있으면 optimal action 을 찾기가 더 쉽다. $q_*$ 가 있으면 agent는 one-step-ahead search를 하지 않아도 된다. 왜냐하면 모든 state $s$에서 $q_*(s,a)$ 가 가장 큰 action $a$ 를 찾으면 되기 때문이다. action-value function 은 모든 one-step-ahead search 의 결과를 보관(cache)한다. 이것은 각 state-action pair 에서 바로 사용할 수 있는 값으로써 <strong>optimal expected long-term return</strong> 를 제공한다.</p>

<p>명시적으로 Bellman optimality equation 을 푸는 것은 optimal policy 를 찾기 위한 또 다른 방법이다. 이것은 곧 reinforcement learning 문제를 해결하는 것이다. 그러나 이 방법은 거의 유용하지 않다. 이 방법은 실제 문제에서는 거의 얻을 수 없는 세가지 가정을 충족해야 한다.</p>

<ol>
  <li>Environment 의 dynamics 에 대해 정확하게 알고 있어야 한다.</li>
  <li>연산에 필요한 충분한 resource 가 있어야 한다.</li>
  <li>Markov property 를 만족해야 한다.</li>
</ol>

<p>그러나 우리가 관심을 가지고 있는 문제들은 거의 위 조건들을 충족하지 않는다. 예를 들어서 backgammon 게임에서 1번과 3번 은 충족하지만 2번 조건에서 문제가 생긴다. 왜냐하면 backgammon 게임은 거의 $10^{20}$ 의 state를 가지는데, 요즘 가장 빠른 컴퓨터를 사용하더라도 $v_*$ 이나 $q_*$ 를 계산하기 위해서는 수천년이 걸리기 때문이다. 그렇기 때문에 reinforcement learning 에서는 일반적으로 <strong>approximate solution</strong> 을 찾아야 한다.</p>

<p>다양한 decision-making 문제는 Bellman optimality equation 을 근사(approximately)하는 방법으로 해결한다고 볼 수 있다. <strong>Dynamic programming</strong> 은 Bellman optimality equation 에 조금 더 관련되어 있다. 많은 reinforcement learning 문제는 사전 정보가 아닌 실제 경험을 통해서 Bellman optimality equation 을 근사적으로(approximately) 해결하는 것으로 이해할 수 있다.</p>

<hr />

<h3 id="optimality-and-approximation">Optimality and Approximation</h3>

<p>지금까지 optimal value function 과 optimal policy 에 대해 정의했다. 분명 agent 는 optimal policy 를 매우 잘 학습할 수 있지만, 이는 현실에서 매우 드물게 이루어진다. 우리가 관심 있어하는 과제에서 optimal policy 를 구하기 위해서는 <strong>극도로 많은 계산량</strong>이 필요하다. 또한 environment 의 dynamic 에 대해 정확하게 알고 있더라도 Bellman optimality equation 으로 부터 optimal policy 를 계산하는 것은 간단하지 않다.</p>

<p>또한 가용 가능한 <strong>메모리</strong> 에도 한계가 있다. 다양한 문제들을 해결하기 위해서는 매우 많은 메모리가 필요하다. 각 array 나 table 을 사용해서 해결할 수 있는 작은 문제들이 있는데, 이런 경우를 <strong><em>tabular</em> case</strong> 라고 하고 여기에 대응하는 식을 tabular method 라고 한다. 그러나 우리가 관심있어하는 많은 문제들은 table 에 담을 수 있는 정보보다 훨씬 많은 state 들이 존재하다. 이런 경우 좀 더 간결한 <strong>매개 변수화된(parameterized)</strong> 함수를 이용하여 <strong>근사화(approximated)</strong> 해야 한다.</p>

<blockquote>
  <p>In these cases the functions must be approximated, using some sort of more compact parameterized function representation.</p>
</blockquote>

<hr />

<h3 id="summary">Summary</h3>

<p>Reinforcement learning 은 목표를 달성하기 위해서 어떻게 행동해야 하는지 배우는 것이다. 그리고 이 학습은 상호작용으로부터 이루어진다. <strong>agent</strong> 와 <strong>environment</strong> 는 연속적인 이산 time step 으로 부터 상호작용 한다. <strong>action</strong> 은 사용자가 선택하는 것이고, <strong>state</strong> 는 action 을 선택하기 위한 기반이 된다. 그리고 <strong>reward</strong> 는 그 선택을 평가하는 기준이다. agent 의 외적인 요소들은 불완전하게 제어가 가능하지만, 완벽하게 알려지지 않았을 수도 있다. <strong>policy</strong> 는 agent 가 action 을 선택하는데 필요한 각 state 에 대한 확률적인 규칙이다. 그리고 agent 의 목표는 앞으로 받을 reward 의 총 합을 최대화 하는 것이다.</p>

<hr />

<h3 id="reference">Reference</h3>
<ul>
  <li>[Reinforcement Learning: An Introduction - Richard S. Sutton a</li>
</ul>

    <article>
    <div class="post-more">
      
      <a href="/reinforcement%20learning/2019/01/31/rl-finite-markov-decision-rocesses/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/reinforcement%20learning/2019/01/31/rl-finite-markov-decision-rocesses/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/reinforcement%20learning/2019/01/08/rl-multi-armed-bandits/">
        강화학습 정리 - Multi-armed Bandits
      </a>
    </h1>

    <span class="post-date">08 Jan 2019</span>
     | 
    
    <a href="/blog/tags/#강화학습" class="post-tag">강화학습</a>
    
    <a href="/blog/tags/#rl" class="post-tag">RL</a>
    
    

    <article>
      <h2 id="2-multi-armed-bandits">2. Multi-armed Bandits</h2>

<p>강화학습이 다른 딥러닝과 구분되는 가장 중요한 특징은 선택한 action 에 대해 <strong>평가(evaluate)</strong> 를 한다는 것이다. 이런 피드백(feedback)은 얼마나 좋은지에 대한 평가이지 정답인지 아닌지를 알려주는 것은 아니다. 이번 챕터에서는 간단한 환경에서 강화학습의 평가(evaluate)에 중점을 두고 공부할 것이다. 오직 단 하나의 상황에서만 evaluate 하기 때문에 <em>full reinforcement learning problem</em> 의 복잡성을 피할 수 있다. 여기서 다룰 문제는 <strong>k-armed bandit problem</strong> 인데 이를 소개하면서 강화학습의 주요한 요소 몇가지도 함께 다룰 것이다.</p>

<div class="message">
이번 장에서는 단순화된 환경(nonassociative setting)에서 설명하기 때문에, 강화학습에서 처음에 많이 다루는 'frozen lake'와 비교하면서 보시면 헷갈리실 수 있습니다. 처음에는 'Multi-armed Bandits'를 독립적인 문제로 보고 접근하시는 것을 추천드립니다.
</div>

<hr />

<h3 id="a-k-armed-bandit-problem">A k-armed Bandit Problem</h3>

<p>A k-armed Bandit Problem 은 k개의 레버가 있는 슬롯머신에서 최대의 reward 를 받기 위한 문제다. 내용은 아래와 같다.</p>

<ol>
  <li>k개의 다른 option 이나 action 중에서 하나를 선택한다.</li>
  <li><em>stationary probability distribution</em>으로 부터 하나의 reward 를 받는다.</li>
  <li>최종 목표는 일정 기간 동안 전체 reward 를 최대화 하는 것이다.</li>
</ol>

<p>위 k-armed bandit problem에서 k action 을 선택할 때마다 reward 를 받는데, 이때 rewards 의 기댓값(expectation)을 선택된 action 의 <strong>value</strong> 라고 한다. 식으로 나타내면 아래와 같다.</p>

<script type="math/tex; mode=display">q_*(a)\doteq \mathbb{E}[R_t | A_t=a]</script>

<ul>
  <li>$A_t$: time step 이 $t$ 일 때 선택된 action</li>
  <li>$R_t$: time step 이 $t$ 일 때 $A_t$에 대한 reward</li>
  <li>$q_*(a)$: action $a$ 가 선택됐을 때 받는 reward 의 기댓값</li>
</ul>

<p>만약 우리가 각 action 에 대한 value 를 알고 있다면 k-armed bandit problem 을 해결하는 것은 매우 쉬울 것이다. 매번 value 가 가장 높은 action 을 선택하면 되기 때문이다. 그런데 처음에는 value 을 알 수 없기 때문에 계속해서 <strong>estimate</strong> 해야 한다. 이렇게 time step $t$에서 선택된 action $a$의 value 를 $Q_t(a)$라고 한다. 우리는 $Q_t(a)$가  $q_*(a)$에 가까워 지도록 계속해서 estimate 해야 한다.</p>

<p>action value 를 계속해서 estimate 한다면, time step 마다 적어도 한 개 이상의 가장 높은 value 를 갖는 action 이 있을 것이다. 그 action 을 <em>greedy</em> action 이라고 한다. 만약 이 greedy action 중 하나를 선택한다면 이를 <strong><em>exploiting</em></strong> 한다고 얘기한다. 반대로 <em>greedy</em> action이 아닌 다른 action 중 하나를 선택한다면 <strong><em>exploring</em></strong> 한다고 얘기한다. 당장 한 스텝만 바라봤을 때는 <strong><em>exploitation</em></strong> 이 value 를 최대화 하는 방법일지 몰라도, 장기적인 관점에서 바라봤을 때 <strong><em>exploration</em></strong>의 total reward 가 더 높을 수 있다.</p>

<p><em>exploration</em> 와 <em>exploitation</em> 의 균형을 맞춰 나가기 위해서 복잡한 수학적인 방법들이 존재하지만, 많은 가정들이 전제해야 하기 때문에 사실상 full reinforcment learning 문제에 적용하기는 불가능하다. 하지만, 이를 해결하기 위한 간단한 방법들이 존재하고 나중에 다룰 것이다.</p>

<hr />

<h3 id="action-value-methods">Action-value Methods</h3>

<p>여기서는 action 의 value 을 estimate 하는 방법(method)에 대해 더 자세하게 알아볼 것이다. 우리는 이것을 <strong><em>action-value methods</em></strong> 라고 부르는데 action 을 선택하기 위해서도 사용된다. 그리고 action 이 선택될 때마다 계산한 reward 의 평균을 <em>action 의 true value</em> 라고 한다. 수식은 아래와 같다.</p>

<script type="math/tex; mode=display">Q_t(a)\doteq \frac{\text{sum of rewards when $a$ taken prior to $t$}}{\text{number of times $a$ taken prior to $t$}} = \frac{\sum_{i=1}^{t-1}R_i \cdot \mathbb{1}_{A_i=a}}{\sum_{i=1}^{t-1}\mathbb{1}_{A_i=a}}</script>

<ul>
  <li>$\mathbb{1}_{predicate}$: $predicate$가 true 이면 1이고 false이면 0이다.</li>
</ul>

<p>위의 식에서 $\mathbb{1}_{A_i=a}$은 $a$가 선택된 경우만 계산하겠다는 의미다. 그리고 한 번도 $a$가 선택된 적이 없는 경우 분모가 0이 되어서 계산이 불가능하기 때문에 $Q_t(a)$를 기본 값(예: 0)으로 대신한다. 사실 이렇게 평균을 내는 것은 action value 를 estimate 하는 여러가지 방법 중 하나다. 우리는 이것을 <em>sample-average</em>라고 부를 것이다.</p>

<p>action 을 선택하는 가장 간단한 방법 중 하나는 당연히 estimate value 가 가장 큰 것(<em>greedy action</em>)을 선택하는 것이다. 만약 두 개 이상의 <em>greedy</em> action 이 있다면 그 중 아무거나 선택하면 된다. 우리는 이런 방법을 <strong><em>greedy</em> action selection</strong> 이라고 하며, 아래와 같이 정의할 수 있다.</p>

<script type="math/tex; mode=display">A_t \doteq \underset{a}{argmax}Q_t(a)</script>

<p>위에서 $argmax_a$는 뒤에 따라오는 $Q_t(a)$를 최대화(maximized)해주는 action(a)을 선택한다는 뜻이다. <em>greedy</em> action selection 은 지금 알고 있는 정보를 기반으로 <em>눈 앞에 보이는</em> reward 를 최대화하기 위해서 항상 <strong>exploit</strong> 한다. 하지만 이런 방식은 장기적 관점에서 봤을 때 더 좋은 action 을 놓칠 수 있다. 이런 단점을 보안하기 위해서 <strong>아주 적은 확률($\varepsilon$)</strong>로 action 중 하나를 랜덤으로 선택하는 방법이 있는데, 우리는 이 방법을 <strong>$\varepsilon-greedy$</strong> method 라고 한다. 이런 방법의 장점은 모든 action 들이 sampling 될 수 있기 때문에 $Q_t(a)$ 가 $q_*(a)$로 수렴된다는 것이다.</p>

<hr />

<h3 id="the-10-armed-testbed">The 10-armed Testbed</h3>

<p><strong><em>k</em>-armed bandit problems</strong> 으로 <em>greedy action-value method</em>(<strong>greedy method</strong>) ₩와 <em>$\varepsilon$-greedy action-value method</em>(<strong>$\varepsilon$-greedy method</strong>) 두 가지 방법을 비교 했다. 아래는 <em>k</em>가 10개인 <em>k</em>-armed bandit problems의 reward 분포다. 분산(variance)이 1이고 평균(mean)이 $q_*(a)$인 정규 분포(normal distribution)다. 이 분포로 부터 $t$ (time step)에서 $A_t$(action)를 선택했을 때 $R_t$(reward)를 얻는다.</p>

<p><img src="/assets/2019-01-02-rl-bulti-armed-bandits/figure2_1.png" alt="image" /></p>

<p>분산(variance)이 1이고 평균(mean)이 0인 정규 분포(normal distribution)에서 1000번의 action 을 선택했고, 이렇게 2000번 반복한 평균으로 성능을 측정했다.</p>

<p>아래는 <strong>greedy method</strong>와 두개의 <strong>$\varepsilon$-greedy method</strong>($\varepsilon$=0.01 과 $\varepsilon$=0.1)로 테스트한 결과다. 자세히 보면 처음에는 greedy method 가 더 빠르게 향상되는 것처럼 보이나 시간이 지날수록 $\varepsilon$-greedy method가 더 향상되는 것을 볼 수 있다. greedy method 는 시간이 지날수록 더디게 향상되는데, 이는 suboptimal 에 빠질 수 있기 때문이다. 아래 그래프를 보면 greedy method 는 optimal action 의 1/3 정도 밖에 도달하지 못했다. 반면에, $\varepsilon$-greedy method 는 계속해서 explore 하고 optimal action 을 찾기 위한 가능성을 높혔기 때문에 최종적으로 더 나은 성능을 보였다.</p>

<p><img src="/assets/2019-01-02-rl-bulti-armed-bandits/figure2_2.png" alt="image" /></p>

<p>$\varepsilon$-greedy method 의 이점은 task 에 따라서 다르다. 만약에 분산(variance)이 매우 크다면(예: 10) optimal policy 를 찾기 위해서 더 많은 exploration 를 해야한다. 이럴 경우 $\varepsilon$-greedy method 가 더 좋은 성능을 보인다. 하지만 만약 분산(variance)이 0이라면 단 한 번만 시도 해보고 true value 를 알 수 있기 때문에 exploration 없이도 optimal policy 를 찾을 수 있을 것이다. 하지만 이런 <strong>deterministic</strong> 한 경우라도 일부 가정이 불확실 하다면 exploration 은 필요하다. 예를 들어서 bandit problem 이 <strong>nonstationary</strong> 하다면 true value 는 시간이 지남에 따라 변경되기 때문에 exploration 이 필요하다. nonstationary 문제는 reinforcement learning 에서 자주 나타나는 상황이다. 이와 같이 reinforcement learning 에서 exploration 과 exploitation 의 균형은 매우 중요하다.</p>

<hr />

<h3 id="incremental-implementation">Incremental Implementation</h3>
<p>지금까지 논의한 <strong>action-value method</strong> 는 얻은 rewards 의 평균(sample averages)을 내어서 estimate 하였다. 이번에는 이렇게 매번 평균을 내는 것보다 더 효율적인 방법에 대해 알아볼 것이다.</p>

<p>복잡한 식를 피하기 위해서 하나의 action 에 대해서만 다룬다. action-value 식은 아래와 같다.</p>

<script type="math/tex; mode=display">Q_n \doteq \frac{R_1 + R_2 + \dots + R_{n-1}}{n-1}</script>

<ul>
  <li>$R_i$: i번째 action 이 선택된 후에 받은 reward</li>
  <li>$Q_n$: n-1번째 까지 측정된 action value</li>
</ul>

<p>위의 방식은 모든 reward 를 보관하고 있다가 value 를 estimate 할 때마다 계산하는데, 이런 방식은 시간이 지나고 reward 를 받을 때마다 더 많은 메모리와 연산을 필요로 한다. 예상할 수 있듯이, 이는 비효율적이기 때문에 평균(average)를 업데이트 하는 다른 방법이 필요하다. 이미 계산된 $Q_n$과 n번째 reward $R_n$이 주어지면 모든 reward 의 평균을 아래와 같이 <strong>incremental formulas</strong> 로 계산할 수 있다.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
Q_n &= \frac{1}{n}\sum_{i=1}^{n}R_i \\
    &= \frac{1}{n}\left(R_n + \sum_{i=1}^{n-1}R_i\right) \\
    &= \frac{1}{n}\left(R_n + (n-1)\frac{1}{n-1}\sum_{i=1}^{n-1}R_i\right) \\
    &= \frac{1}{n}\left(R_n + (n-1)Q_n\right) \\
    &= \frac{1}{n}\left(R_n + nQ_n - Q_n\right) \\
    &= Q_n + \frac{1}{n}\left[R_n - Q_n\right] & (2.3)
\end{align*} %]]></script>

<p>위의 연산은 새로운 reward 를 얻더라도 $Q_n$과 $n$ 그리고 (2.3)정도의 간단한 연산만 필요하다. 위의 update 식은 일반적으로 아래와 같이 표기할 수 있다.</p>

<script type="math/tex; mode=display">NewEstimate \leftarrow OldEstimate + StepSize [Target - OldEstimate]</script>

<p>위 식에서 $[Target - OldEstimate]$가 estimate 에서 <strong>error</strong> 이며, $Target$ 에 가까워질수록 작아진다. target 은 우리가 원하는 방향이며 여기서는 n번째 reward 다.</p>

<p>참고로, <strong>incremental method</strong> (2.3)에서 사용된 step-size parameter(StepSize)는 time step 이 지날 수록 $\frac{1}{n}$으로 변경된다. 우리는 앞으로 이 <strong>step-size</strong> 를 $\alpha$ 혹은 $\alpha_t(a)$로 표기할 것이다.</p>

<p>아래는 incrementally computed sample averages 와 $\varepsilon$-greedy action selection 을 사용한 A Simple bandit algorithm 의 슈도코드(Pseudocode)다.</p>

<p><img src="/assets/2019-01-02-rl-bulti-armed-bandits/pseudocode_1.png" alt="image" /></p>

<hr />

<h3 id="tracking-a-nonstationary-problem">Tracking a Nonstationary Problem</h3>

<p>지금까지는 시간이 지나더라도 reward 의 probability 가 변하지 않는 <strong>stationary</strong> 상황에서 bandit problems 에 대해 알아보았다. 하지만 reinforcement learning 에서는 종종 시간이 지남에 따라 reward 의 probability 가 변하는 <strong>nonstationary</strong> 상황에 대해서 다뤄야 할 때가 있다. 이런 상황에서는 한참 전에 받은 reward 보다 최근에 받은 reward 에 좀 더 비중을 두는 것이 합당하다. 우리는 이를 상수 step-size paramenter 를 사용해서 구현할 수 있다. 예를 들어서, (2.3)식을 수정한 식은 아래와 같다.</p>

<script type="math/tex; mode=display">Q_{n+1} = Q_n + \alpha\left[R_n - Q_n\right]</script>

<p>$\alpha$는 $\alpha\in(0,1]$인 상수다. $Q_{n+1}$은 지난 rewards 의 weighted average 이고 $Q_1$은 초기값이다.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
Q_n &= Q_n + \alpha\left[R_n - Q_n\right] \\
    &= \alpha R_n  + (1 - \alpha)Q_n\\
    &= \alpha R_n  + (1 - \alpha)[\alpha R_{n-1} + (1 - \alpha)Q_{n-1}] \\
    &= \alpha R_n  + (1 - \alpha)\alpha R_{n-1} + (1 - \alpha)^2 Q_{n-1} \\
    &= \alpha R_n  + (1 - \alpha)\alpha R_{n-1} + (1 - \alpha)\alpha R_{n-2} +
       \dots + (1 - \alpha)^{n-1}\alpha R_1 + (1 - \alpha)^n Q_1\\
    &= (1 - \alpha)^n Q_1 + \sum_{i=1}^{n}\alpha(1 - \alpha)^{n-1}R_i & (2.6)

\end{align*} %]]></script>

<p>우리는 위 식을 것은 weighted average 라고 부르는데, weight 의 총 합이 1이기 때문이다.</p>

<script type="math/tex; mode=display">(1 - \alpha)^n + \sum_{i=1}^{n}\alpha(1 - \alpha)^{n-1} = 1</script>

<p>$R_i$의 가중치 $\alpha(1 - \alpha)^{n-1}$ 는 이전에 얼마나 많은 reward 가 있었느냐에 따라 달라진다. (2.6) 식에서 $1-\alpha$ 은 1보다 작기 때문에, 전체 reward 의 수(n)가 증가 할수록 $R_i$ 이 값은 작아진다. 여기서 weight 은 $1 - \alpha$ 의 exponent 에 의해서 기하급수적으로 줄어드는데 이를 <em>exponential recency-weighted average</em> 라고 한다.</p>

<div class="message">
위 설명이 조금 복잡했는데요, 간단하게 요약하면 위 action-value method 에서 $\alpha$ 가 1보다 작기 때문에, 오래전에 받은 reward일 수록 가중치(weight)는 점점 작아지고, 가장 최근에 받은 reward의 가중치(weight)는 더 커진다는 뜻입니다.
</div>

<hr />

<h3 id="optimistic-initial-values">Optimistic Initial Values</h3>

<p>지금까지 나온 모든 수식들은 action-value 의 초기값 $Q_1(a)$ 에 영향을 받는다. 통게학에서는 이를 <em>bias</em>라고 얘기한다. sample-average methods 에서는 모든 action 들이 한 번씩 선택되면 이 bias 는 모두 사라진다. 하지만 step-size 인 $\alpha$ 가 존재한다면 $Q_1$ 은 점점 작아지더라도 완전히 사라지지는 않는다.(식 2.6 참고) 이 bias 는 종종 매우 유용한데, 예상되는 reward 의 값을 미리 설정할 수 있기 때문이다.</p>

<p>또한, action value 를 초기화하는 방법으로 간단하게 <strong>exploration</strong> 을 유도할 수 있다. 예를 들어서, 10-armed bandit testbed 에서 action value 의 초기값을 0이 아닌 +5로 설정했다고 가정해보자. $q_*(a)$ 의 평균은 0이고 분산은 1이다. 그러면 앞으로 action 이 어떤 것으로 선택되더라도 update 된 reward 는 초기값 +5보다 작을 것이기 때문에(식 2.6 참고), 다음에 선택할 greedy action 은 항상 시도해보지 않은 action 일 것이다. 즉, value estimate 가 수렴하기 전까지 <strong>exploration</strong> 할 것이다.</p>

<p>아래는 10-armed bandit testbed 에서 $Q_1(a) = +5$ 으로 설정한 greedy method 와 $Q_1(a) = 0$ 으로 설정한 $\varepsilon$-greedy method 를 비교한 것이다. 초기에는 greedy method($Q_1(a) = +5$) 가 더 많은 exploration 을 하기 때문에 좋은 나쁜 성능을 보이지만, 시간이 지날 수록 exploration하는 횟수가 줄어들면서 Optial action 에 더 가까워지는 것을 볼 수 있다. 이런 테크닉을 <strong><em>optimistic initial values</em></strong> 라고 한다.</p>

<p><img src="/assets/2019-01-02-rl-bulti-armed-bandits/figure2_3.png" alt="image" /></p>

<p>이런 방법은 특정 stationary problems 에서만 유용한 simple trick 으로, 일방적인 상황에서 exploration 하는 방법으로는 효율적이지 않다. 예를 들어서 nonstationary problems 에는 적합하지 않은데, 본질적으로 이런 방법은 일시적으로만 작동하기 때문이다.</p>

<hr />

<h3 id="upper-confidence-bound-action-selection">Upper-Confidence-Bound Action Selection</h3>

<p>action-value estimates 는 불확실성을 내재하고 있기 때문에 exploration 이 필요하다. greedy action 이 현재 시점에서는 가장 좋은 선택일지라도, 장기적인 관점에서 봤을 때 다른 action 이 더 좋은 선택일 수 있기 때문이다. 그래서 $\varepsilon$-greedy action selection 으로 non-greedy action 을 시도하는데, 그 중에서 어떤 action 이 더 좋을지에 대한 기준없이 랜덤으로 선택한다. 이런 방법 보다는 그 중에서 가장 optimal 이 될 가능성이 높은 action 을 선택하는 것이 더 나을 것이다. 아래는 효율적으로 action 을 선택하는 방법중 하나다.</p>

<script type="math/tex; mode=display">A_t \doteq \underset{a}{\operatorname{argmax}} \left[ Q_t(a) + c\sqrt{\frac{\operatorname{ln}t}{N_t(a)}} \right]</script>

<ul>
  <li>$\operatorname{ln}t$: $t$ 의 자연로그</li>
  <li>$N_t(a)$: time $t$ 전에 action $a$ 가 선택된 횟수</li>
  <li>$c$: $c &gt; 0$으로 exploration 의 빈도를 조절</li>
</ul>

<p>만약 $N_t(a) = 0$ 이면, 이때 $a$ 는 식을 가장 최대화 하는 action 으로 간주된다.</p>

<p>이런 아이디어를 <strong><em>upper confidence bound</em> (UCB)</strong> action selection 이라고 하는데 제곱근 식($\sqrt{\frac{\operatorname{ln}t}{N_t(a)}}$)은  $a$ value 의 불확실한 정도(uncertainty) 혹은 변화하는 정도(variance)를 측정한다. 분모에 있는 $N_t(a)$가 증가하면 불확실한 정도(uncertainty)는 낮아지고, 반면에 다른 action 이 선택 되어 $N_t(a)$는 증가하지 않고 $t$ 만 증가한다면 불확실한 정도(uncertainty)는 증가할 것이다. 결국 모든 action 이 선택될 것이다. 다만, 측정(estimate)된 value 가 낮거나 이전에 자주 선택됐던 action 은 시간이 지날수록 낮은 빈도로 선택될 것이다.</p>

<p>아래는 10-armed testbed에서 <strong>UCB</strong> 를 사용한 결과다. 보여지는 것처럼 UCB는 잘 작동하지만 더 일반적으로 셋팅된 reinforcement learning 문제에서는 $\varepsilon$-greedy 보다 낮은 성능을 보인다.</p>

<p><img src="/assets/2019-01-02-rl-bulti-armed-bandits/figure2_4.png" alt="image" /></p>

<hr />

<h3 id="gradient-bandit-algorithms">Gradient Bandit Algorithms</h3>

<p>지금까지는 action 을 선택하는데 action value 를 사용했다. 이 방법은 매우 좋은 방법이지만 유일한 방법은 아니다. 여기서는 action $a$ 를 선택하기 위해 각 action 의 <strong>preference</strong>을 사용할 것이며 $H_t(a)$ 으로 표기한다. preference 가 큰 action 이 더 자주 선택될 것이다. action 에 대한 preference 는 다음과 같이 <em>softmax distribution</em> 으로 나타낼 수 있다.</p>

<script type="math/tex; mode=display">\operatorname{Pr} \left\{ A_t=a \right\} \doteq \frac{e^{H_t(a)}}{\sum^{k}_{b=1}e^{H_t(b)}} \doteq \pi_t(a)</script>

<ul>
  <li>$H_t(a)$: action $a$ 의 preference</li>
  <li>$\pi_t(a)$: time $t$ 에서 action $a$ 를 선택할 확률</li>
</ul>

<p>모든 action 의 preference 초기값은 0으로 같기 때문에 (e.g., $H_1(a) = 0$), 처음에 각 action 의 선택될 확률은 모두 같다. 각 step 에서 action $A_t$ 가 선택되고 reward $R_t$ 를 받으면 action preference 는 다음 식에 의해 업데이트된다.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
H_{t+1}(A_t) &\doteq H_t(A_t) + \alpha(R_t - \bar{R_t})(1 - \pi_t(A_t)) && \text{and} \\
H_{t+1}(A_t) &\doteq H_t(a) - \alpha(R_t - \bar{R_t})\pi_t(A_t) && \text{for all } \alpha \neq A_t
\end{align*} %]]></script>

<ul>
  <li>$\alpha$: 0보다 큰 step-size parameter</li>
  <li>$\bar{R_t}$: time $t$를 포함한 모든 reward 의 평균</li>
</ul>

<p>위 식은 <em>Incremental Implementation</em> 에서 봤던 update 방식으로 계산된다. 그리고, $\bar{R_t}$ 은 reward 의 <strong>baseline</strong> 으로 사용되는데 만약 reward 가 baseline 보다 높으면 $A_t$가 선택될 확률은 증가하고, 반대로 reward 가 basline 보다 낮으면 $A_t$가 선택될 확률은 낮아진다. 선택되지 않은 action 은 이와 반대로 계산된다.</p>

<p>아래 Figure 2.5는 분산이 1이고 평균이 4인 정규 분포로부터 reward 를 얻는 10-armed testbed 에서 테스트한 결과다.</p>

<p><img src="/assets/2019-01-02-rl-bulti-armed-bandits/figure2_5.png" alt="image" /></p>

<div class="message">
사실 이 Gradient Bandit Algorithms 은 정확하게 이해하지 못했습니다. 위의 그래프를 보면 baseline을 사용한 경우 Optimal action 에 더 접근하는 것 같습니다.
</div>

<hr />

<h3 id="associative-search-contextual-bandits">Associative Search (Contextual Bandits)</h3>
<p>지금까지는 각 다른 상황(situation)에서 다른 action 이 필요하지 않은 <em>nonassociative task</em> 에 대해서 다뤘다. task 가 stationary 할 때는 best action 를 찾으면 됐고, task 가 nonstationary 할 때는 시간에 따라 best action 을 추적하면 됐다. 하지만 일반적인 reinforcement learning task 에서는 단지 한가지 상황만 존재하지 않는다. 각 상황에 맞는 action 을 찾기 위한 <strong>policy</strong> 를 학습해야 한다.</p>

<p>예를 들어서, 여러개의 다른 k-armed bandit task 가 존재하고 매 step 마다 그 중 하나를 랜덤으로 선택한다고 가정하자. 그렇다면 bandit task 는 매 step 마다 랜덤하게 바뀔 것이다. 그러면 마치 true action value 가 step 마다 변경되는 단 한개의 <strong>nonstationary</strong> k-armed bandit task 처럼 보일 것이다. 그렇다면 이번 챕터에서 배운 method 를 사용할 수 있더라도 제대로 작동하지 않을 것이다. 하지만 만약 각 <em>slot machine</em> 의 색이 다르다면 이와 연관된 <strong>policy</strong> 를 학습할 수 있을 것이다.</p>

<p>이것을 <em>associative search</em> task 라고 한다. 그리고 k-armed bandit problem 에서는 각 action 이 reward 에만 영향을 미치는데, 만약 다음 상황(next situation)에도 영향을 미친다면 이것을 <strong>full reinforcement learning</strong> 라고 한다.</p>

<hr />

<h3 id="summary">Summary</h3>

<p>아래는 이번 챕터에서 다룬 각 method 를 비교한 결과다.</p>

<p><img src="/assets/2019-01-02-rl-bulti-armed-bandits/figure2_5.png" alt="image" /></p>

<div class="message">
자세한 설명은 생략했습니다.
</div>

<hr />

<h3 id="reference">Reference</h3>
<ul>
  <li><a href="http://incompleteideas.net/book/the-book.html">Reinforcement Learning: An Introduction - Richard S. Sutton and Andrew G. Barto</a></li>
</ul>

    <article>
    <div class="post-more">
      
      <a href="/reinforcement%20learning/2019/01/08/rl-multi-armed-bandits/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/reinforcement%20learning/2019/01/08/rl-multi-armed-bandits/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/reinforcement%20learning/2019/01/01/rl-introduction/">
        강화학습 정리 - Introduction
      </a>
    </h1>

    <span class="post-date">01 Jan 2019</span>
     | 
    
    <a href="/blog/tags/#강화학습" class="post-tag">강화학습</a>
    
    <a href="/blog/tags/#rl" class="post-tag">RL</a>
    
    

    <article>
      <h2 id="1-introduction">1. Introduction</h2>

<p>우리가 배움의 본질에 대해 생각할 때 가장 먼저 떠올리는 것 중 하나는 <strong>환경(environment)</strong>과 상호작용 하면서 배우는 것이다. 예를 들어서 아기들은 가르쳐주는 사람이 없더라도 스스로 팔을 흔들고, 노는 행동을 하고, 주위를 바라보는 방법 등을 터득한다. 아기는 자신과 연결된 환경(environment)과 상호작용하며 배우는데 필요한 정보를 습득하는 것이다.</p>

<hr />

<h3 id="reinforcement-learning">Reinforcement Learning</h3>

<p>강화학습(Reinforcement Learning)은 현재 자신에게 주어진 환경에서 어떤 <strong>행동(action)</strong>을 해야 하는지 배우는 것이다. 그리고 이 행동(action)은 <strong>보상(reward)</strong>을 최대로 얻을 수 있는 방법을 따른다.</p>

<p>여기서 흥미로운 점은 내가 선택하는 행동(action)이 당장 받을 보상(immediate reward)뿐 아니라 다음 상황(state)과 나중에 받을 보상(subsequent rewards)에도 영향을 미친다는 것이다. 즉, 행동(action)을 선택할 때 나중에 받을 보상도 고려를 해야 하는 것이다.</p>

<p>또한, 강화학습이 받는 challenge 중 하나는 <strong>exploitation</strong> 과 <strong>exploration</strong>의 trade-off 이다. 행동(action)을 선택할 때 두 가지 접근 방법 중에서 하나를 선택해야 하기 때문이다.</p>

<ul>
  <li><strong>exploitation</strong>: 이미 학습한 정보를 토대로 최대 reward를 얻는 action을 선택한다.</li>
  <li><strong>exploration</strong>: 새로운 정보를 학습하기 위해서 여러가지 action을 선택해본다.</li>
</ul>

<p>아래는 교재에서 설명된 내용이다.</p>
<blockquote>
  <p>The agent has to <em>exploit</em> what it has already experienced in order to obtain reward, but it also has to <em>explore</em> in order to make better action selections in the future.</p>
</blockquote>

<p>마지막으로, 강화학습에서 <strong>학습하는 대상(agent)</strong>은 <strong>목표 지향적(goal-directed)</strong>이며 불확실한 환경(uncertain environment)에서도 모든 문제를 고려해야 한다.</p>

<hr />

<h3 id="elements-of-reinforcement-learning">Elements of Reinforcement Learning</h3>

<p>위에서 설명 했듯이 강화학습에서는 <strong>agent</strong>는 <strong>environment</strong>와 상호작용 하면서 학습하는데, 이 시스템에서 중요한 네가지 요소를 아래와 같이 정의할 수 있다.</p>

<ul>
  <li><strong>Policy</strong>: 주어진 environment의 state에서 agent가 선택해야 할 action을 정의</li>
  <li><strong>Reward signal</strong>: agent가 action을 선택했을 때 environment로 부터 받는 값</li>
  <li><strong>Value function</strong>: agent가 특정 state에서 앞으로 받을 모든 rewards의 합에 대한 기댓값을 정의</li>
  <li><strong>Model</strong>: environment의 동작(behavior)에 대해 정의</li>
</ul>

<p><strong>Policy</strong>는 현재의 내 상태(states of the environment)에서 취해야 할 action을 정의 하는데, 간단한 function이나 lookup table 일 수도 있다. 일반적으로 확률론(stochastic)적 이며 각 action에 대한 확률(probabilities)를 가지고 있다.</p>

<p><strong>Reward signal</strong>은 agent가 action을 선택 했을 때 즉각적으로 environment로 부터 받는 보상 값으로, 강화학습에서 agent의 유일한 목표는 이 reward의 총 합을 최대화하는 것이다. reward는 policy를 정의하기 위한 기본 요소다.</p>

<p><strong>Value function</strong>은 길게 보았을 때 agent가 특정 state에서 받을 reward의 총 합에 대한 기댓값을 정의한다. agent가 주어진 state에서 특정 action을 선택했을 때 당장 받을 reward는 다른 action을 취했을 때 보다 상대적으로 낮더라도 최종적으로 누적될 reward는 가장 높을 수도 있다.</p>

<p><strong>Model</strong>은 environment의 동작(behavior)에 대해 정의한다. 즉, model을 알고 있으면 state와 action이 주어졌을 때의 가능한 결과를 미리 예상할 수 있다. model은 agent가 직접 경험해 보기도 전에 <em>계획(planning)</em>을 세우는데 사용된다. 문제를 해결하기 위해서 model과 planning을 사용하는 방법을 <em>model-based</em>라고 한다. 반대로 agent가 model을 모르는 상태에서 <em>경험을 하면서(trial-and-error)</em> 문제를 해결하는 방법을 <em>model-free</em>라고 한다.</p>

<hr />

<h3 id="summary">Summary</h3>

<p>강화학습은 agent가 environment와 상호작용 하면서 학습한다는 점에서 다른 학습 방법과는 구분된다. 그리고 states, actions, rewards를 포함한 environment와 agent의 관계를 정의하기 위해서 <strong><a href="https://en.wikipedia.org/wiki/Markov_decision_process">Markov decision processes(MDP)</a></strong>라는 framework 를 사용한다. <strong>value</strong> 와 <strong>value function</strong> 컨셉은 강화학습에서 매우 중요하다. agent는 policy에 따라서 action을 취하는데, value는 policy를 정의하는데 근간이 되기 때문이다.</p>

<hr />

<h3 id="reference">Reference</h3>
<ul>
  <li><a href="http://incompleteideas.net/book/the-book.html">Reinforcement Learning: An Introduction - Richard S. Sutton and Andrew G. Barto</a></li>
</ul>

    <article>
    <div class="post-more">
      
      <a href="/reinforcement%20learning/2019/01/01/rl-introduction/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/reinforcement%20learning/2019/01/01/rl-introduction/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/algorithm/2018/12/29/boj-go-trip-1976/">
        BOJ - 여행 가자(1976)
      </a>
    </h1>

    <span class="post-date">29 Dec 2018</span>
     | 
    
    <a href="/blog/tags/#boj" class="post-tag">BOJ</a>
    
    <a href="/blog/tags/#백준" class="post-tag">백준</a>
    
    <a href="/blog/tags/#알고리즘" class="post-tag">알고리즘</a>
    
    <a href="/blog/tags/#자료구조" class="post-tag">자료구조</a>
    
    

    <article>
      <p>경로를 찾는 문제처럼 보이지만, <strong>유니온 파인드(Union-Find)</strong>를 사용해서 해결할 수 있는 간단한 문제입니다. 입력으로 들어온 각 두 도시를 연결 하고, 마지막 입력으로 주어진 도시들이 모두 연결되어 있는지 확인하는 문제입니다. 주어진 도시들의 root 가 모두 같으면 하나의 그래프로 연결되어 있다고 볼 수 있습니다.</p>

<blockquote>
  <p>[A-B], [B-C], [A-D] 이면 A, B, C, D 는 모두 하나의 그래프로 이루어져 있습니다.</p>
</blockquote>

<h2 id="유니온-파인드">유니온 파인드</h2>
<pre><code>#define MAX 
int parent[201];
int find(int n) {
	if (parent[n] == 0) return n;
	else {
		int p = find(parent[n]);
		parent[n] = p;
		return p;
	}
}

void merge(int a, int b) {
	int ap = find(a);
	int bp = find(b);
	if (ap == bp) return;
	parent[bp] = ap;
}
</code></pre>

<h2 id="메인-함수">메인 함수</h2>
<pre><code>#include &lt;stdio.h&gt;
int main() {

	int N, M;
	scanf("%d %d", &amp;N, &amp;M);

	// input
	int isConn;
	for (int a = 1; a &lt;= N; a++) {
		for (int b = 1; b &lt;= N; b++) {
			scanf("%d", &amp;isConn);
			if (isConn == 1) {
				merge(a, b);
			}
		}
	}

	// check if root same
	int n, np, prevnp, isYES;
	for (int i = 0; i &lt; M; i++) {
		scanf("%d", &amp;n);
		// find root
		np = find(n);
		if (i &gt; 0) {
			// check if roots same
			isYES = (prevnp == np);
			if (isYES == 0)
				break;
		}
		prevnp = np;
	}

	
	// output
	if (isYES) {
		printf("YES\n");
	} else {
		printf("NO\n");
	}
	return 0;
}
</code></pre>

    <article>
    <div class="post-more">
      
      <a href="/algorithm/2018/12/29/boj-go-trip-1976/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/algorithm/2018/12/29/boj-go-trip-1976/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/blog/page2">Older</a>
  
  
    <span class="pagination-item newer">Newer</span>
  
</div>


      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if (target === toggle) {
            checkbox.checked = !checkbox.checked;
            e.preventDefault();
          } else if (checkbox.checked && !sidebar.contains(target)) {
            /* click outside the sidebar when sidebar is open */
            checkbox.checked = false;
          }
        }, false);
      })(document);
    </script>
    
    <script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-130716162-1', 'auto');
ga('send', 'pageview');
    </script>
    
  </body>
  
  <script id="dsq-count-scr" src="//hijigoo.disqus.com/count.js" async></script>
  
</html>
