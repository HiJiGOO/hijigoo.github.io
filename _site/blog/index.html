<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Blog &middot; 안녕지구
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/main.css">
  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.ico?v=2">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">


  <!-- Web Font -->
  <!-- <link href="https://fonts.googleapis.com/css?family=Nanum+Gothic" rel="stylesheet"> -->
  <link href="https://fonts.googleapis.com/css?family=Nanum+Gothic|Nanum+Gothic+Coding|Noto+Sans+KR" rel="stylesheet">

  <!-- Highlighter -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/styles/atom-one-dark.min.css" />
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.11.0/highlight.min.js"></script>
  <script>
    hljs.initHighlightingOnLoad();
    hljs.configure({tabReplace: '    '})
  </script>

  <!-- Custom -->
  <link rel="stylesheet" href="/public/css/custom.css">

  <!-- MathJax -->
  

</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <div class="sidebar-personal-info">
      <!-- 
      <div class="sidebar-personal-info-section">
        <a href="http://gravatar.com/">
          <img src="http://www.gravatar.com/avatar/?s=350" title="View on Gravatar" alt="View on Gravatar" />
        </a>
      </div>
      -->
      <div class="sidebar-personal-info-section">
        <p>안녕하세요. 기록하는 개발자입니다.</p>
      </div>
      
      
      
      <div class="sidebar-personal-info-section">
        <p> Follow me: 
        
        
        
        <a href="https://github.com/HiJiGOO">
          <i class="fa fa-github" aria-hidden="true"></i>
        </a>
        
        |
        
        
        
        <a href="mailto:hi.jigoo@gmail.com">
          <i class="fa fa-envelope" aria-hidden="true"></i>
        </a>
        
        
        
        </p>
      </div>
      
    </div>
  </div>

  <nav class="sidebar-nav">
    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="/">
          Home
        </a>

        
      </span>

    
      
      
      

      

      <span class="foldable">
        <a class="sidebar-nav-item " href="/blog/categories/">
          Categories
        </a>

        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/categories/#algorithm">
                <!-- Algorithm -->
                
                Algorithm&nbsp
                (
                  
                  
                  11
                )
              </a>
          
        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/categories/#bixby">
                <!-- Bixby -->
                
                Bixby&nbsp
                (
                  
                  
                  7
                )
              </a>
          
        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/categories/#NLP">
                <!-- NLP -->
                
                NLP&nbsp
                (
                  
                  
                  0
                )
              </a>
          
        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/categories/#reinforcement-learning">
                <!-- Reinforcement Learning -->
                
                Reinforcement Learning&nbsp
                (
                  
                  
                  2
                )
              </a>
          
        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/categories/#daily">
                <!-- Daily -->
                
                Daily&nbsp
                (
                  
                  
                  0
                )
              </a>
          
        
      </span>

    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="/blog/tags/">
          Tags
        </a>

        
      </span>

    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="/about/">
          About
        </a>

        
      </span>

    

    <!-- <span class="sidebar-nav-item">Currently v1.0.0</span> -->
  </nav>

  <div class="sidebar-item">
    <p>
    &copy; 2019 안녕지구. This work is liscensed under <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a>.
    </p>
  </div>


  <div class="sidebar-item">
    <p>
    Powered by <a href="http://jekyllrb.com">jekyll</a>
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home" title="안녕지구">
              <!-- <img class="masthead-logo" src="/public/logo.png"/> -->
              안녕지구
            </a>
            <small>#developer #bompapa</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/reinforcement%20learning/2019/01/02/rl-multi-armed-bandits/">
        강화학습 정리 - Multi-armed Bandits (업데이트중)
      </a>
    </h1>

    <span class="post-date">02 Jan 2019</span>
     | 
    
    <a href="/blog/tags/#강화학습" class="post-tag">강화학습</a>
    
    <a href="/blog/tags/#rl" class="post-tag">RL</a>
    
    

    <article>
      <h2 id="2-multi-armed-bandits">2. Multi-armed Bandits</h2>

<p>강화학습이 다른 딥러닝과 구분되는 가장 중요한 특징은 선택한 action에 대해 <strong>평가(evaluate)</strong>를 한다는 것이다. 이런 피드백(feedback)은 얼마나 좋은지에 대한 평가이지 정답인지 아닌지를 알려주는 것은 아니다. 이번 챕터에서는 간단한 환경에서 강화학습의 평가(evaluate)에 중점을 두고 공부할 것이다. 오직 단 하나의 상황에서만 evaluate하기 때문에 <em>full reinforcement learning problem</em>의 복잡성을 피할 수 있다. 여기서 다룰 문제는 <strong>k-armed bandit problem</strong>인데 이를 소개하면서 강화학습의 주요한 요소 몇가지도 함께 다룰 것이다.</p>

<div class="message">
이번 장에서는 단순화된 환경(nonassociative setting)에서 설명하기 때문에, 강화학습에서 처음에 많이 다루는 'frozen lake'와 비교하면서 보시면 헷갈리실 수 있습니다. 처음에는 'Multi-armed Bandits'를 독립적인 문제로 보고 접근하시는 것을 추천드립니다.
</div>

<hr />

<h3 id="a-k-armed-bandit-problem">A k-armed Bandit Problem</h3>

<p>A k-armed Bandit Problem은 k개의 레버가 있는 슬롯머신에서 최대의 reward를 받기 위한 문제다. 내용은 아래와 같다.</p>

<ol>
  <li>k개의 다른 option이나 action중에서 하나를 선택한다.</li>
  <li><em>stationary probability distribution</em>으로 부터 하나의 reward를 받는다.</li>
  <li>최종 목표는 일정 기간 동안 전체 reward를 최대화 하는 것이다.</li>
</ol>

<p>위 k-armed bandit problem에서 k action을 선택할 때마다 reward를 받는데, 이때 rewards의 기댓값(expectation)을 선택된 action의 <strong>value</strong>라고 한다. 식으로 나타내면 아래와 같다.</p>

<script type="math/tex; mode=display">q_*(a)\doteq \mathbb{E}[R_t | A_t=a]</script>

<ul>
  <li>$A_t$: time step가 $t$ 일 때 선택된 action</li>
  <li>$R_t$: time step가 $t$ 일 때 $A_t$에 대한 reward</li>
  <li>$q_*(a)$: action $a$ 가 선택됐을 때 받는 reward의 기댓값</li>
</ul>

<p>만약 우리가 각 action에 대한 value를 알고 있다면 k-armed bandit problem을 해결하는 것은 매우 쉬울 것이다. 매번 value가 가장 높은 action을 선택하면 되기 때문이다. 그런데 처음에는 value을 알 수 없기 때문에 계속해서 <strong>estimate</strong>해야 한다. 이렇게 time step $t$에서 선택된 action $a$의 value를 $Q_t(a)$라고 한다. 우리는 $Q_t(a)$가  $q_*(a)$에 가까워 지도록 계속해서 estimate 해야 한다.</p>

<p>action value를 계속해서 estimate 한다면, time step 마다 적어도 한 개 이상의 가장 높은 value를 갖는 action이 있을 것이다. 그 action을 <em>greedy</em> action 이라고 한다. 만약 이 greedy action 중 하나를 선택한다면 이를 <strong><em>exploiting</em></strong> 한다고 얘기한다. 반대로 <em>greedy</em> action이 아닌 다른 action 중 하나를 선택한다면 <strong><em>exploring</em></strong>한다고 얘기한다. 당장 한 스텝만 바라봤을 때는 <strong><em>exploitation</em></strong>이 value를 최대화 하는 방법일지 몰라도, 장기적인 관점에서 바라봤을 때 <strong><em>exploration</em></strong>의 total reward가 더 높을 수 있다.</p>

<p><em>exploration</em> 와 <em>exploitation</em> 의 균형을 맞춰 나가기 위해서 복잡한 수학적인 방법들이 존재하지만, 많은 가정들이 전제해야 하기 때문에 사실상 full reinforcment learning 문제에 적용하기는 불가능하다. 하지만, 이를 해결하기 위한 간단한 방법들이 존재하고 나중에 다룰 것이다.</p>

<hr />

<h3 id="action-value-methods">Action-value Methods</h3>

<p>여기서는 action의 value을 estimate하는 방법(method)에 대해 더 자세하게 알아볼 것이다. 우리는 이것을 <strong><em>action-value methods</em></strong> 라고 부르는데 action을 선택하기 위해서도 사용된다. 그리고 action이 선택될 때마다 계산한 reward의 평균을 <em>action의 true value</em>라고 한다. 수식은 아래와 같다.</p>

<script type="math/tex; mode=display">Q_t(a)\doteq \frac{\text{sum of rewards when $a$ taken prior to $t$}}{\text{number of times $a$ taken prior to $t$}} = \frac{\sum_{i=1}^{t-1}R_i \cdot \mathbb{1}_{A_i=a}}{\sum_{i=1}^{t-1}\mathbb{1}_{A_i=a}}</script>

<ul>
  <li>$\mathbb{1}_{predicate}$: $predicate$가 true 이면 1이고 false이면 0이다.</li>
</ul>

<p>위의 식에서 $\mathbb{1}_{A_i=a}$은 $a$가 선택된 경우만 계산하겠다는 의미다. 그리고 한 번도 $a$가 선택된 적이 없는 경우 분모가 0이 되어서 계산이 불가능하기 때문에 $Q_t(a)$를 기본 값(예: 0)으로 대신한다. 사실 이렇게 평균을 내는 것은 action value를 estimate하는 여러가지 방법 중 하나다. 우리는 이것을 <em>sample-average</em>라고 부를 것이다.</p>

<p>action을 선택하는 가장 간단한 방법 중 하나는 당연히 estimate value가 가장 큰 것(<em>greedy action</em>)을 선택하는 것이다. 만약 두 개 이상의 <em>greedy</em> action이 있다면 그 중 아무거나 선택하면 된다. 우리는 이런 방법을 <strong><em>greedy</em> action selection</strong> 이라고 하며, 아래와 같이 정의할 수 있다.</p>

<script type="math/tex; mode=display">A_t \doteq \underset{a}{argmax}Q_t(a)</script>

<p>위에서 $argmax_a$는 뒤에 따라오는 $Q_t(a)$를 최대화(maximized)해주는 action(a)을 선택한다는 뜻이다. <em>greedy</em> action selection은 지금 알고 있는 정보를 기반으로 <em>눈 앞에 보이는</em> reward를 최대화하기 위해서 항상 <strong>exploit</strong> 한다. 하지만 이런 방식은 장기적 관점에서 봤을 때 더 좋은 action을 놓칠 수 있다. 이런 단점을 보안하기 위해서 <strong>아주 적은 확률($\varepsilon$)</strong>로 action중 하나를 랜덤으로 선택하는 방법이 있는데, 우리는 이 방법을 <strong>$\varepsilon-greedy$</strong> method 라고 한다. 이런 방법의 장점은 모든 action들이 sampling될 수 있기 때문에 $Q_t(a)$ 가 $q_*(a)$로 수렴된다는 것이다.</p>

<hr />

<h3 id="the-10-armed-testbed">The 10-armed Testbed</h3>

<p><strong><em>k</em>-armed bandit problems</strong>으로 <em>greedy action-value method</em>(<strong>greedy method</strong>)와 <em>$\varepsilon$-greedy action-value method</em>(<strong>$\varepsilon$-greedy method</strong>) 두 가지 방법을 비교 했다. 아래는 <em>k</em>가 10개인 <em>k</em>-armed bandit problems의 reward 분포다. 분산(variance)이 1이고 평균(mean)이 $q_*(a)$인 정규 분포(normal distribution)다. 이 분포로 부터 $t$ (time step)에서 $A_t$(action)를 선택했을 때 $R_t$(reward)를 얻는다.</p>

<p><img src="/assets/2019-01-02-rl-bulti-armed-bandits/figure2_1.png" alt="image" /></p>

<p>분산(variance)이 1이고 평균(mean)이 0인 정규 분포(normal distribution)에서 1000번의 action을 선택했고, 이렇게 2000번 반복한 평균으로 성능을 측정했다.</p>

<p>아래는 <strong>greedy method</strong>와 두개의 <strong>$\varepsilon$-greedy method</strong>($\varepsilon$=0.01 과 $\varepsilon$=0.1)로 테스트한 결과다. 자세히 보면 처음에는 greedy method가 더 빠르게 향상되는 것처럼 보이나 시간이 지날수록 $\varepsilon$-greedy method가 더 향상되는 것을 볼 수 있다. greedy method는 시간이 지날수록 더디게 향상되는데, 이는 suboptimal에 빠질 수 있기 때문이다. 아래 그래프를 보면 greedy method는 optimal action의 1/3 정도 밖에 도달하지 못했다. 반면에, $\varepsilon$-greedy method는 계속해서 explore하고 optimal action을 찾기 위한 가능성을 높혔기 때문에 최종적으로 더 나은 성능을 보였다.</p>

<p><img src="/assets/2019-01-02-rl-bulti-armed-bandits/figure2_2.png" alt="image" /></p>

<p>$\varepsilon$-greedy method의 이점은 task에 따라서 다르다. 만약에 분산(variance)이 매우 크다면(예: 10) optimal policy를 찾기 위해서 더 많은 exploration를 해야한다. 이럴 경우 $\varepsilon$-greedy method가 더 좋은 성능을 보인다. 하지만 만약 분산(variance)이 0이라면 단 한 번만 시도 해보고 true value를 알 수 있기 때문에 exploration없이도 optimal policy를 찾을 수 있을 것이다. 하지만 이런 <strong>deterministic</strong>한 경우라도 일부 가정이 불확실 하다면 exploration은 필요하다. 예를 들어서 bandit problem이 <strong>nonstationary</strong>하다면 true value는 시간이 지남에 따라 변경되기 때문에 exploration이 필요하다. nonstationary 문제는 reinforcement learning에서 자주 나타나는 상황이다. 이와 같이 reinforcement learning에서 exploration과 exploitation의 균형은 매우 중요하다.</p>

<hr />

<h3 id="incremental-implementation">Incremental Implementation</h3>
<p>지금까지 논의한 <strong>action-value method</strong>는 얻은 rewards의 평균(sample averages)을 내어서 estimate 하였다. 이번에는 이렇게 매번 평균을 내는 것보다 더 효율적인 방법에 대해 알아볼 것이다.</p>

<p>복잡한 식를 피하기 위해서 하나의 action에 대해서만 다룬다. action value 식을 간단하게 표기하면 아래와 같은데, $R_i$는 i번째 action이 선택된 후에 받은 reward이고, $Q_n$은 n-1번째 까지 측정된 action value 이다.</p>

<script type="math/tex; mode=display">Q_n \doteq \frac{R_1 + R_2 + \dots + R_{n-1}}{n-1}</script>

<p>위의 방식은 모든 reward를 보관하고 있다가 value를 estimate할 때마다 계산한다. 하지만 이런 방식은 시간이 지나고 reward를 받을 때마다 더 많은 메모리와 연산을 필요로 한다. 예상할 수 있듯이, 이는 비효율적이기 때문에 평균(average)를 업데이트 하는 다른 방법이 필요하다. 이미 계산된 $Q_n$과 n번째 reward $R_n$이 주어지면 모든 reward의 평균을 아래와 같이 <strong>incremental formulas</strong>로 계산할 수 있다.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
Q_n &= \frac{1}{n}\sum_{i=1}^{n}R_i \\
    &= \frac{1}{n}\left(R_n + \sum_{i=1}^{n-1}R_i\right) \\
    &= \frac{1}{n}\left(R_n + (n-1)\frac{1}{n-1}\sum_{i=1}^{n-1}R_i\right) \\
    &= \frac{1}{n}\left(R_n + (n-1)Q_n\right) \\
    &= \frac{1}{n}\left(R_n + nQ_n - Q_n\right) \\
    &= Q_n + \frac{1}{n}\left[R_n - Q_n\right]
\end{align} %]]></script>

<p>위의 연산은 새로운 reward를 얻더라도 $Q_n$과 $n$ 그리고 (6)정도의 간단한 연산만 필요하다. 위의 update 식은 일반적으로 아래와 같이 표기한다.</p>

<script type="math/tex; mode=display">NewEstimate \leftarrow OldEstimate + StepSize [Target - OldEstimate]</script>

<p>위 식에서 $[Target - OldEstimate]$가 estimate에서 <strong>error</strong>이며, $Target$에 가까워질수록 작아진다. target은  우리가 원하는 방향이며 여기서는 n번째 reward다.</p>

<p>참고로, <strong>incremental method</strong> (6)에서 사용된 step-size parameter(StepSize)는 time step이 지날 수록 $\frac{1}{n}$으로 변경된다. 우리는 앞으로 이 <strong>step-size</strong>를 $\alpha$ 혹은 $\alpha_t(a)$로 표기할 것이다.</p>

<p>아래는 incrementally computed sample averages 와 $\varepsilon$-greedy action selection을 사용한 A Simple bandit algorithm의 슈도코드(Pseudocode)다.</p>

<p><img src="/assets/2019-01-02-rl-bulti-armed-bandits/pseudocode_1.png" alt="image" /></p>

<hr />

<h3 id="tracking-a-nonstationary-problem">Tracking a Nonstationary Problem</h3>

<hr />

<h3 id="optimistic-initial-values">Optimistic Initial Values</h3>

<hr />

<h3 id="summary">Summary</h3>

<hr />

<h3 id="reference">Reference</h3>
<ul>
  <li><a href="http://incompleteideas.net/book/the-book.html">Reinforcement Learning: An Introduction - Richard S. Sutton and Andrew G. Barto</a></li>
</ul>

    <article>
    <div class="post-more">
      
      <a href="/reinforcement%20learning/2019/01/02/rl-multi-armed-bandits/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/reinforcement%20learning/2019/01/02/rl-multi-armed-bandits/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/reinforcement%20learning/2019/01/01/rl-introduction/">
        강화학습 정리 - Introduction
      </a>
    </h1>

    <span class="post-date">01 Jan 2019</span>
     | 
    
    <a href="/blog/tags/#강화학습" class="post-tag">강화학습</a>
    
    <a href="/blog/tags/#rl" class="post-tag">RL</a>
    
    

    <article>
      <h2 id="1-introduction">1. Introduction</h2>

<p>우리가 배움의 본질에 대해 생각할 때 가장 먼저 떠올리는 것 중 하나는 <strong>환경(environment)</strong>과 상호작용 하면서 배우는 것이다. 예를 들어서 아기들은 가르쳐주는 사람이 없더라도 스스로 팔을 흔들고, 노는 행동을 하고, 주위를 바라보는 방법 등을 터득한다. 아기는 자신과 연결된 환경(environment)과 상호작용하며 배우는데 필요한 정보를 습득하는 것이다.</p>

<hr />

<h3 id="reinforcement-learning">Reinforcement Learning</h3>

<p>강화학습(Reinforcement Learning)은 현재 자신에게 주어진 환경에서 어떤 <strong>행동(action)</strong>을 해야 하는지 배우는 것이다. 그리고 이 행동(action)은 <strong>보상(reward)</strong>을 최대로 얻을 수 있는 방법을 따른다.</p>

<p>여기서 흥미로운 점은 내가 선택하는 행동(action)이 당장 받을 보상(immediate reward)뿐 아니라 다음 상황(state)과 나중에 받을 보상(subsequent rewards)에도 영향을 미친다는 것이다. 즉, 행동(action)을 선택할 때 나중에 받을 보상도 고려를 해야 하는 것이다.</p>

<p>또한, 강화학습이 받는 challenge 중 하나는 <strong>exploitation</strong> 과 <strong>exploration</strong>의 trade-off 이다. 행동(action)을 선택할 때 두 가지 접근 방법 중에서 하나를 선택해야 하기 때문이다.</p>

<ul>
  <li><strong>exploitation</strong>: 이미 학습한 정보를 토대로 최대 reward를 얻는 action을 선택한다.</li>
  <li><strong>exploration</strong>: 새로운 정보를 학습하기 위해서 여러가지 action을 선택해본다.</li>
</ul>

<p>아래는 교재에서 설명된 내용이다.</p>
<blockquote>
  <p>The agent has to <em>exploit</em> what it has already experienced in order to obtain reward, but it also has to <em>explore</em> in order to make better action selections in the future.</p>
</blockquote>

<p>마지막으로, 강화학습에서 <strong>학습하는 대상(agent)</strong>은 <strong>목표 지향적(goal-directed)</strong>이며 불확실한 환경(uncertain environment)에서도 모든 문제를 고려해야 한다.</p>

<hr />

<h3 id="elements-of-reinforcement-learning">Elements of Reinforcement Learning</h3>

<p>위에서 설명 했듯이 강화학습에서는 <strong>agent</strong>는 <strong>environment</strong>와 상호작용 하면서 학습하는데, 이 시스템에서 중요한 네가지 요소를 아래와 같이 정의할 수 있다.</p>

<ul>
  <li><strong>Policy</strong>: 주어진 environment의 state에서 agent가 선택해야 할 action을 정의</li>
  <li><strong>Reward signal</strong>: agent가 action을 선택했을 때 environment로 부터 받는 값</li>
  <li><strong>Value function</strong>: agent가 특정 state에서 앞으로 받을 모든 rewards의 합에 대한 기댓값을 정의</li>
  <li><strong>Model</strong>: environment의 동작(behavior)에 대해 정의</li>
</ul>

<p><strong>Policy</strong>는 현재의 내 상태(states of the environment)에서 취해야 할 action을 정의 하는데, 간단한 function이나 lookup table 일 수도 있다. 일반적으로 확률론(stochastic)적 이며 각 action에 대한 확률(probabilities)를 가지고 있다.</p>

<p><strong>Reward signal</strong>은 agent가 action을 선택 했을 때 즉각적으로 environment로 부터 받는 보상 값으로, 강화학습에서 agent의 유일한 목표는 이 reward의 총 합을 최대화하는 것이다. reward는 policy를 정의하기 위한 기본 요소다.</p>

<p><strong>Value function</strong>은 길게 보았을 때 agent가 특정 state에서 받을 reward의 총 합에 대한 기댓값을 정의한다. agent가 주어진 state에서 특정 action을 선택했을 때 당장 받을 reward는 다른 action을 취했을 때 보다 상대적으로 낮더라도 최종적으로 누적될 reward는 가장 높을 수도 있다.</p>

<p><strong>Model</strong>은 environment의 동작(behavior)에 대해 정의한다. 즉, model을 알고 있으면 state와 action이 주어졌을 때의 가능한 결과를 미리 예상할 수 있다. model은 agent가 직접 경험해 보기도 전에 <em>계획(planning)</em>을 세우는데 사용된다. 문제를 해결하기 위해서 model과 planning을 사용하는 방법을 <em>model-based</em>라고 한다. 반대로 agent가 model을 모르는 상태에서 <em>경험을 하면서(trial-and-error)</em> 문제를 해결하는 방법을 <em>model-free</em>라고 한다.</p>

<hr />

<h3 id="summary">Summary</h3>

<p>강화학습은 agent가 environment와 상호작용 하면서 학습한다는 점에서 다른 학습 방법과는 구분된다. 그리고 states, actions, rewards를 포함한 environment와 agent의 관계를 정의하기 위해서 <strong><a href="https://en.wikipedia.org/wiki/Markov_decision_process">Markov decision processes(MDP)</a></strong>라는 framework 를 사용한다. <strong>value</strong> 와 <strong>value function</strong> 컨셉은 강화학습에서 매우 중요하다. agent는 policy에 따라서 action을 취하는데, value는 policy를 정의하는데 근간이 되기 때문이다.</p>

<hr />

<h3 id="reference">Reference</h3>
<ul>
  <li><a href="http://incompleteideas.net/book/the-book.html">Reinforcement Learning: An Introduction - Richard S. Sutton and Andrew G. Barto</a></li>
</ul>

    <article>
    <div class="post-more">
      
      <a href="/reinforcement%20learning/2019/01/01/rl-introduction/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/reinforcement%20learning/2019/01/01/rl-introduction/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/algorithm/2018/12/29/boj-go-trip-1976/">
        BOJ - 여행 가자(1976)
      </a>
    </h1>

    <span class="post-date">29 Dec 2018</span>
     | 
    
    <a href="/blog/tags/#boj" class="post-tag">BOJ</a>
    
    <a href="/blog/tags/#백준" class="post-tag">백준</a>
    
    <a href="/blog/tags/#알고리즘" class="post-tag">알고리즘</a>
    
    <a href="/blog/tags/#자료구조" class="post-tag">자료구조</a>
    
    

    <article>
      <p>경로를 찾는 문제처럼 보이지만, <strong>유니온 파인드(Union-Find)</strong>를 사용해서 해결할 수 있는 간단한 문제입니다. 입력으로 들어온 각 두 도시를 연결 하고, 마지막 입력으로 주어진 도시들이 모두 연결되어 있는지 확인하는 문제입니다. 주어진 도시들의 root 가 모두 같으면 하나의 그래프로 연결되어 있다고 볼 수 있습니다.</p>

<blockquote>
  <p>[A-B], [B-C], [A-D] 이면 A, B, C, D 는 모두 하나의 그래프로 이루어져 있습니다.</p>
</blockquote>

<h2 id="유니온-파인드">유니온 파인드</h2>
<pre><code>#define MAX 
int parent[201];
int find(int n) {
	if (parent[n] == 0) return n;
	else {
		int p = find(parent[n]);
		parent[n] = p;
		return p;
	}
}

void merge(int a, int b) {
	int ap = find(a);
	int bp = find(b);
	if (ap == bp) return;
	parent[bp] = ap;
}
</code></pre>

<h2 id="메인-함수">메인 함수</h2>
<pre><code>#include &lt;stdio.h&gt;
int main() {

	int N, M;
	scanf("%d %d", &amp;N, &amp;M);

	// input
	int isConn;
	for (int a = 1; a &lt;= N; a++) {
		for (int b = 1; b &lt;= N; b++) {
			scanf("%d", &amp;isConn);
			if (isConn == 1) {
				merge(a, b);
			}
		}
	}

	// check if root same
	int n, np, prevnp, isYES;
	for (int i = 0; i &lt; M; i++) {
		scanf("%d", &amp;n);
		// find root
		np = find(n);
		if (i &gt; 0) {
			// check if roots same
			isYES = (prevnp == np);
			if (isYES == 0)
				break;
		}
		prevnp = np;
	}

	
	// output
	if (isYES) {
		printf("YES\n");
	} else {
		printf("NO\n");
	}
	return 0;
}
</code></pre>

    <article>
    <div class="post-more">
      
      <a href="/algorithm/2018/12/29/boj-go-trip-1976/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/algorithm/2018/12/29/boj-go-trip-1976/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/bixby/2018/12/28/basic_tutorial_4/">
        주사위 프로젝트 - View 및 Dialog 작성
      </a>
    </h1>

    <span class="post-date">28 Dec 2018</span>
     | 
    
    <a href="/blog/tags/#인공지능" class="post-tag">인공지능</a>
    
    <a href="/blog/tags/#빅스비" class="post-tag">빅스비</a>
    
    

    <article>
      <h2 id="view-작성">View 작성</h2>

<p>안녕하세요! 이전 포스트에서는 View를 작성하지 않아 테이블 형태로 결과를 보여주었는데요, 이번 포스트에서는 View를 작성해서 사용자에게 결과 화면을 더 이쁘게(?!) 보여주겠습니다. 자 그럼 아래와 같이 파일을 생성하고 코드를 작성합니다.</p>

<hr />

<p><em>/resources/ko/RollResult.view.bxb</em></p>
<pre><code>result-view {
 match {
   RollResult (rollResult)
 }

 render {
   layout {
     section {
       content {
         single-line {
           text {
             style (Detail_M)
             value ("Sum: #{value(rollResult.sum)}")
           }
         }
         single-line {
           text {
             style (Detail_M)
             value ("Rolls: #{value(rollResult.roll)}")
           }
         }
       }
     }
   }
 }
}
</code></pre>

<hr />

<div class="message">
'ko' 폴더는 단말에서 빅스비 언어를 한국어로 설정했을 때 반영됩니다. 현재는 전체 코드를 나눠놨지만 전체 구조는 공유하고 단어나 문장이 출력되는 부분만 template으로 따로 빼서 Localization을 할 수 있습니다.
</div>

<p>위 코드에서 <strong>match</strong> 부분은 action의 output과 연결됩니다. 코드를 풀이해보면 어떤 action의 output이 RollResult라면 현재 view에서 보여주겠다는 뜻입니다. 자세한 <a href="https://bixbydevelopers.com/dev/docs/dev-guide/developers/customizing-plan.match-patterns"><strong>match patterns</strong></a> 는 공식 홈페이지에서 확인하실 수 있습니다.</p>

<p>그리고 아래 <strong>render</strong>에 화면에 표시해주는 코드를 작성합니다. 화면에 따라서 용도에 맞는 component를 사용해서 작성하실 수 있습니다. 지금은 깊게 다루지 않기 때문에 전체 흐름만 파악하시면 될 것 같습니다. 자세한 내용은 공식 홈페이지의 <a href="https://bixbydevelopers.com/dev/docs/dev-guide/developers/building-views"><strong>Building Views</strong></a> 를 참고 부탁드립니다.</p>

<p>코드 중간에 보면 <strong>#{value(rollResult.sum)}</strong>와 같이 <strong>#{}</strong>로 작성된 부분을 볼 수 있는데요. 이 부분은 빅스비의 <a href="https://bixbydevelopers.com/dev/docs/dev-guide/developers/customizing-plan.using-el"><strong>Expression Language</strong></a>로 다양한 연산자를 View의 String 내부에서 사용할 수 있게 도와줍니다.</p>

<p>이제 아래와 같이 intent를 보내서 화면에 어떻게 나오는지 확인할 수 있습니다.</p>

<pre><code>intent {
  goal: RollResult
  value: NumSides (6)
  value: NumDice (2)
}
</code></pre>

<p>다음은 결과 화면입니다.
<img src="/assets/2018-12-28-basic_tutorial_4/result.png" alt="image" width="50%" height="50%" /></p>

<p>현재는 기본 Dialog로 <strong><em>“검색 완료! 확인해보세요.”</em></strong> 라고 나옵니다.</p>

<h2 id="dialog-작성">Dialog 작성</h2>

<p>이제 기본 Dialog를 변경해서 원하는 문장을 출력해봅시다. 결과물을 보여주는 화면을 만들기 전에 아래와 같이 Dialog 파일을 생성하고 코드를 작성합니다.</p>

<hr />

<p><em>/resources/ko/NumSides.dialog.bxb</em></p>
<pre><code>dialog (Concept) {
 match {
   // Look for this type
   NumSides
 }
 // Use the following template text with this type
 template("면의 수")
}
</code></pre>

<p><em>/resources/ko/NumDice.dialog.bxb</em></p>
<pre><code>dialog (Concept) {
 match {
   // Look for this type
   NumDice
 }
 // Use this template text with this type
 template("주사위 수")
}
</code></pre>

<hr />

<p>참고로, 위의 두 dialog 모드(mode) concept입니다. 이 모드는 <a href="https://bixbydevelopers.com/dev/docs/reference/ref-topics/dialog-modes.dialog-fragments#concept-fragment">Dialog Fragment</a>중 하나로, 필요한 concept을 intent에 넣지 않고 보냈을 경우 아래와 같이 응답을 생성하는데 사용됩니다.</p>

<blockquote>
  <p>계속하려면 <strong>주사위 수</strong>가 필요해요</p>
</blockquote>

<p>다음과 같이 NumDice를 제외한채 intent를 보내다보면 확인하실 수 있습니다.</p>

<pre><code>intent {
  goal: RollResult
  value: NumSides (6)
}
</code></pre>

<p>자, 이제 최종 결과물을 보여주는 Dialog를 작성합시다. 아래와 같이 파일을 만들면 됩니다.</p>

<hr />

<p><em>/resources/ko/RollResult.dialog.bxb</em></p>
<pre><code>dialog (Result) {
 // bind the variable "rollResult" to the result and "rollOutput" to
 // the action of which it was output
 match {
   RollResult (rollResult) {
     from-output: RollDice (rollOutput)
   }
 }
 // define a condition that changes the dialog depending on whether there
 // is one or more dice
 if (rollOutput.numDice == 1) {
   template ("주사위의 번호는 ${value(rollResult.roll)}입니다!")    
 }
 if (rollOutput.numDice &gt; 1) {
   choose (Random) {
     template ("주사위의 총 합은 ${value(rollResult.sum)}입니다.")
     template ("각 주사위의 번호는 ${list(rollResult.roll, 'value')}입니다.")    
   }
 }
}
</code></pre>

<hr />

<p>전체적으로 View코드와 유사한데요, 조금 다른 점이 있습니다. 바로 <strong><em>from-output</em></strong> 인데요. RollDice의 output인 RollResult와 매칭을 시키겠다는 뜻입니다. 사실 이 프로젝트에서는 action이 유일하기 때문에 view와 매칭은 되는데요, 아래 보면 RollDice의 변수로 지정한 <strong>rollOutput</strong>을 이용하는 코드가 존재하기 때문에 유지해야합니다. rollOutput을 을 이용해서 RollDice의 input 값에 접근할 수 있습니다. <strong>choose(Random)</strong> 는 template 중 한가지를 임의 선택합니다.</p>

<div class="message">
같은 질문에 매번 같은 대답만 한다면 인공지능처럼 느껴지지 않을 것입니다. 또한 사용자 입장에서 재미도 없을 겁니다. 그래서 다양한 응답 varidation을 주기 위해서 choose(Random)은 자주 사용하는 키워드입니다.
</div>

<p>이제 다시 아래 intent를 보내면 최종 결과물을 확인할 수 있습니다.</p>

<pre><code>intent {
  goal: RollResult
  value: NumSides (6)
  value: NumDice (2)
}
</code></pre>

<p>다음은 View와 Dialog가 적용된 화면입니다.
<img src="/assets/2018-12-28-basic_tutorial_4/result2.png" alt="image" width="50%" height="50%" /></p>

<p>여기까지 진행 하셨으면 Capsule의 기본적인 부분은 완성되었습니다. 다음 포스트에서는 Default Value 넣는 방법에 대해 다루겠습니다. 감사합니다.</p>

    <article>
    <div class="post-more">
      
      <a href="/bixby/2018/12/28/basic_tutorial_4/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/bixby/2018/12/28/basic_tutorial_4/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/algorithm/2018/12/27/boj-corporative-network-3780/">
        BOJ - 네트워크 연결(3789)
      </a>
    </h1>

    <span class="post-date">27 Dec 2018</span>
     | 
    
    <a href="/blog/tags/#boj" class="post-tag">BOJ</a>
    
    <a href="/blog/tags/#백준" class="post-tag">백준</a>
    
    <a href="/blog/tags/#알고리즘" class="post-tag">알고리즘</a>
    
    <a href="/blog/tags/#자료구조" class="post-tag">자료구조</a>
    
    

    <article>
      <p><strong>Disjoint Set</strong> 문제이며 <strong>유니온 파인드(Union-Find)</strong>를 사용해서 해결할 수 있는 문제입니다. 중요한 부분은 find를 할 때 기존 경로 압축(path compression)을 하면서 거리도 업데이트 해줘야 합니다. merge 할 때는 최상위 노드를 따로 찾을 필요 없이 입력으로 들어온 두 노드를 바로 연결 해주면 됩니다.</p>

<div class="message">
경로 압축(path compression)을 하지 않고, 매번 재귀로 거리를 계산하면 Timeout이 발생합니다.
</div>

<h2 id="라인-길이">라인 길이</h2>
<pre><code>// define
#define MAX 200010

// distance to center
int dist[MAX];

// get distance
int distance(int a, int b) {
	int dist = a &gt; b ? a - b : b - a;
	return dist % 1000;
}
</code></pre>

<h2 id="유니온-파인드">유니온 파인드</h2>
<pre><code>int parent[MAX];

int find(int n) {
	if (parent[n] == 0) return n;
	int p = find(parent[n]);

	//// 길이 업데이트 ////
	dist[n] += dist[parent[n]];
	
	parent[n] = p;
	return p;
	
}

void merge(int a, int b) {
	dist[a] = distance(a, b);
	parent[a] = b;
}
</code></pre>

<h2 id="메인-함수">메인 함수</h2>
<pre><code>#include &lt;stdio.h&gt;
int main() {

	int T;
	scanf("%d", &amp;T);

	while(T--) {
		// initialize
		for (int i = 0; i &lt; MAX; i++) {
			dist[i] = 0;
			parent[i] = 0;
		}

		int N;
		scanf("%d", &amp;N);
		while(true) {

			char command;
			scanf(" %c", &amp;command);
			if (command == 'O') {
				break;
			}

			if (command == 'E') {
				int i;
				scanf("%d", &amp;i);
				find(i);
				printf("%d\n", dist[i]);
			} else {
				int i, j;
				scanf("%d %d", &amp;i, &amp;j);
				merge(i, j);
			};
		}
	}
	return 0;
}
</code></pre>

    <article>
    <div class="post-more">
      
      <a href="/algorithm/2018/12/27/boj-corporative-network-3780/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/algorithm/2018/12/27/boj-corporative-network-3780/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/blog/page2">Older</a>
  
  
    <span class="pagination-item newer">Newer</span>
  
</div>


      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if (target === toggle) {
            checkbox.checked = !checkbox.checked;
            e.preventDefault();
          } else if (checkbox.checked && !sidebar.contains(target)) {
            /* click outside the sidebar when sidebar is open */
            checkbox.checked = false;
          }
        }, false);
      })(document);
    </script>
    
    <script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-130716162-1', 'auto');
ga('send', 'pageview');
    </script>
    
  </body>
  
  <script id="dsq-count-scr" src="//hijigoo.disqus.com/count.js" async></script>
  
</html>
