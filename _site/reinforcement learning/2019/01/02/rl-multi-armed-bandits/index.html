<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      강화학습 정리 - Multi-armed Bandits (업데이트중) &middot; 안녕지구
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/main.css">
  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.ico?v=2">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">


  <!-- Web Font -->
  <!-- <link href="https://fonts.googleapis.com/css?family=Nanum+Gothic" rel="stylesheet"> -->
  <link href="https://fonts.googleapis.com/css?family=Nanum+Gothic|Nanum+Gothic+Coding|Noto+Sans+KR" rel="stylesheet">

  <!-- Highlighter -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/styles/atom-one-dark.min.css" />
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.11.0/highlight.min.js"></script>
  <script>
    hljs.initHighlightingOnLoad();
    hljs.configure({tabReplace: '    '})
  </script>

  <!-- Custom -->
  <link rel="stylesheet" href="/public/css/custom.css">

  <!-- MathJax -->
  
    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
      <!-- inlineMath: [ ['$','$'], ['\(', '\)'] ], -->
      inlineMath: [ ['$','$'] ],
      displayMath: [ ['$$','$$'] ],
      processEscapes: true,
    }
  });
</script>
<script type="text/javascript"
        src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">

</script>

  

</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <div class="sidebar-personal-info">
      <!-- 
      <div class="sidebar-personal-info-section">
        <a href="http://gravatar.com/">
          <img src="http://www.gravatar.com/avatar/?s=350" title="View on Gravatar" alt="View on Gravatar" />
        </a>
      </div>
      -->
      <div class="sidebar-personal-info-section">
        <p>안녕하세요. 기록하는 개발자입니다.</p>
      </div>
      
      
      
      <div class="sidebar-personal-info-section">
        <p> Follow me: 
        
        
        
        <a href="https://github.com/HiJiGOO">
          <i class="fa fa-github" aria-hidden="true"></i>
        </a>
        
        |
        
        
        
        <a href="mailto:hi.jigoo@gmail.com">
          <i class="fa fa-envelope" aria-hidden="true"></i>
        </a>
        
        
        
        </p>
      </div>
      
    </div>
  </div>

  <nav class="sidebar-nav">
    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="/">
          Home
        </a>

        
      </span>

    
      
      
      

      

      <span class="foldable">
        <a class="sidebar-nav-item " href="/blog/categories/">
          Categories
        </a>

        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/categories/#algorithm">
                <!-- Algorithm -->
                
                Algorithm&nbsp
                (
                  
                  
                  11
                )
              </a>
          
        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/categories/#bixby">
                <!-- Bixby -->
                
                Bixby&nbsp
                (
                  
                  
                  7
                )
              </a>
          
        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/categories/#NLP">
                <!-- NLP -->
                
                NLP&nbsp
                (
                  
                  
                  0
                )
              </a>
          
        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/categories/#reinforcement-learning">
                <!-- Reinforcement Learning -->
                
                Reinforcement Learning&nbsp
                (
                  
                  
                  2
                )
              </a>
          
        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/categories/#daily">
                <!-- Daily -->
                
                Daily&nbsp
                (
                  
                  
                  0
                )
              </a>
          
        
      </span>

    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="/blog/tags/">
          Tags
        </a>

        
      </span>

    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="/about/">
          About
        </a>

        
      </span>

    

    <!-- <span class="sidebar-nav-item">Currently v1.0.0</span> -->
  </nav>

  <div class="sidebar-item">
    <p>
    &copy; 2019 안녕지구. This work is liscensed under <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a>.
    </p>
  </div>


  <div class="sidebar-item">
    <p>
    Powered by <a href="http://jekyllrb.com">jekyll</a>
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home" title="안녕지구">
              <!-- <img class="masthead-logo" src="/public/logo.png"/> -->
              안녕지구
            </a>
            <small>#developer #bompapa</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="post">
  <h1 class="post-title">강화학습 정리 - Multi-armed Bandits (업데이트중)</h1>
  <span class="post-date">02 Jan 2019</span>
   | 
  
    <a href="/blog/tags/#강화학습" class="post-tag">강화학습</a>
  
    <a href="/blog/tags/#rl" class="post-tag">RL</a>
  
  
  <article>
    <h2 id="2-multi-armed-bandits">2. Multi-armed Bandits</h2>

<p>강화학습이 다른 딥러닝과 구분되는 가장 중요한 특징은 선택한 action에 대해 <strong>평가(evaluate)</strong>를 한다는 것이다. 이런 피드백(feedback)은 얼마나 좋은지에 대한 평가이지 정답인지 아닌지를 알려주는 것은 아니다. 이번 챕터에서는 간단한 환경에서 강화학습의 평가(evaluate)에 중점을 두고 공부할 것이다. 오직 단 하나의 상황에서만 evaluate하기 때문에 <em>full reinforcement learning problem</em>의 복잡성을 피할 수 있다. 여기서 다룰 문제는 <strong>k-armed bandit problem</strong>인데 이를 소개하면서 강화학습의 주요한 요소 몇가지도 함께 다룰 것이다.</p>

<div class="message">
이번 장에서는 단순화된 환경(nonassociative setting)에서 설명하기 때문에, 강화학습에서 처음에 많이 다루는 'frozen lake'와 비교하면서 보시면 헷갈리실 수 있습니다. 처음에는 'Multi-armed Bandits'를 독립적인 문제로 보고 접근하시는 것을 추천드립니다.
</div>

<hr />
<h3 id="a-k-armed-bandit-problem">A k-armed Bandit Problem</h3>

<p>A k-armed Bandit Problem은 k개의 레버가 있는 슬롯머신에서 최대의 reward를 받기 위한 문제다. 내용은 아래와 같다.</p>

<ol>
  <li>k개의 다른 option이나 action중에서 하나를 선택한다.</li>
  <li><em>stationary probability distribution</em>으로 부터 하나의 reward를 받는다.</li>
  <li>최종 목표는 일정 기간 동안 전체 reward를 최대화 하는 것이다.</li>
</ol>

<p>위 k-armed bandit problem에서 k action을 선택할 때마다 reward를 받는데, 이때 rewards의 기댓값(expectation)을 선택된 action의 <strong>value</strong>라고 한다. 식으로 나타내면 아래와 같다.</p>

<script type="math/tex; mode=display">q_*(a)\doteq \mathbb{E}[R_t | A_t=a]</script>

<ul>
  <li>$A_t$: time step가 $t$ 일 때 선택된 action</li>
  <li>$R_t$: time step가 $t$ 일 때 $A_t$에 대한 reward</li>
  <li>$q_*(a)$: action $a$ 가 선택됐을 때 받는 reward의 기댓값</li>
</ul>

<p>만약 우리가 각 action에 대한 value를 알고 있다면 k-armed bandit problem을 해결하는 것은 매우 쉬울 것이다. 매번 value가 가장 높은 action을 선택하면 되기 때문이다. 그런데 처음에는 value을 알 수 없기 때문에 계속해서 <strong>estimate</strong>해야 한다. 이렇게 time step $t$에서 선택된 action $a$의 value를 $Q_t(a)$라고 한다. 우리는 $Q_t(a)$가  $q_*(a)$에 가까워 지도록 계속해서 estimate 해야 한다.</p>

<p>action value를 계속해서 estimate 한다면, time step 마다 적어도 한 개 이상의 가장 높은 value를 갖는 action이 있을 것이다. 그 action을 <em>greedy</em> action 이라고 한다. 만약 이 greedy action 중 하나를 선택한다면 이를 <strong><em>exploiting</em></strong> 한다고 얘기한다. 반대로 <em>greedy</em> action이 아닌 다른 action 중 하나를 선택한다면 <strong><em>exploring</em></strong>한다고 얘기한다. 당장 한 스텝만 바라봤을 때는 <strong><em>exploitation</em></strong>이 value를 최대화 하는 방법일지 몰라도, 장기적인 관점에서 바라봤을 때 <strong><em>exploration</em></strong>의 total reward가 더 높을 수 있다.</p>

<p><em>exploration</em> 와 <em>exploitation</em> 의 균형을 맞춰 나가기 위해서 복잡한 수학적인 방법들이 존재하지만, 많은 가정들이 전제해야 하기 때문에 사실상 full reinforcment learning 문제에 적용하기는 불가능하다. 하지만, 이를 해결하기 위한 간단한 방법들이 존재하고 나중에 다룰 것이다.</p>

<hr />
<h3 id="action-value-methods">Action-value Methods</h3>

<p>여기서는 action의 value을 estimate하는 방법(method)에 대해 더 자세하게 알아볼 것이다. 우리는 이것을 <strong><em>action-value methods</em></strong> 라고 부르는데 action을 선택하기 위해서도 사용된다. 그리고 action이 선택될 때마다 계산한 reward의 평균을 <em>action의 true value</em>라고 한다. 수식은 아래와 같다.</p>

<script type="math/tex; mode=display">Q_t(a)\doteq \frac{\text{sum of rewards when $a$ taken prior to $t$}}{\text{number of times $a$ taken prior to $t$}} = \frac{\sum_{i=1}^{t-1}R_i \cdot \mathbb{1}_{A_i=a}}{\sum_{i=1}^{t-1}\mathbb{1}_{A_i=a}}</script>

<ul>
  <li>$\mathbb{1}_{predicate}$: $predicate$가 true 이면 1이고 false이면 0이다.</li>
</ul>

<p>위의 식에서 $\mathbb{1}_{A_i=a}$은 $a$가 선택된 경우만 계산하겠다는 의미다. 그리고 한 번도 $a$가 선택된 적이 없는 경우 분모가 0이 되어서 계산이 불가능하기 때문에 $Q_t(a)$를 기본 값(예: 0)으로 대신한다. 사실 이렇게 평균을 내는 것은 action value를 estimate하는 여러가지 방법 중 하나다. 우리는 이것을 <em>sample-average</em>라고 부를 것이다.</p>

<p>action을 선택하는 가장 간단한 방법 중 하나는 당연히 estimate value가 가장 큰 것(<em>greedy action</em>)을 선택하는 것이다. 만약 두 개 이상의 <em>greedy</em> action이 있다면 그 중 아무거나 선택하면 된다. 우리는 이런 방법을 <strong><em>greedy</em> action selection</strong> 이라고 하며, 아래와 같이 정의할 수 있다.</p>

<script type="math/tex; mode=display">A_t \doteq \underset{a}{argmax}Q_t(a)</script>

<p>위에서 $argmax_a$는 뒤에 따라오는 $Q_t(a)$를 최대화(maximized)해주는 action(a)을 선택한다는 뜻이다. <em>greedy</em> action selection은 지금 알고 있는 정보를 기반으로 <em>눈 앞에 보이는</em> reward를 최대화하기 위해서 항상 <strong>exploit</strong> 한다. 하지만 이런 방식은 장기적 관점에서 봤을 때 더 좋은 action을 놓칠 수 있다. 이런 단점을 보안하기 위해서 <strong>아주 적은 확률($\varepsilon$)</strong>로 action중 하나를 랜덤으로 선택하는 방법이 있는데, 우리는 이 방법을 <strong>$\varepsilon-greedy$</strong> method 라고 한다. 이런 방법의 장점은 모든 action들이 sampling될 수 있기 때문에 $Q_t(a)$ 가 $q_*(a)$로 수렴된다는 것이다.</p>

<hr />
<h3 id="the-10-armed-testbed">The 10-armed Testbed</h3>

<p><strong><em>k</em>-armed bandit problems</strong>으로 <em>greedy action-value method</em>(<strong>greedy method</strong>)와 <em>$\varepsilon$-greedy action-value method</em>(<strong>$\varepsilon$-greedy method</strong>) 두 가지 방법을 비교 했다. 아래는 <em>k</em>가 10개인 <em>k</em>-armed bandit problems의 reward 분포다. 분산(variance)이 1이고 평균(mean)이 $q_*(a)$인 정규 분포(normal distribution)다. 이 분포로 부터 $t$ (time step)에서 $A_t$(action)를 선택했을 때 $R_t$(reward)를 얻는다.</p>

<p><img src="/assets/2019-01-02-rl-bulti-armed-bandits/figure2_1.png" alt="image" /></p>

<p>분산(variance)이 1이고 평균(mean)이 0인 정규 분포(normal distribution)에서 1000번의 action을 선택했고, 이렇게 2000번 반복한 평균으로 성능을 측정했다.</p>

<p>아래는 <strong>greedy method</strong>와 두개의 <strong>$\varepsilon$-greedy method</strong>($\varepsilon$=0.01 과 $\varepsilon$=0.1)로 테스트한 결과다. 자세히 보면 처음에는 greedy method가 더 빠르게 향상되는 것처럼 보이나 시간이 지날수록 $\varepsilon$-greedy method가 더 향상되는 것을 볼 수 있다. greedy method는 시간이 지날수록 더디게 향상되는데, 이는 suboptimal에 빠질 수 있기 때문이다. 아래 그래프를 보면 greedy method는 optimal action의 1/3 정도 밖에 도달하지 못했다. 반면에, $\varepsilon$-greedy method는 계속해서 explore하고 optimal action을 찾기 위한 가능성을 높혔기 때문에 최종적으로 더 나은 성능을 보였다.</p>

<p><img src="/assets/2019-01-02-rl-bulti-armed-bandits/figure2_2.png" alt="image" /></p>

<p>$\varepsilon$-greedy method의 이점은 task에 따라서 다르다. 만약에 분산(variance)이 매우 크다면(예: 10) optimal policy를 찾기 위해서 더 많은 exploration를 해야한다. 이럴 경우 $\varepsilon$-greedy method가 더 좋은 성능을 보인다. 하지만 만약 분산(variance)이 0이라면 단 한 번만 시도 해보고 true value를 알 수 있기 때문에 exploration없이도 optimal policy를 찾을 수 있을 것이다. 하지만 이런 <strong>deterministic</strong>한 경우라도 일부 가정이 불확실 하다면 exploration은 필요하다. 예를 들어서 bandit problem이 <strong>nonstationary</strong>하다면 true value는 시간이 지남에 따라 변경되기 때문에 exploration이 필요하다. nonstationary 문제는 reinforcement learning에서 자주 나타나는 상황이다. 이와 같이 reinforcement learning에서 exploration과 exploitation의 균형은 매우 중요하다.</p>

<hr />
<h3 id="incremental-implementation">Incremental Implementation</h3>
<p>지금까지 논의한 <strong>action-value method</strong>는 얻은 rewards의 평균(sample averages)을 내어서 estimate 하였다. 이번에는 이렇게 매번 평균을 내는 것보다 더 효율적인 방법에 대해 알아볼 것이다.</p>

<p>복잡한 식를 피하기 위해서 하나의 action에 대해서만 다룬다. action value 식을 간단하게 표기하면 아래와 같은데, $R_i$는 i번째 action이 선택된 후에 받은 reward이고, $Q_n$은 n-1번째 까지 측정된 action value 이다.</p>

<script type="math/tex; mode=display">Q_n \doteq \frac{R_1 + R_2 + \dots + R_{n-1}}{n-1}</script>

<p>위의 방식은 모든 reward를 보관하고 있다가 value를 estimate할 때마다 계산한다. 하지만 이런 방식은 시간이 지나고 reward를 받을 때마다 더 많은 메모리와 연산을 필요로 한다. 예상할 수 있듯이, 이는 비효율적이기 때문에 평균(average)를 업데이트 하는 다른 방법이 필요하다. 이미 계산된 $Q_n$과 n번째 reward $R_n$이 주어지면 모든 reward의 평균을 아래와 같이 <strong>incremental formulas</strong>로 계산할 수 있다.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
Q_n &= \frac{1}{n}\sum_{i=1}^{n}R_i \\
    &= \frac{1}{n}\left(R_n + \sum_{i=1}^{n-1}R_i\right) \\
    &= \frac{1}{n}\left(R_n + (n-1)\frac{1}{n-1}\sum_{i=1}^{n-1}R_i\right) \\
    &= \frac{1}{n}\left(R_n + (n-1)Q_n\right) \\
    &= \frac{1}{n}\left(R_n + nQ_n - Q_n\right) \\
    &= Q_n + \frac{1}{n}\left[R_n - Q_n\right]
\end{align} %]]></script>

<p>위의 연산은 새로운 reward를 얻더라도 $Q_n$과 $n$ 그리고 (6)정도의 간단한 연산만 필요하다. 위의 update 식은 일반적으로 아래와 같이 표기한다.</p>

<script type="math/tex; mode=display">NewEstimate \leftarrow OldEstimate + StepSize [Target - OldEstimate]</script>

<p>위 식에서 $[Target - OldEstimate]$가 estimate에서 <strong>error</strong>이며, $Target$에 가까워질수록 작아진다. target은  우리가 원하는 방향이며 여기서는 n번째 reward다.</p>

<p>참고로, <strong>incremental method</strong> (6)에서 사용된 step-size parameter(StepSize)는 time step이 지날 수록 $\frac{1}{n}$으로 변경된다. 우리는 앞으로 이 <strong>step-size</strong>를 $\alpha$ 혹은 $\alpha_t(a)$로 표기할 것이다.</p>

<p>아래는 incrementally computed sample averages 와 $\varepsilon$-greedy action selection을 사용한 A Simple bandit algorithm의 슈도코드(Pseudocode)다.</p>

<p><img src="/assets/2019-01-02-rl-bulti-armed-bandits/pseudocode_1.png" alt="image" /></p>

<hr />
<h3 id="tracking-a-nonstationary-problem">Tracking a Nonstationary Problem</h3>

<hr />
<h3 id="optimistic-initial-values">Optimistic Initial Values</h3>

<hr />
<h3 id="summary">Summary</h3>

<hr />
<h3 id="reference">Reference</h3>
<ul>
  <li><a href="http://incompleteideas.net/book/the-book.html">Reinforcement Learning: An Introduction - Richard S. Sutton and Andrew G. Barto</a></li>
</ul>

  </article>
</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/reinforcement%20learning/2019/01/01/rl-introduction/">
            강화학습 정리 - Introduction
            <small>01 Jan 2019</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/algorithm/2018/12/29/boj-go-trip-1976/">
            BOJ - 여행 가자(1976)
            <small>29 Dec 2018</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/bixby/2018/12/28/basic_tutorial_4/">
            주사위 프로젝트 - View 및 Dialog 작성
            <small>28 Dec 2018</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>


<div class="comments">
  <h2>Comments</h2>
  <div id="disqus_thread"></div>
  <script>
var disqus_config = function () {
  this.page.url = 'http://localhost:4000/reinforcement%20learning/2019/01/02/rl-multi-armed-bandits/'; // Replace PAGE_URL with your page's canonical URL variable
  this.page.identifier = '/reinforcement%20learning/2019/01/02/rl-multi-armed-bandits'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
(function() {
  var d = document, s = d.createElement('script');
  s.src = '//hijigoo.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
})();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
</div>


      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if (target === toggle) {
            checkbox.checked = !checkbox.checked;
            e.preventDefault();
          } else if (checkbox.checked && !sidebar.contains(target)) {
            /* click outside the sidebar when sidebar is open */
            checkbox.checked = false;
          }
        }, false);
      })(document);
    </script>
    
    <script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-130716162-1', 'auto');
ga('send', 'pageview');
    </script>
    
  </body>
  
  <script id="dsq-count-scr" src="//hijigoo.disqus.com/count.js" async></script>
  
</html>
