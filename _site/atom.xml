<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>안녕지구</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000/"/>
 <updated>2019-01-29T01:32:50+09:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name>안녕지구</name>
   <email></email>
 </author>

 
 <entry>
   <title>강화학습 정리 - Finite Markov Decision Processes (업데이트중)</title>
   <link href="http://localhost:4000/reinforcement%20learning/2019/01/15/rl-finite-markov-decision-rocesses/"/>
   <updated>2019-01-15T00:00:00+09:00</updated>
   <id>http://localhost:4000/reinforcement%20learning/2019/01/15/rl-finite-markov-decision-rocesses</id>
   <content type="html">&lt;h2 id=&quot;3-finite-markov-decision-processes&quot;&gt;3. Finite Markov Decision Processes&lt;/h2&gt;

&lt;p&gt;이번 챕터에서는 앞으로 계속 다룰 finite &lt;strong&gt;Markov decision processes (MDPs)&lt;/strong&gt; 에 대해서 소개한다. 그런데 이전 챕터에서 다룬 bandits problem 과는 다르게, 상황에 따라서 다른 action을 취해야 하는 &lt;em&gt;associative task&lt;/em&gt; 이다. MDPs는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Sequential_decision_making&quot;&gt;순차적 의사 결정(sequential decision making)&lt;/a&gt;을 하는 고전적인 공식인데, 여기서 선택된 action은 immediate reward 에만 영향을 미치는 것이 아니라, 나중의 상황(situation)이나 &lt;strong&gt;state&lt;/strong&gt; 그리고 &lt;strong&gt;future reward&lt;/strong&gt; 에도 영향을 미친다. 그렇기 때문에 MDPs는 &lt;strong&gt;delayed reward&lt;/strong&gt; 라는 개념을 가지고 있으며, immediate reward 와 tradeoff 해야 하는 필요성도 수반하고 있다. Badit problem 에서는 각 action $a$ 의 value로 &lt;strong&gt;$q_*(a)$&lt;/strong&gt; 를 estimate했지만, MDPs에서는 action $a$ 와 각 state $s$ 의 value로 &lt;strong&gt;$q_*(s,a)$&lt;/strong&gt; 를 estimate한다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;the-agentenvironment-interface&quot;&gt;The Agent–Environment Interface&lt;/h3&gt;

&lt;p&gt;MDPs는 상호 작용하면서 학습하는 문제를 간단하게 정의한다. 학습하고 의사 결정하는 것을 &lt;strong&gt;&lt;em&gt;agent&lt;/em&gt;&lt;/strong&gt; 라고 하며, 이 agent와 상호작용 하며 agent외부에 존재하는 모든 것을 &lt;strong&gt;&lt;em&gt;environment&lt;/em&gt;&lt;/strong&gt; 라고 한다. agent가 action을 선택하면 environment는 새로운 상황(situation)을 제시한다. 또한 environment는 reward를 제공하는데, agent는 이 reward가  최대가 되도록 action을 선택해 나아가야 한다.&lt;/p&gt;

&lt;div class=&quot;message&quot;&gt;
situation과 state 단어가 나오는데요, 여기서는 situation을 state를 포함한 개념으로 사용합니다. 중요한 키워드는 state와 reward입니다. 
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-01-15-rl-finite-mdps/figure3_1.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림처럼 각 time step($t$ = 0, 1, 2, 3, …)마다 &lt;strong&gt;agent&lt;/strong&gt;와 &lt;strong&gt;environment&lt;/strong&gt;는  상호작용 한다. time step $t$ 에서 agent는 environment로부터 &lt;strong&gt;state&lt;/strong&gt; ($ S_t \in \mathscr{S}$ )를 받으며 이를 고려해서 &lt;strong&gt;action&lt;/strong&gt; ($ A_t \in \mathscr{A}(s)$ )을 선택한다. 그리고 한 time step 이후에 action에 대한 결과로 &lt;strong&gt;reward&lt;/strong&gt; ($ R_{t+1} \in \mathscr{R} \subset \mathbb{R} $)와 새로운 state ($ S_{t+1} $)를 받는다. 이를 순서대로 나타내면 아래와 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, ... &amp;&amp; (3.1)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;이전 time step 에서 state $s$와 action $a$이 주어졌을 때, $s^\prime$ 와 $r$이 나올 확률은 아래와 같이 표현할 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
p(s^\prime,r \mid s,a) \doteq \operatorname{Pr}\left\{ S_t=s^\prime, R_t=r  \mid  S_{t-1}=s, A_{t-1}=a \right\} &amp;&amp; (3.2)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;함수 $p$는 MDP의 &lt;em&gt;dynamics&lt;/em&gt;를 정의한다. 또한, $p$는 확률 분포이기 때문에 아래와 같이 정의할 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{s^\prime \in \mathscr{S}}\sum_{r \in \mathscr{R}}p(s^\prime,r \mid s,a) = 1, \text{  for all }s \in \mathscr{S}, a \in \mathscr{A}(s)&lt;/script&gt;

&lt;p&gt;$S_t$와 $R_t$의 값은 바로 직전의 $S_{t-1}$와 $A_{t-1}$에 의해서만 결정된다. state는 이전에 발생한 agent 와 environment의 상호작용에 대한 모든 정보를 내포하고 있어야 하는데, 여기서는 이 state를 &lt;strong&gt;&lt;em&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Markov_property&quot;&gt;Markov property&lt;/a&gt;&lt;/em&gt;&lt;/strong&gt;라고 한다.&lt;/p&gt;

&lt;p&gt;다음과 같이 함수 $p$으로부터 environment에 대해서 알고 싶은 정보를 계산할 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;state-transition probabilities&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(s^\prime \mid s,a) \doteq \operatorname{Pr}\left\{ S_t=s^\prime  \mid  S_{t-1}=s, A_{t-1}=a \right\} = \sum_{r \in \mathscr{R}}p(s^\prime,r \mid s,a)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;state-action&lt;/em&gt;&lt;/strong&gt;의 &lt;strong&gt;reward 기댓값(expected reward)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;r(s,a) \doteq  \mathbb{E}\left[R_t  \mid  S_{t-1}=s, A_{t-1}=a\right] = \sum_{r \in \mathscr{R}}r\sum_{s^\prime \in \mathscr{S}}p(s^\prime,r \mid s,a)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;state-action-next-state&lt;/em&gt;&lt;/strong&gt;의 &lt;strong&gt;reward 기댓값(expected reward)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;r(s,a,s^\prime) \doteq  \mathbb{E}\left[R_t  \mid  S_{t-1}=s, A_{t-1}=a, S_t=s^\prime\right] = \sum_{r \in \mathscr{R}}r\frac{p(s^\prime,r \mid s,a)}{p(s^\prime \mid s,a)}&lt;/script&gt;

&lt;p&gt;MDP framework는 다양한 방법으로 다양한 문제에 적용할 수 있다. 예를 들어서, time step은 꼭 고정된 간격이 아니여도 되고, action은 로보트 팔에 적용될 volatage 처럼 low-level 컨트롤 이거나 학교에서 점심을 먹을지 말지에 대한 선택이여도 된다. 또한 state는 sensor에 대한 정보나 방(room)에 있는 물체들에 대한 설명일 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MDP framework&lt;/strong&gt;는 상호작용으로부터 목표 지향적인(goal-directed) 학습 문제를 추상화한 것이다. 목표 지향적인(goal-directed) 행동을 취하는 어떤 문제에서도, agen와 environment 사이를 오가는 신호를 세가지로 요약할 수 있다.  agent가 선택하는 신호를 &lt;strong&gt;action&lt;/strong&gt;이라고 하고, 선택이 이루어지는 기본이 되는 신호를 &lt;strong&gt;state&lt;/strong&gt;라고 하며, agent의 목표인 신호를 &lt;strong&gt;reward&lt;/strong&gt;라고 한다. MDP framework가 모든 decision-learning problem을 표현하기에는 충분하지 않을 수 있으나, 지금까지 상당히 유용하게 사용되어왔다.&lt;/p&gt;

&lt;p&gt;물론 특정 state와 action은 작업마다 크게 다르며, 어떻게 표현 되느냐에 따라 성능에 크게 영향을 줄 수 있다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;goals-and-rewards&quot;&gt;Goals and Rewards&lt;/h3&gt;

&lt;p&gt;Reinforcement Learning 에서 agent의 &lt;strong&gt;목표(goal)&lt;/strong&gt;는 environment로 부터 받는 특별한 신호인 &lt;strong&gt;&lt;em&gt;reward&lt;/em&gt;&lt;/strong&gt;를 공식 하는 것이다. 각 time step에서 reward는 단순한 숫자다. 조금 더 간단하게 얘기하면 agent의 목표(goal)는 reward의 총 합을 최대화 하는 것이다. 총 합이라는 것은 즉시 받는(immediate) reward뿐 아니라, 장기적으로 봤을 때 받을 모든 reward의 누적값이 라는 의미다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;That all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;reward는 reinforcement learning에서 가장 두드러지는 특징 중 하나다.&lt;/p&gt;

&lt;p&gt;reward에 대해서 goal을 공식화하는 것이 한계가 있어 보이지만, 유용하고 넓은 범위에서 사용 가능하다는 것을 증명해왔다. 예를 들어서, 로봇(agent)이 미로를 탈출하는 방법을 학습 시킬때 time step 마다 &lt;strong&gt;-1&lt;/strong&gt; reward를 주었더니 이 agent는 더 빨리 탈출 하는 방법을 학습했다. 또한 재활용 로봇이 빈 캔을 찾고 수집하는 방법을 학습할 수 있도록 캔을 수집할 때마다 &lt;strong&gt;+1&lt;/strong&gt; reward를 주어서 학습시키기도 했다.&lt;/p&gt;

&lt;p&gt;위의 예에서 보면, agent는 항상 reward를 최대화 하기 위해서 학습한다. 만약 agent가 우리가 원하는 것을 학습하기 원한다면 agent가 reward를 최대화 하면서 목표(goal)를 달성할 수 있도록 reward를 제공해야 한다. 따라서 우리가 설정한 reward는 달성하고자 하는 목표(goal)을 확실하게 가리키도록 해야 한다. Reward는 agent와 &lt;strong&gt;&lt;em&gt;‘how’&lt;/em&gt;&lt;/strong&gt; 가 아닌 &lt;strong&gt;&lt;em&gt;‘what’&lt;/em&gt;&lt;/strong&gt; 을 communication하는 방법이다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;returns-and-episodes&quot;&gt;Returns and Episodes&lt;/h3&gt;

&lt;p&gt;agent의 목표는 장기적으로 누적은 reward를 최대화 하는 것이다. 바로 이 누적된 reward를 $G_t$ 로 아래와 같이 표현할 수 있다. 이 값을 &lt;strong&gt;&lt;em&gt;expected return&lt;/em&gt;&lt;/strong&gt; 이라고 한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
G_t \doteq R_{t+1} + R_{t+2} + R_{t+3} + \dots + R_T &amp;&amp; (3.7)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;위 식에서 $T$를 final time step 이라고 한다. agent-envirionment 사이에서 이뤄지는 상호작용을 &lt;strong&gt;episode&lt;/strong&gt; 개념으로 나눌 수 있다. 이 episode는 마치 게임에서 한 번 플레이 하는 것과 같은데, 여기서 final time step의 개념이 있는 것은 자연스러운 접근이다. 또한, 각 episode 에서 마지막 state를 &lt;strong&gt;terminal state&lt;/strong&gt;라고 한다. episode가 게임에서 승패와 같이 특정 결과로 끝나더라도 다음 episode는 이전의 결과와 상관없이 새로 시작된다. 이런 episode로 이루어진 task를 &lt;em&gt;episodic task&lt;/em&gt;라고 한다. episodic task에서 terminal state의 존재 여부에 따라서 state set을 아래와 같이 구분한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathscr{S}$  : terminal state가 포함되지 않는 모든 state set&lt;/li&gt;
  &lt;li&gt;$\mathscr{S}^+$: terminal state이 포함된 모든 모든 state set&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;반면에 하나의 episode가 끝나지 않는 경우가 있는데. 이를 &lt;strong&gt;continuing task&lt;/strong&gt;라고 한다. 이런 경우 final time step 은 $T=\infty$로 무한이기 때문에, 최대화 하려고 했던 return 식 (3.7)에 문제가 생긴다. 그렇기 때문에 앞으로 다른 return 식을 사용한다. 여기에 &lt;strong&gt;discounting&lt;/strong&gt; 라는 컨셉이 추가된다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
G_t \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2R_{t+3} + \dots =  \sum_{k=0}^\infty \gamma^kR_{t+k+1} &amp;&amp; (3.8)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$\gamma$: discount rate ($ 0 \le \gamma \le 1 $)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;discount rate&lt;/strong&gt; 는 미래(future) reward의 현재 가치을 정의한다. 미래 시점인 $k$ time step 에서 받을 reward의 현재 가치는 $\gamma^{k-1}$ 배 만큼 줄어든다. 만약 $\gamma &amp;lt; 1$ 이면 (3.8) 식은 finite value를 가지게 될 것이고, $\gamma = 0$이라면 agent는 즉시 받을(immediate) reward $R_{t+1}$ 만 고려해서 action $A_t$를 선택할 것이다. 하지만 이렇게 immediate reward만 고려해서 action을 선택한다면 전체 return 값은 감소할 수 있다. $\gamma$ 가 1에 가까울 수록 전체 return은 future reward를 더 많이 수용한다. 이는 agent가 미래에 받을 reward를 신뢰한다고 볼 수 있다. 식 (3.8)은 아래와 같이 전개될 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
G_t &amp;\doteq R_{t+1} + \gamma R_{t+2} + \gamma^2R_{t+3} + \gamma^2R_{t+4} +\dots \\
	&amp;= R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \gamma^2 R_{t+4} + \dots) \\
	&amp;= R_{t+1} + \gamma G_{t+1} &amp; (3.9)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;return 식 (3.8) 에서 만약 reward가 0이 아니고 $\gamma &amp;lt; 1$ 이라면 식(3.8)은 finite하다. 예를 들어서 reward가 1이라고 가정하면 return은 아래와 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;G_t = \sum_{k=0}^\infty \gamma^k = \frac{1}{1 - \gamma}&lt;/script&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;unified-notation-for-episodic-and-continuing-tasks&quot;&gt;Unified Notation for Episodic and Continuing Tasks&lt;/h3&gt;

&lt;p&gt;이전 섹션에서 agent-environment 상호작용이 일련의 분리된 episode로 나눠지는 &lt;strong&gt;episodic task&lt;/strong&gt;와 그렇지 않은 &lt;strong&gt;continuing task&lt;/strong&gt;에 대해서 알아봤다. 앞으로 이 두 종류의 task에 대해서 다룰 것이기 때문에, 두 가지를 모두 정확하게 표현할 수 있는 하나의 표기법(notation)을 정의하는 것이 유용하다.&lt;/p&gt;

&lt;p&gt;원래는 episodic task를 더 정확하게 표현하기 위해서 추가적인 notation이 필요하다. 왜냐하면 여러개의 연속적인 episode을 표현해야 할 수도 있기 때문이다. 이 episode와 time step 둘 모두 고려했을 때 episode $i$ 에서 time step $t$ 일 때 state를 $S_{t,i}$ 로 나타낼 수 있다. 다른 것들도 마찬가지로 $A_{t,i}$, $R_{t,i}$, $\pi_{t,i}$, $T_{i}$, 등으로 나타낼 수 있다. 그러나 여기서는 대부분의 경우 episode를 구분할 필요가 없기 때문에 단일 episode만 고려할 것이기 때문에 $S_{t,i}$ 대신에 $S_t$ 을 사용할 것이다.&lt;/p&gt;

&lt;p&gt;episodic task와 continuing task는 episode의 종료 시점을 고려하면 통합 될 수 있는데 그림으로 나타내면 아래와 같다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;80%&quot; height=&quot;80%&quot; src=&quot;/assets/2019-01-15-rl-finite-mdps/pic1.png&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;위 그림에서 네모 모양은 episode의 마지막에 해당하는 special absorbing state 이다. $S_0$ 에서 시작해서 연속적인 reward +1, +1, 0, 0, 0, … 를 받는데, 모두 더하면 $T=3$ 이거나 $T=\infty$ 이더라도 똑같은 reward를 얻는다. discount rate를 적용해도 마찬가지다. 식으로 나타내면 아래와 같이 나타낼 수 있다. 앞으로는 아래 식을 계속 사용한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
G_t \doteq \sum_{k=t+1}^T \gamma^{k-t-1}R_k &amp;&amp; (3.11)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;div class=&quot;message&quot;&gt;
식 (3.8)과 (3.11)은 상당히 유사합니다. (3.8)에서 k가 t시점에서 부터 진행된 time step 이라면, (3.11)에서 k는 시작 time step를 의미합니다. T time step을 표현하기 위해서라고 생각하시면 될 것 같습니다. (3.8)에서 단순히 무한 을 T로 변환하면 식이 성립하지 않습니다.
&lt;/div&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;policies-and-value-functions&quot;&gt;Policies and Value Functions&lt;/h3&gt;

&lt;p&gt;거의 모든 reinforcement learning 알고리즘은 value function을 estimating하는 것을 포함한다. 이 &lt;strong&gt;&lt;em&gt;value function&lt;/em&gt;&lt;/strong&gt;은 agent가 주어진 state에 있는 것이 얼마나 많은 &lt;strong&gt;expected return&lt;/strong&gt;을 받을지 estimate한다. 그리고 agent가 주어진 state에서 선택한 action이 얼마나 많은 expected return을 받을지 estimate하는 것을 &lt;strong&gt;&lt;em&gt;action-value function&lt;/em&gt;&lt;/strong&gt;이라고 한다.&lt;/p&gt;

&lt;div class=&quot;message&quot;&gt;
여기서 expected return은 agent가 앞으로 받을 rewards의 합입니다. 식 (3.7), (3.8), (3.9)에 자세하게 나와있습니다.
&lt;/div&gt;

&lt;p&gt;물론, expected reward 는 agent가 어떤 action을 선택해 나아가냐에 따라 달려 있다. 그렇기 때문에 value function은 &lt;strong&gt;정책(policy)&lt;/strong&gt;이라고 하는 action을 선택하는 방법에 영향을 받는다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;policy&lt;/strong&gt;는 state 와 action의 선택 확률을 맵핑한다. agent 가 time $t$ 에서 policy $\pi$ 를 따른다면, $S_t = s$ 가 주어졌을 때 $A_t = a$ 일 확률을 $\pi(a \mid s)$ 로 나타낼 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MDPs&lt;/strong&gt; 에서 policy $\pi$ 를 따르는 &lt;strong&gt;&lt;em&gt;value function&lt;/em&gt;&lt;/strong&gt; 와  &lt;strong&gt;action value function&lt;/strong&gt; 을 아래와 같이 정의할 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;state-value function for policy $\pi$&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
v_\pi(s) \doteq  \mathbb{E}_\pi\left[G_t  \mid  S_t=s \right] = \mathbb{E}_\pi\left[ \sum_{k=0}^\infty \gamma^kR_{t+k+1} \mid S_t = s \right], \text{for all } s \in \mathscr{S}, &amp;&amp; (3.12)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;action-value function for policy $\pi$&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
q_\pi(s,a) \doteq  \mathbb{E}_\pi\left[G_t  \mid  S_t=s, A_t=a \right] = \mathbb{E}_\pi\left[ \sum_{k=0}^\infty \gamma^kR_{t+k+1} \mid S_t = s, A_t = a \right], &amp;&amp; (3.13)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;위 두 value function 값은 경험을 통해서 측정할 수 있다. 예를 들어서, agent가 policy $\pi$ 를 따르는 상황에서 무한히 각 state에서 value의 평균을 구한다면 &lt;strong&gt;$v_\pi(s)$&lt;/strong&gt;에 &lt;strong&gt;수렴(converge)&lt;/strong&gt; 한다. 그리고 마찬가지로 &lt;strong&gt;$q_\pi(s,a)$&lt;/strong&gt; 도 수렴한다. 실제 return 을 매우 많이 샘플링해서 평균을 구하는 방법으로, &lt;strong&gt;&lt;em&gt;Monte Carlo method&lt;/em&gt;&lt;/strong&gt; 라고 한다.&lt;/p&gt;

&lt;p&gt;Reinforcement learning 과 &lt;strong&gt;dynamic programming&lt;/strong&gt; 에서 value function 의 근본적인 성질은 식(3.9) 처럼 &lt;strong&gt;재귀적인 관계(recursive relationship)&lt;/strong&gt; 를 충족 시킨다는 것이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
v_\pi(s) &amp;\doteq  \mathbb{E}_\pi\left[G_t  \mid  S_t=s \right]  \\
			&amp;= \mathbb{E}_\pi \left[R_{t+1} + \gamma G_{t+1}  \mid  S_t=s \right] &amp;  (by (3.9))\\
			&amp;= \sum_a \pi(a \mid s) \sum_{s^\prime}\sum_r p(s^\prime,r \mid s,a) \left[ r + \gamma \mathbb{E}_\pi \left[G_{t+1} \mid S_{t+1} = s^\prime \right] \right] \\
			&amp;= \sum_a \pi(a \mid s) \sum_{s^\prime, r} p(s^\prime,r \mid s,a) \left[ r + \gamma v_\pi(s^\prime) \right], \text{for all } s \in \mathscr{S}, &amp; (3.14)
			
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;식 (3.14)는 $v_\pi$에 대한 &lt;strong&gt;&lt;em&gt;Bellman equation&lt;/em&gt;&lt;/strong&gt; 이다. 이것은 state 의 value 와 다음 state 의 value 에 대한 관계를 표현한다. 아래는 그림으로 나타낸 것이다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;30%&quot; height=&quot;30%&quot; src=&quot;/assets/2019-01-15-rl-finite-mdps/pic2.png&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;하얀 원은 &lt;strong&gt;state&lt;/strong&gt; 이고 검정 점은 &lt;strong&gt;action&lt;/strong&gt; 을 의미한다. 제일 위에 있는 state $s$ 에서 시작하여, agent는 policy $\pi$ 를 따라서 세 개의 action 중 하나를 선택한다. 그러면 environment 는 dynamics 함수 $p$에 따라서 reward $r$ 과 다음 state $s_\prime$ 를 준다. Bellman Equation(3.14)은 발생될 확률을 가지는 모든 가능성에 대한 평균이다. 처음 state 의 value 는 다음 expected state 의 (discounted) value 와 그에 따른 reward 의 합과 같다.&lt;/p&gt;

&lt;p&gt;Reinforcement learning method 에서 심장이라고 할 수 있는 update 혹은 backup 연산을 나타내기 때문에, 위 다이어그램을 &lt;strong&gt;backup diagram&lt;/strong&gt; 이라고 한다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;$q_\pi(s, a)$&lt;/strong&gt;에 대한 Bellman equation을 backup digram으로 나타내면 아래와 같다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;30%&quot; height=&quot;30%&quot; src=&quot;/assets/2019-01-15-rl-finite-mdps/pic3.png&quot; /&gt;
&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;optimal-policies-and-optimal-value-functions&quot;&gt;Optimal Policies and Optimal Value Functions&lt;/h3&gt;

&lt;p&gt;Reinforcement learning 문제를 해결한다는 것은 장기적 관점에서 가장 많은 &lt;strong&gt;reward&lt;/strong&gt; 를 얻을 수 있는 &lt;strong&gt;policy&lt;/strong&gt; 를 찾는 것이다. 만약 모든 state 에서 $\pi$ 의 expected return 이 $\pi^\prime$ 보다 같거나 더 크다면, policy $\pi$ 가 policy $\pi^\prime$ 보다 더 잘 정의된 것이다. 다시 말해서 $ s \in \mathscr{S}$ 에서 $v_\pi(s) \geq v_{\pi^\prime}$ 이라면 $\pi \geq \pi^\prime$ 이다. 이중에서 다른 모든 policy 보다 제일 나은 policy 를 &lt;strong&gt;&lt;em&gt;optimal policy&lt;/em&gt;&lt;/strong&gt; 라고 하며, &lt;strong&gt;$\pi_*$&lt;/strong&gt; 로 표기한다. 그리고 이 optimal policy 를 따르는 state-value function 을 &lt;strong&gt;&lt;em&gt;optimal state-value function&lt;/em&gt;&lt;/strong&gt; 이라고 하며, action-value function 을 &lt;strong&gt;&lt;em&gt;optimal action-value function&lt;/em&gt;&lt;/strong&gt; 이라고 한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;optimal state-value function&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
v_*(s) &amp;\doteq  \underset{\pi}{\text{max}} v_\pi(s), \text{	for all } s \in \mathscr{S}
			
\end{align*} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;optimal action-value function&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
q_*(s,a) &amp;\doteq  \underset{\pi}{\text{max}} q_\pi(s,a), \text{	for all } s \in \mathscr{S} \text{ and } a \in \mathscr{A}(s)
			
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;$q_*$ 를 $v_*$ 에 대한 식으로 아래와 같이 정의할 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
q_*(s,a) = \mathbb{E} \left[ R_{t+1} + \gamma v_*(S_{t+1}) \mid  S_t=s, A_t=a \right]
\end{align*}&lt;/script&gt;

&lt;p&gt;$v_*$는 policy 에 대한 value function 이기 때문에, Bellman Equation (3.14) 의 조건에 대해 일관성을 가져야 한다. 그러나 &lt;strong&gt;optimal value function&lt;/strong&gt; 이기 때문에 특정한 policy 와 상관 없는 식을 가져야 한다. 이를 &lt;strong&gt;Bellman optimality equation&lt;/strong&gt; 이라고 한다. 직관적으로 말하면, Bellman optimality equation 은 optimal policy 를 따른 state 의 value 와 가장 좋은 action을 선택했을 때 받는 expected return 은 같다는 사실을 표현한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
 	v_*(s) &amp;= \underset{a \in \mathscr{A}(s)}{\text{max}} q_{\pi_*}(s,a) \\
	&amp;= \underset{a}{\text{max}} \mathbb{E}_{\pi_*} \left[ G_t \mid S_t = s, A_t = a \right]\\
	&amp;= \underset{a}{\text{max}} \mathbb{E}_{\pi_*} \left[ R_{t+1} + \gamma G_{t+1} \mid S_t = s, A_t = a \right] &amp; (by (3.9))\\
	&amp;= \underset{a}{\text{max}} \mathbb{E} \left[ R_{t+1} + \gamma v_*(S_{t+1}) \mid S_t = s, A_t = a \right] &amp; (3.18)\\
	&amp;= \underset{a}{\text{max}} \sum_{s^\prime, r}p(s^\prime, r \mid s, a) \left[ r + \gamma v_*(s^\prime) \right] &amp; (3.19)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;위 식에서 마지막 두 방정식은 $v_*$ 에 대한 Bellman optimality equation 의 두가지 형태이다. $q_*$ 에 대한 &lt;strong&gt;Bellman optimality equation&lt;/strong&gt; 은 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
	q_*(s,a) &amp;= \mathbb{E} \left[ R_{t+1} + \gamma \underset{a^\prime}{\text{max}}q_*(S_{t+1}, a^\prime) \mid S_t=s, A_t = a \right] \\
	&amp;= \sum_{s^\prime, r}p(s^\prime, r \mid s, a) \left[ r + \gamma \underset{a^\prime}{\text{max}}q_*(s^\prime, a^\prime)) \right] &amp; (3.20)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;아래 backup diagram 은 $v_*$ 와 $q_*$ 에 대한 Bellman optimality equation 을 시각적으로 보여준다. 이 backup digram 은 agent 가 max 를 선택하는 부분인 호(arc)를 제외하면 앞의 $v_\pi$ 및 $q_\pi$ 와 같다. 왼쪽은 식 (3.19)을 보여주고, 오른쪽은 식 (3.20)을 보여준다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;60%&quot; height=&quot;60%&quot; src=&quot;/assets/2019-01-15-rl-finite-mdps/figure3_4.png&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;만약 $v_*$ 를 알고 있다면 optimal policy 를 찾는 건 상대적으로 쉽다. 각 state $s$ 에서 Bellman optimality equation 의 최대 값을 얻을 수 있는 action 이 하나 이상은 있을 것이다. 이 action 에 0이 아닌 확률이 부여된 policy 가 &lt;strong&gt;optimal policy&lt;/strong&gt; 이다. 이것을 &lt;strong&gt;one-step search&lt;/strong&gt; 로 생각할 수 있는데, one-step search 후에 선택한 action 이 optimal action 이다. &lt;strong&gt;장기적인 관점에서 기대되는 최적의 반환 값(optimal expected long-term return)&lt;/strong&gt;인 $v_*$ 은 각 state 에서 사용된다. 그러므로 one-step-ahead search 는 long-term optimal action 을 찾는다고 볼 수 있다.&lt;/p&gt;

&lt;p&gt;$q_*$ 가 있으면 optimal action 을 찾기가 더 쉽다. $q_*$ 가 있으면 agent는 one-step-ahead search를 하지 않아도 된다. 왜냐하면 모든 state $s$에서 $q_*(s,a)$ 가 가장 큰 action $a$ 를 찾으면 되기 때문이다. action-value function 은 모든 one-step-ahead search 의 결과를 보관(cache)한다. 이것은 각 state-action pair 에서 바로 사용할 수 있는 값으로써 &lt;strong&gt;optimal expected long-term return&lt;/strong&gt; 를 제공한다.&lt;/p&gt;

&lt;p&gt;명시적으로 Bellman optimality equation 을 푸는 것은 optimal policy 를 찾기 위한 또 다른 방법이다. 이것은 곧 reinforcement learning 문제를 해결하는 것이다. 그러나 이 방법은 거의 유용하지 않다. 이 방법은 실제 문제에서는 거의 얻을 수 없는 세가지 가정을 충족해야 한다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Environment 의 dynamics 에 대해 정확하게 알고 있어야 한다.&lt;/li&gt;
  &lt;li&gt;연산에 필요한 충분한 resource 가 있어야 한다.&lt;/li&gt;
  &lt;li&gt;Markov property 를 만족해야 한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;그러나 우리가 관심을 가지고 있는 문제들은 거의 위 조건들을 충족하지 않는다. 예를 들어서 backgammon 게임에서 1번과 3번 은 충족하지만 2번 조건에서 문제가 생긴다. 왜냐하면 backgammon 게임은 거의 $10^20$ 의 state를 가지는데, 요즘 가장 빠른 컴퓨터를 사용하더라도 $v_*$ 이나 $q_*$ 를 계산하기 위해서는 수천년이 걸리기 때문이다. 그렇기 때문에 reinforcement learning 에서는 일반적으로 &lt;strong&gt;approximate solution&lt;/strong&gt; 을 찾아야 한다.&lt;/p&gt;

&lt;p&gt;다양한 decision-making 문제는 Bellman optimality equation 을 근사(approximately)하는 방법으로 해결한다고 볼 수 있다. &lt;strong&gt;Dynamic programming&lt;/strong&gt; 은 Bellman optimality equation 에 조금 더 관련되어 있다. 많은 reinforcement learning 은 사전 정보가 아닌 실제 경험을 통해서 Bellman optimality equation 을 근사적으로(approximately) 해결하는 것으로 이해할 수 있다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;optimality-and-approximation&quot;&gt;Optimality and Approximation&lt;/h3&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;[Reinforcement Learning: An Introduction - Richard S. Sutton a&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>강화학습 정리 - Multi-armed Bandits</title>
   <link href="http://localhost:4000/reinforcement%20learning/2019/01/08/rl-multi-armed-bandits/"/>
   <updated>2019-01-08T00:00:00+09:00</updated>
   <id>http://localhost:4000/reinforcement%20learning/2019/01/08/rl-multi-armed-bandits</id>
   <content type="html">&lt;h2 id=&quot;2-multi-armed-bandits&quot;&gt;2. Multi-armed Bandits&lt;/h2&gt;

&lt;p&gt;강화학습이 다른 딥러닝과 구분되는 가장 중요한 특징은 선택한 action에 대해 &lt;strong&gt;평가(evaluate)&lt;/strong&gt;를 한다는 것이다. 이런 피드백(feedback)은 얼마나 좋은지에 대한 평가이지 정답인지 아닌지를 알려주는 것은 아니다. 이번 챕터에서는 간단한 환경에서 강화학습의 평가(evaluate)에 중점을 두고 공부할 것이다. 오직 단 하나의 상황에서만 evaluate하기 때문에 &lt;em&gt;full reinforcement learning problem&lt;/em&gt;의 복잡성을 피할 수 있다. 여기서 다룰 문제는 &lt;strong&gt;k-armed bandit problem&lt;/strong&gt;인데 이를 소개하면서 강화학습의 주요한 요소 몇가지도 함께 다룰 것이다.&lt;/p&gt;

&lt;div class=&quot;message&quot;&gt;
이번 장에서는 단순화된 환경(nonassociative setting)에서 설명하기 때문에, 강화학습에서 처음에 많이 다루는 'frozen lake'와 비교하면서 보시면 헷갈리실 수 있습니다. 처음에는 'Multi-armed Bandits'를 독립적인 문제로 보고 접근하시는 것을 추천드립니다.
&lt;/div&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;a-k-armed-bandit-problem&quot;&gt;A k-armed Bandit Problem&lt;/h3&gt;

&lt;p&gt;A k-armed Bandit Problem은 k개의 레버가 있는 슬롯머신에서 최대의 reward를 받기 위한 문제다. 내용은 아래와 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;k개의 다른 option이나 action중에서 하나를 선택한다.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;stationary probability distribution&lt;/em&gt;으로 부터 하나의 reward를 받는다.&lt;/li&gt;
  &lt;li&gt;최종 목표는 일정 기간 동안 전체 reward를 최대화 하는 것이다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;위 k-armed bandit problem에서 k action을 선택할 때마다 reward를 받는데, 이때 rewards의 기댓값(expectation)을 선택된 action의 &lt;strong&gt;value&lt;/strong&gt;라고 한다. 식으로 나타내면 아래와 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;q_*(a)\doteq \mathbb{E}[R_t | A_t=a]&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$A_t$: time step가 $t$ 일 때 선택된 action&lt;/li&gt;
  &lt;li&gt;$R_t$: time step가 $t$ 일 때 $A_t$에 대한 reward&lt;/li&gt;
  &lt;li&gt;$q_*(a)$: action $a$ 가 선택됐을 때 받는 reward의 기댓값&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;만약 우리가 각 action에 대한 value를 알고 있다면 k-armed bandit problem을 해결하는 것은 매우 쉬울 것이다. 매번 value가 가장 높은 action을 선택하면 되기 때문이다. 그런데 처음에는 value을 알 수 없기 때문에 계속해서 &lt;strong&gt;estimate&lt;/strong&gt;해야 한다. 이렇게 time step $t$에서 선택된 action $a$의 value를 $Q_t(a)$라고 한다. 우리는 $Q_t(a)$가  $q_*(a)$에 가까워 지도록 계속해서 estimate 해야 한다.&lt;/p&gt;

&lt;p&gt;action value를 계속해서 estimate 한다면, time step 마다 적어도 한 개 이상의 가장 높은 value를 갖는 action이 있을 것이다. 그 action을 &lt;em&gt;greedy&lt;/em&gt; action 이라고 한다. 만약 이 greedy action 중 하나를 선택한다면 이를 &lt;strong&gt;&lt;em&gt;exploiting&lt;/em&gt;&lt;/strong&gt; 한다고 얘기한다. 반대로 &lt;em&gt;greedy&lt;/em&gt; action이 아닌 다른 action 중 하나를 선택한다면 &lt;strong&gt;&lt;em&gt;exploring&lt;/em&gt;&lt;/strong&gt;한다고 얘기한다. 당장 한 스텝만 바라봤을 때는 &lt;strong&gt;&lt;em&gt;exploitation&lt;/em&gt;&lt;/strong&gt;이 value를 최대화 하는 방법일지 몰라도, 장기적인 관점에서 바라봤을 때 &lt;strong&gt;&lt;em&gt;exploration&lt;/em&gt;&lt;/strong&gt;의 total reward가 더 높을 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;exploration&lt;/em&gt; 와 &lt;em&gt;exploitation&lt;/em&gt; 의 균형을 맞춰 나가기 위해서 복잡한 수학적인 방법들이 존재하지만, 많은 가정들이 전제해야 하기 때문에 사실상 full reinforcment learning 문제에 적용하기는 불가능하다. 하지만, 이를 해결하기 위한 간단한 방법들이 존재하고 나중에 다룰 것이다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;action-value-methods&quot;&gt;Action-value Methods&lt;/h3&gt;

&lt;p&gt;여기서는 action의 value을 estimate하는 방법(method)에 대해 더 자세하게 알아볼 것이다. 우리는 이것을 &lt;strong&gt;&lt;em&gt;action-value methods&lt;/em&gt;&lt;/strong&gt; 라고 부르는데 action을 선택하기 위해서도 사용된다. 그리고 action이 선택될 때마다 계산한 reward의 평균을 &lt;em&gt;action의 true value&lt;/em&gt;라고 한다. 수식은 아래와 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q_t(a)\doteq \frac{\text{sum of rewards when $a$ taken prior to $t$}}{\text{number of times $a$ taken prior to $t$}} = \frac{\sum_{i=1}^{t-1}R_i \cdot \mathbb{1}_{A_i=a}}{\sum_{i=1}^{t-1}\mathbb{1}_{A_i=a}}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathbb{1}_{predicate}$: $predicate$가 true 이면 1이고 false이면 0이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;위의 식에서 $\mathbb{1}_{A_i=a}$은 $a$가 선택된 경우만 계산하겠다는 의미다. 그리고 한 번도 $a$가 선택된 적이 없는 경우 분모가 0이 되어서 계산이 불가능하기 때문에 $Q_t(a)$를 기본 값(예: 0)으로 대신한다. 사실 이렇게 평균을 내는 것은 action value를 estimate하는 여러가지 방법 중 하나다. 우리는 이것을 &lt;em&gt;sample-average&lt;/em&gt;라고 부를 것이다.&lt;/p&gt;

&lt;p&gt;action을 선택하는 가장 간단한 방법 중 하나는 당연히 estimate value가 가장 큰 것(&lt;em&gt;greedy action&lt;/em&gt;)을 선택하는 것이다. 만약 두 개 이상의 &lt;em&gt;greedy&lt;/em&gt; action이 있다면 그 중 아무거나 선택하면 된다. 우리는 이런 방법을 &lt;strong&gt;&lt;em&gt;greedy&lt;/em&gt; action selection&lt;/strong&gt; 이라고 하며, 아래와 같이 정의할 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A_t \doteq \underset{a}{argmax}Q_t(a)&lt;/script&gt;

&lt;p&gt;위에서 $argmax_a$는 뒤에 따라오는 $Q_t(a)$를 최대화(maximized)해주는 action(a)을 선택한다는 뜻이다. &lt;em&gt;greedy&lt;/em&gt; action selection은 지금 알고 있는 정보를 기반으로 &lt;em&gt;눈 앞에 보이는&lt;/em&gt; reward를 최대화하기 위해서 항상 &lt;strong&gt;exploit&lt;/strong&gt; 한다. 하지만 이런 방식은 장기적 관점에서 봤을 때 더 좋은 action을 놓칠 수 있다. 이런 단점을 보안하기 위해서 &lt;strong&gt;아주 적은 확률($\varepsilon$)&lt;/strong&gt;로 action중 하나를 랜덤으로 선택하는 방법이 있는데, 우리는 이 방법을 &lt;strong&gt;$\varepsilon-greedy$&lt;/strong&gt; method 라고 한다. 이런 방법의 장점은 모든 action들이 sampling될 수 있기 때문에 $Q_t(a)$ 가 $q_*(a)$로 수렴된다는 것이다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;the-10-armed-testbed&quot;&gt;The 10-armed Testbed&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;k&lt;/em&gt;-armed bandit problems&lt;/strong&gt;으로 &lt;em&gt;greedy action-value method&lt;/em&gt;(&lt;strong&gt;greedy method&lt;/strong&gt;)와 &lt;em&gt;$\varepsilon$-greedy action-value method&lt;/em&gt;(&lt;strong&gt;$\varepsilon$-greedy method&lt;/strong&gt;) 두 가지 방법을 비교 했다. 아래는 &lt;em&gt;k&lt;/em&gt;가 10개인 &lt;em&gt;k&lt;/em&gt;-armed bandit problems의 reward 분포다. 분산(variance)이 1이고 평균(mean)이 $q_*(a)$인 정규 분포(normal distribution)다. 이 분포로 부터 $t$ (time step)에서 $A_t$(action)를 선택했을 때 $R_t$(reward)를 얻는다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-01-02-rl-bulti-armed-bandits/figure2_1.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;분산(variance)이 1이고 평균(mean)이 0인 정규 분포(normal distribution)에서 1000번의 action을 선택했고, 이렇게 2000번 반복한 평균으로 성능을 측정했다.&lt;/p&gt;

&lt;p&gt;아래는 &lt;strong&gt;greedy method&lt;/strong&gt;와 두개의 &lt;strong&gt;$\varepsilon$-greedy method&lt;/strong&gt;($\varepsilon$=0.01 과 $\varepsilon$=0.1)로 테스트한 결과다. 자세히 보면 처음에는 greedy method가 더 빠르게 향상되는 것처럼 보이나 시간이 지날수록 $\varepsilon$-greedy method가 더 향상되는 것을 볼 수 있다. greedy method는 시간이 지날수록 더디게 향상되는데, 이는 suboptimal에 빠질 수 있기 때문이다. 아래 그래프를 보면 greedy method는 optimal action의 1/3 정도 밖에 도달하지 못했다. 반면에, $\varepsilon$-greedy method는 계속해서 explore하고 optimal action을 찾기 위한 가능성을 높혔기 때문에 최종적으로 더 나은 성능을 보였다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-01-02-rl-bulti-armed-bandits/figure2_2.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$\varepsilon$-greedy method의 이점은 task에 따라서 다르다. 만약에 분산(variance)이 매우 크다면(예: 10) optimal policy를 찾기 위해서 더 많은 exploration를 해야한다. 이럴 경우 $\varepsilon$-greedy method가 더 좋은 성능을 보인다. 하지만 만약 분산(variance)이 0이라면 단 한 번만 시도 해보고 true value를 알 수 있기 때문에 exploration없이도 optimal policy를 찾을 수 있을 것이다. 하지만 이런 &lt;strong&gt;deterministic&lt;/strong&gt;한 경우라도 일부 가정이 불확실 하다면 exploration은 필요하다. 예를 들어서 bandit problem이 &lt;strong&gt;nonstationary&lt;/strong&gt;하다면 true value는 시간이 지남에 따라 변경되기 때문에 exploration이 필요하다. nonstationary 문제는 reinforcement learning에서 자주 나타나는 상황이다. 이와 같이 reinforcement learning에서 exploration과 exploitation의 균형은 매우 중요하다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;incremental-implementation&quot;&gt;Incremental Implementation&lt;/h3&gt;
&lt;p&gt;지금까지 논의한 &lt;strong&gt;action-value method&lt;/strong&gt;는 얻은 rewards의 평균(sample averages)을 내어서 estimate 하였다. 이번에는 이렇게 매번 평균을 내는 것보다 더 효율적인 방법에 대해 알아볼 것이다.&lt;/p&gt;

&lt;p&gt;복잡한 식를 피하기 위해서 하나의 action에 대해서만 다룬다. action-value 식은 아래와 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q_n \doteq \frac{R_1 + R_2 + \dots + R_{n-1}}{n-1}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$R_i$: i번째 action이 선택된 후에 받은 reward&lt;/li&gt;
  &lt;li&gt;$Q_n$: n-1번째 까지 측정된 action value&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;위의 방식은 모든 reward를 보관하고 있다가 value를 estimate할 때마다 계산하는데, 이런 방식은 시간이 지나고 reward를 받을 때마다 더 많은 메모리와 연산을 필요로 한다. 예상할 수 있듯이, 이는 비효율적이기 때문에 평균(average)를 업데이트 하는 다른 방법이 필요하다. 이미 계산된 $Q_n$과 n번째 reward $R_n$이 주어지면 모든 reward의 평균을 아래와 같이 &lt;strong&gt;incremental formulas&lt;/strong&gt;로 계산할 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
Q_n &amp;= \frac{1}{n}\sum_{i=1}^{n}R_i \\
    &amp;= \frac{1}{n}\left(R_n + \sum_{i=1}^{n-1}R_i\right) \\
    &amp;= \frac{1}{n}\left(R_n + (n-1)\frac{1}{n-1}\sum_{i=1}^{n-1}R_i\right) \\
    &amp;= \frac{1}{n}\left(R_n + (n-1)Q_n\right) \\
    &amp;= \frac{1}{n}\left(R_n + nQ_n - Q_n\right) \\
    &amp;= Q_n + \frac{1}{n}\left[R_n - Q_n\right] &amp; (2.3)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;위의 연산은 새로운 reward를 얻더라도 $Q_n$과 $n$ 그리고 (2.3)정도의 간단한 연산만 필요하다. 위의 update 식은 일반적으로 아래와 같이 표기할 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;NewEstimate \leftarrow OldEstimate + StepSize [Target - OldEstimate]&lt;/script&gt;

&lt;p&gt;위 식에서 $[Target - OldEstimate]$가 estimate에서 &lt;strong&gt;error&lt;/strong&gt;이며, $Target$에 가까워질수록 작아진다. target은  우리가 원하는 방향이며 여기서는 n번째 reward다.&lt;/p&gt;

&lt;p&gt;참고로, &lt;strong&gt;incremental method&lt;/strong&gt; (2.3)에서 사용된 step-size parameter(StepSize)는 time step이 지날 수록 $\frac{1}{n}$으로 변경된다. 우리는 앞으로 이 &lt;strong&gt;step-size&lt;/strong&gt;를 $\alpha$ 혹은 $\alpha_t(a)$로 표기할 것이다.&lt;/p&gt;

&lt;p&gt;아래는 incrementally computed sample averages 와 $\varepsilon$-greedy action selection을 사용한 A Simple bandit algorithm의 슈도코드(Pseudocode)다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-01-02-rl-bulti-armed-bandits/pseudocode_1.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;tracking-a-nonstationary-problem&quot;&gt;Tracking a Nonstationary Problem&lt;/h3&gt;

&lt;p&gt;지금까지는 시간이 지나더라도 reward의 probability가 변하지 않는 &lt;strong&gt;stationary&lt;/strong&gt; 상황에서 bandit problems에 대해 알아보았다. 하지만 reinforcement learning에서는 종종 시간이 지남에 따라 reward의 probability가 변하는 &lt;strong&gt;nonstationary&lt;/strong&gt; 상황에 대해서 다뤄야 할 때가 있다. 이런 상황에서는 한참 전에 받은 reward보다 최근에 받은 reward에 좀 더 비중을 두는 것이 합당하다. 우리는 이를 상수 step-size paramenter를 사용해서 구현할 수 있다. 예를 들어서, (2.3)식을 수정한 식은 아래와 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q_{n+1} = Q_n + \alpha\left[R_n - Q_n\right]&lt;/script&gt;

&lt;p&gt;$\alpha$는 $\alpha\in(0,1]$인 상수다. $Q_{n+1}$은 지난 rewards의 weighted average이고 $Q_1$은 초기값이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
Q_n &amp;= Q_n + \alpha\left[R_n - Q_n\right] \\
    &amp;= \alpha R_n  + (1 - \alpha)Q_n\\
    &amp;= \alpha R_n  + (1 - \alpha)[\alpha R_{n-1} + (1 - \alpha)Q_{n-1}] \\
    &amp;= \alpha R_n  + (1 - \alpha)\alpha R_{n-1} + (1 - \alpha)^2 Q_{n-1} \\
    &amp;= \alpha R_n  + (1 - \alpha)\alpha R_{n-1} + (1 - \alpha)\alpha R_{n-2} +
       \dots + (1 - \alpha)^{n-1}\alpha R_1 + (1 - \alpha)^n Q_1\\
    &amp;= (1 - \alpha)^n Q_1 + \sum_{i=1}^{n}\alpha(1 - \alpha)^{n-1}R_i &amp; (2.6)

\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;우리는 위 식을 것은 weighted average라고 부르는데, weight의 총 합이 1이기 때문이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(1 - \alpha)^n + \sum_{i=1}^{n}\alpha(1 - \alpha)^{n-1} = 1&lt;/script&gt;

&lt;p&gt;$R_i$의 가중치 $\alpha(1 - \alpha)^{n-1}$는 이전에 얼마나 많은 reward가 있었느냐에 따라 달라진다. (2.6) 식에서 $1-\alpha$은 1보다 작기 때문에, 전체 reward의 수(n)가 증가 할수록 $R_i$이 값은 작아진다. 여기서 weight은 $1 - \alpha$의 exponent에 의해서 기하급수적으 줄어드는데 이를 &lt;em&gt;exponential recency-weighted average&lt;/em&gt; 라고 한다.&lt;/p&gt;

&lt;div class=&quot;message&quot;&gt;
위 설명이 조금 복잡했는데요, 간단하게 요약하면 위 action-value method에서 $\alpha$ 가 1보다 작기 때문에, 오래전에 받은 reward일 수록 가중치(weight)는 점점 작아지고, 가장 최근에 받은 reward의 가중치(weight)는 더 커진다는 뜻입니다.
&lt;/div&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;optimistic-initial-values&quot;&gt;Optimistic Initial Values&lt;/h3&gt;

&lt;p&gt;지금까지 나온 모든 수식들은 action-value의 초기값 $Q_1(a)$에 영향을 받는다. 통게학에서는 이를 &lt;em&gt;bias&lt;/em&gt;라고 얘기한다. sample-average methods 에서는 모든 action들이 한 번씩 선택되면 이 bias는 모두 사라진다. 하지만 step-size인 $\alpha$가 존재한다면 $Q_1$은 점점 작아지더라도 완전히 사라지지는 않는다.(식 2.6 참고) 이 bias는 종종 매우 유용한데, 예상되는 reward의 값을 미리 설정할 수 있기 때문이다.&lt;/p&gt;

&lt;p&gt;또한, action value를 초기화하는 방법으로 간단하게 &lt;strong&gt;exploration&lt;/strong&gt;을 유도할 수 있다. 예를 들어서, 10-armed bandit testbed 에서 action value의 초기값을 0이 아닌 +5로 설정했다고 가정해보자. $q_*(a)$의 평균은 0이고 분산은 1이다. 그러면 앞으로 action이 어떤 것으로 선택되더라도 update된 reward는 초기값 +5보다 작을 것이기 때문에(식 2.6 참고), 다음에 선택할 greedy action은 항상 시도해보지 않은 action일 것이다. 즉, value estimate가 수렴하기 전까지 &lt;strong&gt;exploration&lt;/strong&gt; 할 것이다.&lt;/p&gt;

&lt;p&gt;아래는 10-armed bandit testbed 에서 $Q_1(a) = +5$ 으로 설정한 greedy method와 $Q_1(a) = 0$ 으로 설정한 $\varepsilon$-greedy method를 비교한 것이다. 초기에는 greedy method($Q_1(a) = +5$)가 더 많은 exploration을 하기 때문에 좋은 나쁜 성능을 보이지만, 시간이 지날 수록 exploration하는 횟수가 줄어들면서 Optial action에 더 가까워지는 것을 볼 수 있다. 이런 테크닉을 &lt;strong&gt;&lt;em&gt;optimistic initial values&lt;/em&gt;&lt;/strong&gt; 라고 한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-01-02-rl-bulti-armed-bandits/figure2_3.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이런 방법은 특정 stationary problems에서만 유용한 simple trick으로, 일방적인 상황에서 exploration 하는 방법으로는 효율적이지 않다. 예를 들어서 nonstationary problems에는 적합하지 않은데, 본질적으로 이런 방법은 일시적으로만 작동하기 때문이다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;upper-confidence-bound-action-selection&quot;&gt;Upper-Confidence-Bound Action Selection&lt;/h3&gt;

&lt;p&gt;action-value estimates는 불확실성을 내재하고 있기 때문에 exploration이 필요하다. greedy action이 현재 시점에서는 가장 좋은 선택일지라도, 장기적인 관점에서 봤을 때 다른 action이 더 좋은 선택일 수 있기 때문이다. 그래서 $\varepsilon$-greedy action selection으로 non-greedy action을 시도하는데, 그 중에서 어떤 action이 더 좋을지에 대한 기준없이 랜덤으로 선택한다. 이런 방법 보다는 그 중에서 가장 optimal이 될 가능성이 높은 action을 선택하는 것이 더 나을 것이다. 아래는 효율적으로 action을 선택하는 방법중 하나다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A_t \doteq \underset{a}{\operatorname{argmax}} \left[ Q_t(a) + c\sqrt{\frac{\operatorname{ln}t}{N_t(a)}} \right]&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$\operatorname{ln}t$: $t$의 자연로그&lt;/li&gt;
  &lt;li&gt;$N_t(a)$: time $t$전에 action $a$가 선택된 횟수&lt;/li&gt;
  &lt;li&gt;$c$: $c &amp;gt; 0$으로 exploration의 빈도를 조절&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;만약 $N_t(a) = 0$ 이면, 이때 $a$는 식을 가장 최대화 하는 action으로 간주된다.&lt;/p&gt;

&lt;p&gt;이런 아이디어를 &lt;strong&gt;&lt;em&gt;upper confidence bound&lt;/em&gt; (UCB)&lt;/strong&gt; action selection 이라고 하는데 제곱근 식($\sqrt{\frac{\operatorname{ln}t}{N_t(a)}}$)은  $a$ value의 불확실한 정도(uncertainty) 혹은 변화하는 정도(variance)를 측정한다. 분모에 있는 $N_t(a)$가 증가하면 불확실한 정도(uncertainty)는 낮아지고, 반면에 다른 action이 선택 되어 $N_t(a)$는 증가하지 않고 $t$ 만 증가한다면 불확실한 정도(uncertainty)는 증가할 것이다. 결국 모든 action이 선택될 것이다. 다만, 측정(estimate)된 value가 낮거나 이전에 자주 선택됐던 action은 시간이 지날수록 낮은 빈도로 선택될 것이다.&lt;/p&gt;

&lt;p&gt;아래는 10-armed testbed에서 &lt;strong&gt;UCB&lt;/strong&gt; 를 사용한 결과다. 보여지는 것처럼 UCB는 잘 작동하지만 더 일반적으로 셋팅된 reinforcement learning 문제에서는 $\varepsilon$-greedy 보다 낮은 성능을 보인다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-01-02-rl-bulti-armed-bandits/figure2_4.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;gradient-bandit-algorithms&quot;&gt;Gradient Bandit Algorithms&lt;/h3&gt;

&lt;p&gt;지금까지는 action을 선택하는데 action value를 사용했다. 이 방법은 매우 좋은 방법이지만 유일한 방법은 아니다. 여기서는 action $a$를 선택하기 위해 각 action의 &lt;strong&gt;preference&lt;/strong&gt;을 사용할 것이며 $H_t(a)$ 으로 표기한다. preference가 큰 action이 더 자주 선택될 것이다. action에 대한 preference는 다음과 같이 &lt;em&gt;softmax distribution&lt;/em&gt;으로 나타낼 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\operatorname{Pr} \left\{ A_t=a \right\} \doteq \frac{e^{H_t(a)}}{\sum^{k}_{b=1}e^{H_t(b)}} \doteq \pi_t(a)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$H_t(a)$: action $a$의 preference&lt;/li&gt;
  &lt;li&gt;$\pi_t(a)$: time $t$ 에서 action $a$를 선택할 확률&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;모든 action의 preference 초기값은 0으로 같기 때문에 (e.g., $H_1(a) = 0$), 처음에 각 action의 선택될 확률은 모두 같다. 각 step에서 action $A_t$가 선택되고 reward $R_t$를 받으면 action preference는 다음 식에 의해 업데이트된다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
H_{t+1}(A_t) &amp;\doteq H_t(A_t) + \alpha(R_t - \bar{R_t})(1 - \pi_t(A_t)) &amp;&amp; \text{and} \\
H_{t+1}(A_t) &amp;\doteq H_t(a) - \alpha(R_t - \bar{R_t})\pi_t(A_t) &amp;&amp; \text{for all } \alpha \neq A_t
\end{align*} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$\alpha$: 0보다 큰 step-size parameter&lt;/li&gt;
  &lt;li&gt;$\bar{R_t}$: time $t$를 포함한 모든 reward의 평균&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;위 식은 &lt;em&gt;Incremental Implementation&lt;/em&gt; 에서 봤던 update 방식으로 계산된다. 그리고, $\bar{R_t}$은 reward의 &lt;strong&gt;baseline&lt;/strong&gt;으로 사용되는데 만약 reward가 baseline보다 높으면 $A_t$가 선택될 확률은 증가하고, 반대로 reward가 basline보다 낮으면 $A_t$가 선택될 확률은 낮아진다. 선택되지 않은 action은 이와 반대로 계산된다.&lt;/p&gt;

&lt;p&gt;아래 Figure 2.5는 분산이 1이고 평균이 4인 정규 분포로부터 reward를 얻는 10-armed testbed에서 테스트한 결과다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-01-02-rl-bulti-armed-bandits/figure2_5.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;message&quot;&gt;
사실 이 Gradient Bandit Algorithms은 정확하게 이해하지 못했습니다. 위의 그래프를 보면 baseline을 사용한 경우 Optimal action에 더 접근하는 것 같습니다.
&lt;/div&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;associative-search-contextual-bandits&quot;&gt;Associative Search (Contextual Bandits)&lt;/h3&gt;
&lt;p&gt;지금까지는 각 다른 상황(situation)에서 다른 action이 필요하지 않은 &lt;em&gt;nonassociative task&lt;/em&gt;에 대해서 다뤘다. task가 stationary할 때는 best action를 찾으면 됐고, task가 nonstationary할 때는 시간에 따라 best action을 추적하면 됐다. 하지만 일반적인 reinforcement learning task에서는 단지 한가지 상황만 존재하지 않는다. 각 상황에 맞는 action을 찾기 위한 &lt;strong&gt;policy&lt;/strong&gt;를 학습해야 한다.&lt;/p&gt;

&lt;p&gt;예를 들어서, 여러개의 다른 k-armed bandit task가 존재하고 매 step마다 그 중 하나를 랜덤으로 선택한다고 가정하자. 그렇다면 bandit task는 매 step마다 랜덤하게 바뀔 것이다. 그러면 마치 true action value가 step마다 변경되는 단 한개의 &lt;strong&gt;nonstationary&lt;/strong&gt; k-armed bandit task 처럼 보일 것이다. 그렇다면 이번 챕터에서 배운 method를 사용할 수 있더라도 제대로 작동하지 않을 것이다. 하지만 만약 각 &lt;em&gt;slot machine&lt;/em&gt;의 색이 다르다면 이와 연관된 &lt;strong&gt;policy&lt;/strong&gt;를 학습할 수 있을 것이다.&lt;/p&gt;

&lt;p&gt;이것을 &lt;em&gt;associative search&lt;/em&gt; task라고 한다. 그리고 k-armed bandit problem 에서는 각 action이 reward에만 영향을 미치는데, 만약 다음 상황(next situation)에도 영향을 미친다면 이것을 &lt;strong&gt;full reinforcement learning&lt;/strong&gt; 라고 한다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;아래는 이번 챕터에서 다룬 각 method를 비교한 결과다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-01-02-rl-bulti-armed-bandits/figure2_5.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;message&quot;&gt;
자세한 설명은 생략했습니다.
&lt;/div&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://incompleteideas.net/book/the-book.html&quot;&gt;Reinforcement Learning: An Introduction - Richard S. Sutton and Andrew G. Barto&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>강화학습 정리 - Introduction</title>
   <link href="http://localhost:4000/reinforcement%20learning/2019/01/01/rl-introduction/"/>
   <updated>2019-01-01T00:00:00+09:00</updated>
   <id>http://localhost:4000/reinforcement%20learning/2019/01/01/rl-introduction</id>
   <content type="html">&lt;h2 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h2&gt;

&lt;p&gt;우리가 배움의 본질에 대해 생각할 때 가장 먼저 떠올리는 것 중 하나는 &lt;strong&gt;환경(environment)&lt;/strong&gt;과 상호작용 하면서 배우는 것이다. 예를 들어서 아기들은 가르쳐주는 사람이 없더라도 스스로 팔을 흔들고, 노는 행동을 하고, 주위를 바라보는 방법 등을 터득한다. 아기는 자신과 연결된 환경(environment)과 상호작용하며 배우는데 필요한 정보를 습득하는 것이다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;reinforcement-learning&quot;&gt;Reinforcement Learning&lt;/h3&gt;

&lt;p&gt;강화학습(Reinforcement Learning)은 현재 자신에게 주어진 환경에서 어떤 &lt;strong&gt;행동(action)&lt;/strong&gt;을 해야 하는지 배우는 것이다. 그리고 이 행동(action)은 &lt;strong&gt;보상(reward)&lt;/strong&gt;을 최대로 얻을 수 있는 방법을 따른다.&lt;/p&gt;

&lt;p&gt;여기서 흥미로운 점은 내가 선택하는 행동(action)이 당장 받을 보상(immediate reward)뿐 아니라 다음 상황(state)과 나중에 받을 보상(subsequent rewards)에도 영향을 미친다는 것이다. 즉, 행동(action)을 선택할 때 나중에 받을 보상도 고려를 해야 하는 것이다.&lt;/p&gt;

&lt;p&gt;또한, 강화학습이 받는 challenge 중 하나는 &lt;strong&gt;exploitation&lt;/strong&gt; 과 &lt;strong&gt;exploration&lt;/strong&gt;의 trade-off 이다. 행동(action)을 선택할 때 두 가지 접근 방법 중에서 하나를 선택해야 하기 때문이다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;exploitation&lt;/strong&gt;: 이미 학습한 정보를 토대로 최대 reward를 얻는 action을 선택한다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;exploration&lt;/strong&gt;: 새로운 정보를 학습하기 위해서 여러가지 action을 선택해본다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;아래는 교재에서 설명된 내용이다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;The agent has to &lt;em&gt;exploit&lt;/em&gt; what it has already experienced in order to obtain reward, but it also has to &lt;em&gt;explore&lt;/em&gt; in order to make better action selections in the future.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;마지막으로, 강화학습에서 &lt;strong&gt;학습하는 대상(agent)&lt;/strong&gt;은 &lt;strong&gt;목표 지향적(goal-directed)&lt;/strong&gt;이며 불확실한 환경(uncertain environment)에서도 모든 문제를 고려해야 한다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;elements-of-reinforcement-learning&quot;&gt;Elements of Reinforcement Learning&lt;/h3&gt;

&lt;p&gt;위에서 설명 했듯이 강화학습에서는 &lt;strong&gt;agent&lt;/strong&gt;는 &lt;strong&gt;environment&lt;/strong&gt;와 상호작용 하면서 학습하는데, 이 시스템에서 중요한 네가지 요소를 아래와 같이 정의할 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Policy&lt;/strong&gt;: 주어진 environment의 state에서 agent가 선택해야 할 action을 정의&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reward signal&lt;/strong&gt;: agent가 action을 선택했을 때 environment로 부터 받는 값&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Value function&lt;/strong&gt;: agent가 특정 state에서 앞으로 받을 모든 rewards의 합에 대한 기댓값을 정의&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Model&lt;/strong&gt;: environment의 동작(behavior)에 대해 정의&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Policy&lt;/strong&gt;는 현재의 내 상태(states of the environment)에서 취해야 할 action을 정의 하는데, 간단한 function이나 lookup table 일 수도 있다. 일반적으로 확률론(stochastic)적 이며 각 action에 대한 확률(probabilities)를 가지고 있다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reward signal&lt;/strong&gt;은 agent가 action을 선택 했을 때 즉각적으로 environment로 부터 받는 보상 값으로, 강화학습에서 agent의 유일한 목표는 이 reward의 총 합을 최대화하는 것이다. reward는 policy를 정의하기 위한 기본 요소다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Value function&lt;/strong&gt;은 길게 보았을 때 agent가 특정 state에서 받을 reward의 총 합에 대한 기댓값을 정의한다. agent가 주어진 state에서 특정 action을 선택했을 때 당장 받을 reward는 다른 action을 취했을 때 보다 상대적으로 낮더라도 최종적으로 누적될 reward는 가장 높을 수도 있다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Model&lt;/strong&gt;은 environment의 동작(behavior)에 대해 정의한다. 즉, model을 알고 있으면 state와 action이 주어졌을 때의 가능한 결과를 미리 예상할 수 있다. model은 agent가 직접 경험해 보기도 전에 &lt;em&gt;계획(planning)&lt;/em&gt;을 세우는데 사용된다. 문제를 해결하기 위해서 model과 planning을 사용하는 방법을 &lt;em&gt;model-based&lt;/em&gt;라고 한다. 반대로 agent가 model을 모르는 상태에서 &lt;em&gt;경험을 하면서(trial-and-error)&lt;/em&gt; 문제를 해결하는 방법을 &lt;em&gt;model-free&lt;/em&gt;라고 한다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;강화학습은 agent가 environment와 상호작용 하면서 학습한다는 점에서 다른 학습 방법과는 구분된다. 그리고 states, actions, rewards를 포함한 environment와 agent의 관계를 정의하기 위해서 &lt;strong&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Markov_decision_process&quot;&gt;Markov decision processes(MDP)&lt;/a&gt;&lt;/strong&gt;라는 framework 를 사용한다. &lt;strong&gt;value&lt;/strong&gt; 와 &lt;strong&gt;value function&lt;/strong&gt; 컨셉은 강화학습에서 매우 중요하다. agent는 policy에 따라서 action을 취하는데, value는 policy를 정의하는데 근간이 되기 때문이다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://incompleteideas.net/book/the-book.html&quot;&gt;Reinforcement Learning: An Introduction - Richard S. Sutton and Andrew G. Barto&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>BOJ - 여행 가자(1976)</title>
   <link href="http://localhost:4000/algorithm/2018/12/29/boj-go-trip-1976/"/>
   <updated>2018-12-29T00:00:00+09:00</updated>
   <id>http://localhost:4000/algorithm/2018/12/29/boj-go-trip-1976</id>
   <content type="html">&lt;p&gt;경로를 찾는 문제처럼 보이지만, &lt;strong&gt;유니온 파인드(Union-Find)&lt;/strong&gt;를 사용해서 해결할 수 있는 간단한 문제입니다. 입력으로 들어온 각 두 도시를 연결 하고, 마지막 입력으로 주어진 도시들이 모두 연결되어 있는지 확인하는 문제입니다. 주어진 도시들의 root 가 모두 같으면 하나의 그래프로 연결되어 있다고 볼 수 있습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;[A-B], [B-C], [A-D] 이면 A, B, C, D 는 모두 하나의 그래프로 이루어져 있습니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;유니온-파인드&quot;&gt;유니온 파인드&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;#define MAX 
int parent[201];
int find(int n) {
	if (parent[n] == 0) return n;
	else {
		int p = find(parent[n]);
		parent[n] = p;
		return p;
	}
}

void merge(int a, int b) {
	int ap = find(a);
	int bp = find(b);
	if (ap == bp) return;
	parent[bp] = ap;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;메인-함수&quot;&gt;메인 함수&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;#include &amp;lt;stdio.h&amp;gt;
int main() {

	int N, M;
	scanf(&quot;%d %d&quot;, &amp;amp;N, &amp;amp;M);

	// input
	int isConn;
	for (int a = 1; a &amp;lt;= N; a++) {
		for (int b = 1; b &amp;lt;= N; b++) {
			scanf(&quot;%d&quot;, &amp;amp;isConn);
			if (isConn == 1) {
				merge(a, b);
			}
		}
	}

	// check if root same
	int n, np, prevnp, isYES;
	for (int i = 0; i &amp;lt; M; i++) {
		scanf(&quot;%d&quot;, &amp;amp;n);
		// find root
		np = find(n);
		if (i &amp;gt; 0) {
			// check if roots same
			isYES = (prevnp == np);
			if (isYES == 0)
				break;
		}
		prevnp = np;
	}

	
	// output
	if (isYES) {
		printf(&quot;YES\n&quot;);
	} else {
		printf(&quot;NO\n&quot;);
	}
	return 0;
}
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>주사위 프로젝트 - View 및 Dialog 작성</title>
   <link href="http://localhost:4000/bixby/2018/12/28/basic_tutorial_4/"/>
   <updated>2018-12-28T00:00:00+09:00</updated>
   <id>http://localhost:4000/bixby/2018/12/28/basic_tutorial_4</id>
   <content type="html">&lt;h2 id=&quot;view-작성&quot;&gt;View 작성&lt;/h2&gt;

&lt;p&gt;안녕하세요! 이전 포스트에서는 View를 작성하지 않아 테이블 형태로 결과를 보여주었는데요, 이번 포스트에서는 View를 작성해서 사용자에게 결과 화면을 더 이쁘게(?!) 보여주겠습니다. 자 그럼 아래와 같이 파일을 생성하고 코드를 작성합니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;/resources/ko/RollResult.view.bxb&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;result-view {
 match {
   RollResult (rollResult)
 }

 render {
   layout {
     section {
       content {
         single-line {
           text {
             style (Detail_M)
             value (&quot;Sum: #{value(rollResult.sum)}&quot;)
           }
         }
         single-line {
           text {
             style (Detail_M)
             value (&quot;Rolls: #{value(rollResult.roll)}&quot;)
           }
         }
       }
     }
   }
 }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;div class=&quot;message&quot;&gt;
'ko' 폴더는 단말에서 빅스비 언어를 한국어로 설정했을 때 반영됩니다. 현재는 전체 코드를 나눠놨지만 전체 구조는 공유하고 단어나 문장이 출력되는 부분만 template으로 따로 빼서 Localization을 할 수 있습니다.
&lt;/div&gt;

&lt;p&gt;위 코드에서 &lt;strong&gt;match&lt;/strong&gt; 부분은 action의 output과 연결됩니다. 코드를 풀이해보면 어떤 action의 output이 RollResult라면 현재 view에서 보여주겠다는 뜻입니다. 자세한 &lt;a href=&quot;https://bixbydevelopers.com/dev/docs/dev-guide/developers/customizing-plan.match-patterns&quot;&gt;&lt;strong&gt;match patterns&lt;/strong&gt;&lt;/a&gt; 는 공식 홈페이지에서 확인하실 수 있습니다.&lt;/p&gt;

&lt;p&gt;그리고 아래 &lt;strong&gt;render&lt;/strong&gt;에 화면에 표시해주는 코드를 작성합니다. 화면에 따라서 용도에 맞는 component를 사용해서 작성하실 수 있습니다. 지금은 깊게 다루지 않기 때문에 전체 흐름만 파악하시면 될 것 같습니다. 자세한 내용은 공식 홈페이지의 &lt;a href=&quot;https://bixbydevelopers.com/dev/docs/dev-guide/developers/building-views&quot;&gt;&lt;strong&gt;Building Views&lt;/strong&gt;&lt;/a&gt; 를 참고 부탁드립니다.&lt;/p&gt;

&lt;p&gt;코드 중간에 보면 &lt;strong&gt;#{value(rollResult.sum)}&lt;/strong&gt;와 같이 &lt;strong&gt;#{}&lt;/strong&gt;로 작성된 부분을 볼 수 있는데요. 이 부분은 빅스비의 &lt;a href=&quot;https://bixbydevelopers.com/dev/docs/dev-guide/developers/customizing-plan.using-el&quot;&gt;&lt;strong&gt;Expression Language&lt;/strong&gt;&lt;/a&gt;로 다양한 연산자를 View의 String 내부에서 사용할 수 있게 도와줍니다.&lt;/p&gt;

&lt;p&gt;이제 아래와 같이 intent를 보내서 화면에 어떻게 나오는지 확인할 수 있습니다.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;intent {
  goal: RollResult
  value: NumSides (6)
  value: NumDice (2)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;다음은 결과 화면입니다.
&lt;img src=&quot;/assets/2018-12-28-basic_tutorial_4/result.png&quot; alt=&quot;image&quot; width=&quot;50%&quot; height=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;현재는 기본 Dialog로 &lt;strong&gt;&lt;em&gt;“검색 완료! 확인해보세요.”&lt;/em&gt;&lt;/strong&gt; 라고 나옵니다.&lt;/p&gt;

&lt;h2 id=&quot;dialog-작성&quot;&gt;Dialog 작성&lt;/h2&gt;

&lt;p&gt;이제 기본 Dialog를 변경해서 원하는 문장을 출력해봅시다. 결과물을 보여주는 화면을 만들기 전에 아래와 같이 Dialog 파일을 생성하고 코드를 작성합니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;/resources/ko/NumSides.dialog.bxb&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;dialog (Concept) {
 match {
   // Look for this type
   NumSides
 }
 // Use the following template text with this type
 template(&quot;면의 수&quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;/resources/ko/NumDice.dialog.bxb&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;dialog (Concept) {
 match {
   // Look for this type
   NumDice
 }
 // Use this template text with this type
 template(&quot;주사위 수&quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;참고로, 위의 두 dialog 모드(mode) concept입니다. 이 모드는 &lt;a href=&quot;https://bixbydevelopers.com/dev/docs/reference/ref-topics/dialog-modes.dialog-fragments#concept-fragment&quot;&gt;Dialog Fragment&lt;/a&gt;중 하나로, 필요한 concept을 intent에 넣지 않고 보냈을 경우 아래와 같이 응답을 생성하는데 사용됩니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;계속하려면 &lt;strong&gt;주사위 수&lt;/strong&gt;가 필요해요&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;다음과 같이 NumDice를 제외한채 intent를 보내다보면 확인하실 수 있습니다.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;intent {
  goal: RollResult
  value: NumSides (6)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;자, 이제 최종 결과물을 보여주는 Dialog를 작성합시다. 아래와 같이 파일을 만들면 됩니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;/resources/ko/RollResult.dialog.bxb&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;dialog (Result) {
 // bind the variable &quot;rollResult&quot; to the result and &quot;rollOutput&quot; to
 // the action of which it was output
 match {
   RollResult (rollResult) {
     from-output: RollDice (rollOutput)
   }
 }
 // define a condition that changes the dialog depending on whether there
 // is one or more dice
 if (rollOutput.numDice == 1) {
   template (&quot;주사위의 번호는 ${value(rollResult.roll)}입니다!&quot;)    
 }
 if (rollOutput.numDice &amp;gt; 1) {
   choose (Random) {
     template (&quot;주사위의 총 합은 ${value(rollResult.sum)}입니다.&quot;)
     template (&quot;각 주사위의 번호는 ${list(rollResult.roll, 'value')}입니다.&quot;)    
   }
 }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;전체적으로 View코드와 유사한데요, 조금 다른 점이 있습니다. 바로 &lt;strong&gt;&lt;em&gt;from-output&lt;/em&gt;&lt;/strong&gt; 인데요. RollDice의 output인 RollResult와 매칭을 시키겠다는 뜻입니다. 사실 이 프로젝트에서는 action이 유일하기 때문에 view와 매칭은 되는데요, 아래 보면 RollDice의 변수로 지정한 &lt;strong&gt;rollOutput&lt;/strong&gt;을 이용하는 코드가 존재하기 때문에 유지해야합니다. rollOutput을 을 이용해서 RollDice의 input 값에 접근할 수 있습니다. &lt;strong&gt;choose(Random)&lt;/strong&gt; 는 template 중 한가지를 임의 선택합니다.&lt;/p&gt;

&lt;div class=&quot;message&quot;&gt;
같은 질문에 매번 같은 대답만 한다면 인공지능처럼 느껴지지 않을 것입니다. 또한 사용자 입장에서 재미도 없을 겁니다. 그래서 다양한 응답 varidation을 주기 위해서 choose(Random)은 자주 사용하는 키워드입니다.
&lt;/div&gt;

&lt;p&gt;이제 다시 아래 intent를 보내면 최종 결과물을 확인할 수 있습니다.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;intent {
  goal: RollResult
  value: NumSides (6)
  value: NumDice (2)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;다음은 View와 Dialog가 적용된 화면입니다.
&lt;img src=&quot;/assets/2018-12-28-basic_tutorial_4/result2.png&quot; alt=&quot;image&quot; width=&quot;50%&quot; height=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;여기까지 진행 하셨으면 Capsule의 기본적인 부분은 완성되었습니다. 다음 포스트에서는 Default Value 넣는 방법에 대해 다루겠습니다. 감사합니다.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>BOJ - 네트워크 연결(3789)</title>
   <link href="http://localhost:4000/algorithm/2018/12/27/boj-corporative-network-3780/"/>
   <updated>2018-12-27T00:00:00+09:00</updated>
   <id>http://localhost:4000/algorithm/2018/12/27/boj-corporative-network-3780</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;Disjoint Set&lt;/strong&gt; 문제이며 &lt;strong&gt;유니온 파인드(Union-Find)&lt;/strong&gt;를 사용해서 해결할 수 있는 문제입니다. 중요한 부분은 find를 할 때 기존 경로 압축(path compression)을 하면서 거리도 업데이트 해줘야 합니다. merge 할 때는 최상위 노드를 따로 찾을 필요 없이 입력으로 들어온 두 노드를 바로 연결 해주면 됩니다.&lt;/p&gt;

&lt;div class=&quot;message&quot;&gt;
경로 압축(path compression)을 하지 않고, 매번 재귀로 거리를 계산하면 Timeout이 발생합니다.
&lt;/div&gt;

&lt;h2 id=&quot;라인-길이&quot;&gt;라인 길이&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;// define
#define MAX 200010

// distance to center
int dist[MAX];

// get distance
int distance(int a, int b) {
	int dist = a &amp;gt; b ? a - b : b - a;
	return dist % 1000;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;유니온-파인드&quot;&gt;유니온 파인드&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;int parent[MAX];

int find(int n) {
	if (parent[n] == 0) return n;
	int p = find(parent[n]);

	//// 길이 업데이트 ////
	dist[n] += dist[parent[n]];
	
	parent[n] = p;
	return p;
	
}

void merge(int a, int b) {
	dist[a] = distance(a, b);
	parent[a] = b;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;메인-함수&quot;&gt;메인 함수&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;#include &amp;lt;stdio.h&amp;gt;
int main() {

	int T;
	scanf(&quot;%d&quot;, &amp;amp;T);

	while(T--) {
		// initialize
		for (int i = 0; i &amp;lt; MAX; i++) {
			dist[i] = 0;
			parent[i] = 0;
		}

		int N;
		scanf(&quot;%d&quot;, &amp;amp;N);
		while(true) {

			char command;
			scanf(&quot; %c&quot;, &amp;amp;command);
			if (command == 'O') {
				break;
			}

			if (command == 'E') {
				int i;
				scanf(&quot;%d&quot;, &amp;amp;i);
				find(i);
				printf(&quot;%d\n&quot;, dist[i]);
			} else {
				int i, j;
				scanf(&quot;%d %d&quot;, &amp;amp;i, &amp;amp;j);
				merge(i, j);
			};
		}
	}
	return 0;
}
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>주사위 프로젝트 - Capsule 테스트</title>
   <link href="http://localhost:4000/bixby/2018/12/23/basic_tutorial_3/"/>
   <updated>2018-12-23T00:00:00+09:00</updated>
   <id>http://localhost:4000/bixby/2018/12/23/basic_tutorial_3</id>
   <content type="html">&lt;h2 id=&quot;capsule-테스트&quot;&gt;Capsule 테스트&lt;/h2&gt;

&lt;p&gt;이제 아래와 같은 순서로 실행해볼 수 있습니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;View &amp;gt; Open Simulator&lt;/strong&gt; 로 가서 Simulator를 실행합니다.&lt;/li&gt;
  &lt;li&gt;팝업창이 뜨면 &lt;strong&gt;Confirm&lt;/strong&gt;을 눌러서 컴파일 하고 학습된 데이터를 준비합니다.&lt;/li&gt;
  &lt;li&gt;아직 자연어 학습을 하지 않았기 때문에 아래의 intent를 직접 입력합니다.
    &lt;pre&gt;&lt;code&gt;intent {
  goal: RollResult
  value: NumSides (6)
  value: NumDice (2)
}
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Run NL&lt;/strong&gt; 을 선택하여 실행합니다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;간단하게 설명드리면, 사용자는 RollResult를 얻기를 원하며, NumSides값을 6으로, NumDice값을 2로 빅스비에게 알려준 것입니다.&lt;/p&gt;

&lt;div class=&quot;message&quot;&gt;
자연어를 학습시키면 &quot;면이 6개인 주사위 2개를 굴려줘!&quot; 라는 발화가 위의 intent로 변환되어 실행됩니다. 이 부분은 맨 마지막에 다룰 예정입니다.
&lt;/div&gt;

&lt;p&gt;지금까지 잘 따라오셨다면 아래와 같은 화면을 보실 수 있을 겁니다. 현재는 따로 화면(Moment)을 만들지 않았기 때문에 리턴된 output 데이터를 테이블형식으로 보여줍니다.&lt;/p&gt;

&lt;div class=&quot;message&quot;&gt;
빅스비에서는 사용자에게 보여주는 응답, 즉 화면 자체를 Moment라고 지칭합니다.
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2018-12-23-basic_tutorial_3/screenshot02.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 빅스비가 실행 그래프(execution graph)가 어떻게 만들었는지 확인해봅시다. simulator에서 왼쪽 위에 Reset 버튼 옆에 있는 두개의 버튼 중 &lt;strong&gt;Debug&lt;/strong&gt; 버튼을 눌러서 Debug Console을 실행하면 아래와 같은 화면을 보실 수 있을겁니다.&lt;/p&gt;

&lt;!-- ![image](){: width=&quot;50%&quot; height=&quot;50%&quot;} --&gt;
&lt;p&gt;&lt;img src=&quot;/assets/2018-12-23-basic_tutorial_3/screenshot03.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 그래프를 보면 intent에서 두 개의 value 값이 concept으로 맵핑되고 goal인 RollResult를 얻기 위해서 RollDice로 연결되었습니다. 그리고 그래프가 실행되면서 RollDice Action에 도달했을 때는 endpoint에서 연결 된 RollDice.js 를 실행시켜 최종 아웃풋인 RollResult를 결과로 얻습니다.&lt;/p&gt;

&lt;p&gt;다음 포스트에서는 RollResult를 화면에 보여주는 방법에 대해서 정리하겠습니다.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>BOJ - 친구 네트워크(4195)</title>
   <link href="http://localhost:4000/algorithm/2018/12/23/boj-friend-network-4195/"/>
   <updated>2018-12-23T00:00:00+09:00</updated>
   <id>http://localhost:4000/algorithm/2018/12/23/boj-friend-network-4195</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;유니온 파인드(Union-Find)&lt;/strong&gt;를 사용해서 해결할 수 있는 문제입니다. 그런데 find 과정에서 단순하게 문자열 비교를 하면 Timeout이 발생합니다. 이를 해결하기 위해서 문자열을 &lt;strong&gt;hash code&lt;/strong&gt;로 변환해서 비교해야 합니다. &lt;strong&gt;HashMap&lt;/strong&gt;을 만들 때 hash code가 겹칠 수 있기 때문에 &lt;strong&gt;Linked List&lt;/strong&gt;를 사용했습니다. 이런 방식을 &lt;a href=&quot;https://en.wikipedia.org/wiki/Hash_table#Separate_chaining&quot;&gt;&lt;strong&gt;Separate chaining&lt;/strong&gt;&lt;/a&gt;이라고 합니다. 기존의 HashMap 라이브러리를 사용하셔도 됩니다.&lt;/p&gt;

&lt;p&gt;hash code 구하는 식은 &lt;a href=&quot;https://d2.naver.com/helloworld/831311&quot;&gt;Naver D2&lt;/a&gt;를 참고했습니다.&lt;/p&gt;

&lt;div class=&quot;message&quot;&gt;
문자열을 배열로 받으면 주소가 고정되어서, Node에 저장된 name이 다음 입력을 받을 때 변경되는 문제가 있었습니다. 이 문제를 해결하고자 문자열을 동적 할당해서 매번 다른 주소값에 맵핑 되도록 수정했습니다. 문자열을 카피해서 해결하는 방법도 있습니다.
&lt;/div&gt;

&lt;h2 id=&quot;노드&quot;&gt;노드&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;// define
#define MAX 200010

// node
struct Node {
	Node* next;
	int idx;
	char* name;
}nodearr[MAX];

int nodeidx = 0;
Node* allocnode(char* name, int idx) {
	Node* node = &amp;amp;nodearr[nodeidx++];
	node-&amp;gt;name = name;
	node-&amp;gt;idx = idx;
	return node;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;hash-code&quot;&gt;Hash Code&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;int hashCode(char* str) {
	int hash = 0;
	for (int i = 0; str[i] != '\0'; i++) {
		hash = (hash * 31 + str[i]) % MAX;
	}
	return hash;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;hash-map&quot;&gt;Hash Map&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;Node* h[MAX];

int get(char* name) {
	int hash = hashCode(name);

	Node* cur = h[hash];
	while (cur != '\0') {
		int isSame = 1;
		for (int i = 0; i &amp;lt; 21; i++) {
			if (name[i] != cur-&amp;gt;name[i]) {
				isSame = 0;
				break;
			}
		}
		if (isSame) return cur-&amp;gt;idx;
		cur = cur-&amp;gt;next;
	}
}

void put(char* name, int idx) {
	int hash = hashCode(name);
	Node* node = allocnode(name, idx);
	if (h[hash] == '\0') h[hash] = node;
	else {
		node-&amp;gt;next = h[hash];
		h[hash] = node;
	}
}

int isContain(char* name) {
	int hash = hashCode(name);
	if (h[hash] == '\0') return 0;
	else {
		Node* cur = h[hash];
		while (cur != '\0') {
			int isSame = 1;
			for (int i = 0; i &amp;lt; 21; i++) {
				if (name[i] != cur-&amp;gt;name[i]) {
					isSame = 0;
					break;
				}
			}
			if (isSame) return 1;
			cur = cur-&amp;gt;next;
		}
		return 0;
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;유니온-파인드&quot;&gt;유니온 파인드&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;int find(int n) {
	if (p[n] &amp;lt; 0) return n;
	else {
		// path compression
		p[n] = find(p[n]);
		return p[n];
	}
}

int uni(int a, int b) {

	int ap = find(a);
	int bp = find(b);

	if (ap != bp) {
		p[bp] = ap;
		c[ap] += c[bp];
	}

	return c[ap];
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;메인-함수&quot;&gt;메인 함수&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;
int main() {

	int T;
	scanf(&quot;%d&quot;, &amp;amp;T);

	while (T--) {
		int nameCount = 1;
		for (int i = 0; i &amp;lt; MAX; i++) {
			p[i] = -1;
			c[i] = 1;
		}

		int F;
		scanf(&quot;%d&quot;, &amp;amp;F);

		while (F--) {

			char* a = (char*)malloc(sizeof(char) * 21);
			char* b = (char*)malloc(sizeof(char) * 21);

			scanf(&quot;%s&quot;, a);
			scanf(&quot;%s&quot;, b);

			if (isContain(a) == 0) {
				put(a, nameCount++);
			}
			if (isContain(b) == 0) {
				put(b, nameCount++);
			}

			int count = uni(get(a), get(b));
			printf(&quot;%d\n&quot;, count);
		}

	}

	return 0;
}
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>주사위 프로젝트 - Concept과 Action 작성</title>
   <link href="http://localhost:4000/bixby/2018/12/22/basic_tutorial_2/"/>
   <updated>2018-12-22T00:00:00+09:00</updated>
   <id>http://localhost:4000/bixby/2018/12/22/basic_tutorial_2</id>
   <content type="html">&lt;h2 id=&quot;concepts과-actions-작성&quot;&gt;Concepts과 Actions 작성&lt;/h2&gt;

&lt;p&gt;이번 포스트에서는 Concept과 Action을 만들어서 모델링하는 과정을 설명드리겠습니다. Concept는 우리가 사용 할 변수라고 보시면 되고, Action은 함수라고 생각하면 조금 더 이해하기 편하실 겁니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Concepts explain what Bixby knows, while actions determine what Bixby can do.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;프로젝트에서는 총 다섯개의 Concept과  한개의 Action을 사용합니다. 각 의미는 아래와 같습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Concept&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;NumDice.model.bxb&lt;/strong&gt; : 주사위의 개수를 의미하는 primitive concept 입니다.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;NumSides.model.bxb&lt;/strong&gt; : 주사위 면의 개수를 의미하는 primitive concept 입니다.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Roll.model.bxb&lt;/strong&gt; : 주사위 하나에서 나온 숫자를 의미하는 primitive concept 입니다.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Sum.model.bxb&lt;/strong&gt; : 모든 주사위에서 나온 숫자의 합을 의미하는 primitive concept 입니다.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;RollResult.model.bxb&lt;/strong&gt; : 최종 결과이며 Roll과 Sum을 포함하는 structure concept 입니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Action&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;RollDice.model.bxb&lt;/strong&gt; : NumDice와 NumSides를 입력으로 받아서 RollResult를 생성하는 action입니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;위에서 primitive는 integer나 text 같은 기본 자료형을 의미합니다. 이제 models 폴더 아래에 concepts 폴더를 생성하고 위에 있는 Concept 파일들을 만들어줍니다. RollResult를 제외한 모든 Concept의 데이터 타입은 모두 integer 입니다. 위에서도 설명드렸듯이 RollResult는 Roll과 Sum을 포함하고 있는 strucutre 타입입니다. 아래는 각 파일 경로와 코드입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;/model/concepts/Numdice.model.bxb&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;integer (NumDice) {
  description (The number of dice.)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;/model/concepts/NumSides.model.bxb&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;integer (NumSides) {
  description (The number of sides to the dice.)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;/model/concepts/Roll.model.bxb&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;integer (Roll) {
  description (The result of a dice roll.)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;/model/concepts/Sum.model.bxb&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;integer (Sum) {
  description (The total sum of all the dice rolled.)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;/model/concepts/RollResult.model.bxb&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;structure (RollResult) {
  description (The result object produced by the RollDice action.)
  property (sum) {
    type (Sum)
    min (Required)
    max (One)
  }
  property (roll) {
    type (Roll)
    min (Required)
    max (Many)
  }      
}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;위의 RollResult에서 roll은 주사위의 개수만큼 있기 때문에 리스트로 받아야 합니다. &lt;strong&gt;max값이 Many이면 값을 리스트로 받습니다.&lt;/strong&gt; Concept을 정의했으니 이제 Action을 만들어보겠습니다. Action은 models 폴더 아래에 actions 폴더를 생성하고 RollDice.model.bxb 파일을 만듭니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;/model/actions/RollDice.model.bxb&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;action (RollDice) {
  type (Calculation)

  collect{
    input (numDice) {
      type (NumDice)
      min (Required)
      max (One)
    }

    input (numSides) {
      type (NumSides)
      min (Required)
      max (One)
    }
  } 

  output (RollResult)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;위의 코드를 보시면 입력(collect)으로는 numDice와 numSides를 각각 하나씩 받고 아웃풋으로는 RollResult가 생성되도록 정의했습니다. type은 선택사항으로 &lt;a href=&quot;https://bixbydevelopers.com/dev/docs/reference/type/action.type&quot;&gt;링크&lt;/a&gt;를 참고하여 넣어주시면 됩니다. 여기까지 기본적인 모델링을 했다고 생각하시면 됩니다. Bixby는 우리가 모델링한 Action과 Concept를 참고해서 실행 될 그래프를 만들어 줄 겁니다.&lt;/p&gt;

&lt;p&gt;이제 RollDice의 함수를 만들어야 합니다. 앞서 만든 RollDice의 Action이 interface라면, 이제 만들 함수는 implementation이라고 생각하시면 됩니다. 코드는 다음과 같습니다. code 폴더 아래에 RollDice.js 함수를 생성합니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;/model/code/RollDice.js&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Main entry point
module.exports.function = function rollDice(numDice, numSides) {

  var sum = 0;
  var result = [];

  // Generate random roll number (1~numSides)
  for (var i = 0; i &amp;lt; numDice; i++) {
    var roll = Math.ceil(Math.random() * numSides);
    result.push(roll);
    sum += roll;
  }

  // RollResult
  return {
    sum: sum, // required Sum
    roll: result // required list Roll
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;위의 함수를 보시면 RollDice.model.bxb 에 정의한 내용과 같게 numDice와 numSides를 입력으로 받고 RollResult를 리턴합니다. 마찬가지로 반환되는 JSON도 RollResult.model.bxb 와 같은 구조입니다.&lt;/p&gt;

&lt;p&gt;이제 Bixby가 실행될 때 Javascript 함수에도 접근할 수 있도록 endpoint를 정의 해주어야 합니다. resources 폴더 아래에 다시 ko 폴더를 만들고 endpoints.bxb 파일을 추가합니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;/resources/ko/endpoints.bxb&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;endpoints {
   authorization {
     none
   }
   action-endpoints {
     action-endpoint (RollDice) {
       accepted-inputs (numDice, numSides)
       local-endpoint (&quot;RollDice.js&quot;)
     }
   }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;여기까지 진행하셨으면, Capsule에 intent를 날리고 실행 그래프가 어떻게 만들어지는지 확인할 수 있습니다. 이 내용은 다음 포스트에서 다루도록 하겠습니다. 감사합니다.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>BOJ - 집합의 표현(1717)</title>
   <link href="http://localhost:4000/algorithm/2018/12/22/boj-union-find-1717/"/>
   <updated>2018-12-22T00:00:00+09:00</updated>
   <id>http://localhost:4000/algorithm/2018/12/22/boj-union-find-1717</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;유니온 파인드(Union-Find)&lt;/strong&gt;를 연습하기 위해 풀어본 문제입니다. 유니온 파인드는 배열을 사용해서 구현했습니다. 같은 집합에 포함되어 있는지 확인할 때는 find() 함수로 root노드가 같은지 확인하고, 합집합을 할 때는 각 트리의 root 노드가 다르다면 하나를 다른 노드의 자식으로 넣어서 트리를 합칩니다.&lt;/p&gt;

&lt;div class=&quot;message&quot;&gt;
단순하게 구현했을 때는 Timeout이 발생했습니다. find를 할 떄 부모를 찾아가는 경로가 길어진 경우 시간이 많이 소비되었기 때문입니다. 이를 해결하기 위해 find() 함수에 p[n] = find(p[n]); 를 넣어서 Root 번호를 바로 찾을 수 있게 해서 시간을 단축했습니다. 이를 경로 압축(path compression)이라고 합니다.
&lt;/div&gt;

&lt;h2 id=&quot;유니온-파인드&quot;&gt;유니온 파인드&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;#define MAX 1000001
int p[MAX];

int find(int n) {
	if (p[n] &amp;lt; 0) return n;
	else {
		//경로 압축(path compression)
		p[n] = find(p[n]);
		return p[n];
	}
}

void uni(int a, int b) {
	int ap = find(a);
	int bp = find(b);
	if (ap != bp) {
		p[bp] = ap;
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;메인-함수&quot;&gt;메인 함수&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;#include &amp;lt;stdio.h&amp;gt;

int main() {

	// init
	for (int i = 0; i &amp;lt; MAX; i++) {
		p[i] = -1;
	};

	int n, m;
	scanf(&quot;%d %d&quot;, &amp;amp;n, &amp;amp;m);

	while(m--) {
		int op, a, b;
		scanf(&quot;%d %d %d&quot;, &amp;amp;op, &amp;amp;a, &amp;amp;b);

		if (op == 0) {
			uni(a, b);
		} else {
			int ap = find(a);
			int bp = find(b);

			if (ap == bp) printf(&quot;YES\n&quot;);
			else printf(&quot;NO\n&quot;);
		}
	}

	return 0;
}
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>주사위 프로젝트 - 프로젝트 생성</title>
   <link href="http://localhost:4000/bixby/2018/12/21/basic_tutorial_1/"/>
   <updated>2018-12-21T00:00:00+09:00</updated>
   <id>http://localhost:4000/bixby/2018/12/21/basic_tutorial_1</id>
   <content type="html">&lt;p&gt;안녕하세요. 이번에는 빅스비 개발자 홈페이지에 나와있는 &lt;a href=&quot;https://bixbydevelopers.com/dev/docs/get-started/quick-start&quot;&gt;Dice 프로젝트&lt;/a&gt;를 만들어 보려고 합니다. 영어가 편하신분은 공식 사이트에서 보시는 것을 추천드립니다. Dice프로젝트는 사용자가 아래와 같이 말하면 그 결과를 보여주는 간단한 캡슐입니다. 아래 예제에서 숫자 4 와 2는 1~6 중에서 랜덤으로 나온 숫자입니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;사용자: “&lt;strong&gt;면이 6개&lt;/strong&gt;인 &lt;strong&gt;주사위 2개&lt;/strong&gt;를 굴려줘!”&lt;br /&gt;
빅스비: “4 그리고 2 가 나왔습니다. 총 합은 6입니다.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;프로젝트-생성&quot;&gt;프로젝트 생성&lt;/h2&gt;

&lt;p&gt;그럼 프로젝트를 만들어 보겠습니다. IDE는 &lt;a href=&quot;https://bixbydevelopers.com/&quot;&gt;공식 홈페이지&lt;/a&gt;에서 자신이 사용하는 OS에 맞게 &lt;a href=&quot;https://bixbydevelopers.com/&quot;&gt;다운&lt;/a&gt; 받으시면 됩니다. IDE를 설치하시고 아래와 같은 순서로 진행합니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Fie-&amp;gt;New Capsule&lt;/strong&gt;을 선택하여 프로젝트를 생성합니다.&lt;/li&gt;
  &lt;li&gt;프로젝트를 생성할 폴더를 선택합니다.&lt;/li&gt;
  &lt;li&gt;Capsule ID를 입력합니다. 저는 &lt;strong&gt;playground.dice&lt;/strong&gt;로 생성했습니다.&lt;/li&gt;
  &lt;li&gt;마지막으로 &lt;strong&gt;Create&lt;/strong&gt; 버튼을 누르시면 기본 구조로 프로젝트가 생성됩니다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2018-12-21-basic_tutorial_1/screenshot01.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;처음 프로젝트를 생성하면 code와 resources폴더가 생깁니다. code폴더에는 Javascript 코드가 들어가는데 서버와 api 통신이나, 연산과 같은 각 Action이 실행될 때 필요한 로직이 들어갑니다. 그리고 resource폴더에는 언어별로 필요한 학습 데이터와 화면을 구성하는데 필요한 파일들이 들어갑니다.&lt;/p&gt;

&lt;div class=&quot;message&quot;&gt;
캡슐에서는 사용자에게 보여지는 하나의 화면을 Moment라고 하며 여기에는 Dialog, View 그리고 Follow-Ups이 포함됩니다.
&lt;/div&gt;
&lt;p&gt;참고: &lt;a href=&quot;https://bixbydevelopers.com/dev/docs/dev-guide/design-guides/service&quot;&gt;Designing Your Capsule&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;그리고 &lt;strong&gt;capsule.bxb&lt;/strong&gt; 파일을 보시면 아래와 같이 나와있는데요, 캡슐의 기본 설정 값을 입력한다고 생각하시면 됩니다. 저희는 한국어를 기본으로 할 것이기 때문에 &lt;strong&gt;targets&lt;/strong&gt; 값을 &lt;strong&gt;bixby-mobile-ko-KR&lt;/strong&gt; 로 변경합니다.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;capsule {
  id (study.dice)
  version (0.1.0)
  format (3)
  targets {
    target (bixby-mobile-ko-KR)
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;다음에는 Concept과 Actions를 추가하여 모델링을 해보겠습니다.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>BOJ - 에디터(1406)</title>
   <link href="http://localhost:4000/algorithm/2018/12/18/boj-editor-1406/"/>
   <updated>2018-12-18T00:00:00+09:00</updated>
   <id>http://localhost:4000/algorithm/2018/12/18/boj-editor-1406</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;이중 링크드 리스트(Double Linked List)&lt;/strong&gt;이용하여 해결할 수 있는 문제입니다. 각 노드를 ‘커서’가 가리키고 있는 위치라고 한다면, 노드 안에 데이터는 ‘커서’ 바로 뒤의 문자입니다. 예를 들어서 문자열 ‘ABC’가 있으면 첫 번째 노드는 맨 앞 ‘커서’이고 가지고 있는 데이터는 ‘A’입니다. 즉, 각 노드는 ‘첫 번째 커서[A]’, ‘두 번째 커서[B]’, ‘세 번째 커서[C]’ 를 의미합니다. 그런데 ‘커서’가 문자열의 맨 뒤에도 위치할 수 있기 때문에 임의의 노드를 하나 더 추가해야 합니다.&lt;/p&gt;

&lt;div class=&quot;message&quot;&gt;
scanf 함수로 문자를 받으니, 다음 scanf 함수에 개행(\n)이 입력으로 들어갔습니다. 이 문제를 해결하기 위해서 scanf로 문자를 받은 다음에 바로 다음에 getchar() 함수를 사용해서 개행(\n)문자를 입력으로 받았습니다.
&lt;/div&gt;

&lt;h2 id=&quot;구조체-배열&quot;&gt;구조체 배열&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;struct Node {
	Node* next;
	Node* prev;
	char data;
} nr[600005];
int nidx = 0;
Node* allocNode(char data) {
	Node* nn = &amp;amp;nr[nidx++];
	nn-&amp;gt;data = data;
	return nn;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;이중-링크드-리스트&quot;&gt;이중 링크드 리스트&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;Node* head = nullptr;
Node* tail = nullptr;

void add(char data) {
	Node* nn = allocNode(data);

	if (head == nullptr) {
		head = nn;
		tail = nn;
	} else {
		tail-&amp;gt;next = nn;
		nn-&amp;gt;prev = tail;
		tail = nn;
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;편집기-기능&quot;&gt;편집기 기능&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;Node* pointer = nullptr;
void addAtPointer(char data) {
	Node* nn = allocNode(data);
	
	if (pointer == head) {
		nn-&amp;gt;next = pointer;
		pointer-&amp;gt;prev = nn;

		head = nn;
	} else {
		nn-&amp;gt;next = pointer;
		nn-&amp;gt;prev = pointer-&amp;gt;prev;
		pointer-&amp;gt;prev-&amp;gt;next = nn;
		pointer-&amp;gt;prev = nn;
	}
}
void del() {
	if (pointer == head) {
		return;
	}
	if (pointer-&amp;gt;prev == head) {
		head = pointer;
		pointer-&amp;gt;prev = nullptr;
	} else {
		pointer-&amp;gt;prev-&amp;gt;prev-&amp;gt;next = pointer;
		pointer-&amp;gt;prev = pointer-&amp;gt;prev-&amp;gt;prev;
	}
}

void left() {
	if (pointer != head) {
		pointer = pointer-&amp;gt;prev;
	}
}

void right() {
	if (pointer != tail) {
		pointer = pointer-&amp;gt;next;
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;메인-함수&quot;&gt;메인 함수&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;int main() {

	// 문자열 입력
	scanf(&quot;%s&quot;, str);
	getchar();

	// 문자열 링크드 리스트에 추가
	int strIdx = 0;
	while (str[strIdx] != '\0') {
		add(str[strIdx]);
		strIdx++;
	}

	// 맨 뒤에 커서 추가
	add('-');

	// 포인터 위치 초기화
	pointer = tail;

	int N;
	scanf(&quot;%d&quot;, &amp;amp;N);
	getchar();

	while (N--) {
		char key;
		scanf(&quot;%c&quot;, &amp;amp;key);
		getchar();
		if (key == 'P') {
			char nc;
			scanf(&quot;%c&quot;, &amp;amp;nc);
			getchar();
			addAtPointer(nc);
		} else if (key == 'L') {
			left();
		} else if (key == 'D') {
			right();
		} else if (key == 'B') {
			del();
		}
	}

	Node* cur = head;
	while (cur-&amp;gt;data != '-') {
		printf(&quot;%c&quot;, cur-&amp;gt;data);
		cur = cur-&amp;gt;next;
	}
}
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>Gettting Started - Overview (2)</title>
   <link href="http://localhost:4000/bixby/2018/12/17/overview_2/"/>
   <updated>2018-12-17T00:00:00+09:00</updated>
   <id>http://localhost:4000/bixby/2018/12/17/overview_2</id>
   <content type="html">&lt;p&gt;이번 포스트에서는 우리가 구현하는 &lt;strong&gt;캡슐(Capsule)&lt;/strong&gt;이 플랫폼에서 어떻게 동작하는지 알아보려고 합니다. 내용은 공식 &lt;a href=&quot;https://bixbydevelopers.com/dev/docs/get-started/overview&quot;&gt;Bixby Developer&lt;/a&gt; 페이지를 참고했습니다.&lt;/p&gt;

&lt;div class=&quot;message&quot;&gt;
빅스비 캡슐(Capsule)은 빅스비 위에서 동작하는 어플리케이션입니다. 스타벅스 캡슐(Startbucks Capsule)이 그중에 하나입니다.
&lt;/div&gt;

&lt;h2 id=&quot;플랫폼과-상호-작용는-캡슐&quot;&gt;플랫폼과 상호 작용는 캡슐&lt;/h2&gt;

&lt;p&gt;개발자는 &lt;strong&gt;‘자연어 학습(Natural Language Traning)’&lt;/strong&gt;, &lt;strong&gt;‘모델링(Concept and Action Models)’&lt;/strong&gt;, &lt;strong&gt;‘함수 작성(API Function)’&lt;/strong&gt; 그리고 사용자에게 보여주는 &lt;strong&gt;화면(Dialog &amp;amp; Layout)&lt;/strong&gt; 을 구현해야 한다고 했는데요, 개발자가 구현한 내용들이 플랫폼에서 동작하는 과정을 알아보려고 합니다. 다음은 사용자 발화가 들어왔을 때 빅스비가 실행되는 Flow입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2018-12-17-overview_2/flow-18218708982895970544.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;첫 번째&lt;/strong&gt;로, 사용자가 발화를 하면 빅스비 플랫폼은 개발자가 학습한 데이터를 기반으로 &lt;strong&gt;intent&lt;/strong&gt;로 변환합니다. intent 다음과 같습니다.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Give two values (number of dice and number of sides) with the
// goal of rolling dice
intent {
 goal: example.dice.RollResult
 value: example.dice.NumSides (6)
 value: example.dice.NumDice (2)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;두 번째&lt;/strong&gt;로, 변환된 intent를 실행시킬 수 있도록 개발자가 정의한 모델인 Concept과 Action을 참고하여 실행될 프로그램(그래프)를 생성합니다.
&lt;!-- ![image](/assets/2018-12-16-overview/1st-execution-graph-11771877356118750247.png) --&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;세 번째&lt;/strong&gt;로, 빅스비가 그래프를 실행하면서 Action에 도달했을 때, 함수(Javascript)가 있다면 같이 실행합니다. 이 함수에는 서버와 통신이나 추가적인 로직들을 넣을 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;마지막&lt;/strong&gt;으로 Dialog와 View을 통해 사용자에게 보여줄 화면을 표시합니다.
&lt;!-- ![image](/assets/2018-12-16-overview/layout-result-16107432446437221263.png) --&gt;&lt;/p&gt;

&lt;p&gt;이 모든 과정들은 빅스비 캡슐(Bixby Capsule)에서 동작합니다. 다음 포스트에서는 예제 프로젝트를 개발해보면서 자세하게 알아보도록 하겠습니다. 감사합니다.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Gettting Started - Overview (1)</title>
   <link href="http://localhost:4000/bixby/2018/12/16/overview_1/"/>
   <updated>2018-12-16T00:00:00+09:00</updated>
   <id>http://localhost:4000/bixby/2018/12/16/overview_1</id>
   <content type="html">&lt;p&gt;빅스비 개발을 시작하기 전에 전체적으로 어떻게 동작하는지 간략하게 소개드리려고 합니다. 내용은 공식 &lt;a href=&quot;https://bixbydevelopers.com/dev/docs/get-started/overview&quot;&gt;Bixby Developer&lt;/a&gt; 페이지를 참고했습니다.&lt;/p&gt;

&lt;h2 id=&quot;무엇을-개발해야-하나요&quot;&gt;무엇을 개발해야 하나요&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;/bixby/2018/12/13/intro/&quot;&gt;이전 포스팅&lt;/a&gt;에서도 설명 드렸듯이 빅스비는 전통적인 프로그래밍 방식과 다른 개발 방식을 보입니다. 개발자는 빅스비가 사용자의 발화를 이해할 수 있도록 IDE에서 자연어를 학습시켜야 합니다. 또한 빅스비가 &lt;strong&gt;실행 그래프&lt;/strong&gt; 를 구성할 수 있게 &lt;strong&gt;모델링&lt;/strong&gt;을 해주고, 필요하다면 서버와 통신과 같은 추가적인 함수를 작성해야 합니다. 마지막으로 사용자에게 보여줄 Dialog와 Layout을 작성합니다.&lt;/p&gt;

&lt;p&gt;다음은 빅스비가 사용자의 발화를 이해하고 그래프를 구성한 예제입니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;“&lt;strong&gt;면이 여섯 개&lt;/strong&gt;인 &lt;strong&gt;두 개&lt;/strong&gt;의 주사위를 굴렸을 때 나온 숫자의 총 합을 구해줘”&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2018-12-16-overview_1/1st-execution-graph-11771877356118750247.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;개발자는 그림과 같이 그래프를 구성하기 위해서 아래의 작업을 해줘야 합니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;사용자 발화에서 NumDice(2)와 NumSides(6)를 추출할 수 있도록 &lt;strong&gt;자연어를 학습&lt;/strong&gt;합니다.&lt;/li&gt;
  &lt;li&gt;그래프가 만들어 질 수 있도록 NumDice, NumSides 그리고 RollDice를 &lt;strong&gt;모델링&lt;/strong&gt;을 합니다.&lt;/li&gt;
  &lt;li&gt;RollDice로 입력된 두 값을 사용하여 RollResult를 계산하는 &lt;strong&gt;함수&lt;/strong&gt;를 넣습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- 
**모델링(Modeling)**은 **Concept**과 **Action**을 정의하는 작업이라고 생각하시면 됩니다. 우리가 보고 있는 예제에서는 NumDice, NumSides, RollResult가 Concept이고 RollDice가 Action입니다. 나중에 조금 더 자세하게 다루겠습니다. 아래는 공식 홈페이지에 나와있는 Concept과 Action에 대한 설명입니다.
&gt;A **concept** describes any &quot;thing.&quot; It could represent a concrete object, such as coffee, flowers, or an airport.

&gt;An **action** defines an operation that Bixby can perform, directly or indirectly, on behalf of a user. If concepts are nouns, actions are verbs. 
--&gt;

&lt;p&gt;다음은 RollDice 함수입니다. 서버와의 통신이나 추가로 필요한 함수는 Javascript로 작성합니다.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// RollDice
// Rolls a dice given a number of sides and a number of dice

// Main entry point
function rollDice(numDice, numSides) {

  var sum = 0;
  var result = [];

  for (var i = 0; i &amp;lt; numDice; i++) {
    var roll = Math.ceil(Math.random() * numSides);
    result.push(roll);
    sum += roll;
  }

  // RollResult
  return {
    sum: sum, // required Sum
    roll: result // required list Roll
  }
}

// Exports
module.exports = {
  function: rollDice
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;그리고 RollResult가 사용자에게 보여지도록 &lt;strong&gt;Dialog&lt;/strong&gt;와 &lt;strong&gt;Layout&lt;/strong&gt;을 작성하면 아래처럼 결과를 볼 수 있습니다. 아래 그림에서 빅스비가 얘기하는 부분(‘Here’s what I found’)이 Dialog이고 추가 정보를 보여주는 부분이 Layout 입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2018-12-16-overview_1/layout-result-16107432446437221263.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번 포스트에서는 설명을 위해서 많은 부분을 생략했는데요, 자연어 학습 및 모델링 하는 부분은 나중에 예제를 프로젝트를 통해서 알아보도록 하겠습니다.
다음 포스트에서는 빅스비 플랫폼이 어떻게 동작하는지 설명드리겠습니다. 감사합니다.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>BOJ - 조세퍼스 문제(1158)</title>
   <link href="http://localhost:4000/algorithm/2018/12/16/boj-josephus-1158/"/>
   <updated>2018-12-16T00:00:00+09:00</updated>
   <id>http://localhost:4000/algorithm/2018/12/16/boj-josephus-1158</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;이중 연결 리스트(Double Linked List)&lt;/strong&gt;를 연습하기 위해 풀어본 문제입니다. 문제를 풀기 위해서 &lt;strong&gt;원형 링크드 리스트(Circular Liked List)&lt;/strong&gt;를 구현해야 하는데 이중 연결 리스트를 이용해서 구현했습니다.&lt;/p&gt;

&lt;h2 id=&quot;구조체-배열&quot;&gt;구조체 배열&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;struct Node {
	Node* next;
	Node* prev;
	int data;
}narr[5005];

int nidx = 0;
Node* allocNode() {
	return &amp;amp;narr[nidx++];
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;원형-링크드-리스트&quot;&gt;원형 링크드 리스트&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;Node* head = NULL;
Node* tail = NULL;

void addNode(int data) {
	Node* nn = allocNode();
	nn-&amp;gt;data = data;
	if (head == NULL) {
		head = nn;
		tail = nn;
		nn-&amp;gt;prev = tail;
		nn-&amp;gt;next = head;
	} else {
		head-&amp;gt;prev = nn;
		tail-&amp;gt;next = nn;
		nn-&amp;gt;prev = tail;
		nn-&amp;gt;next = head;
		tail = nn;
	}
}

int delNode(int idx) {
	Node* cn = head;
	if (head == NULL) return 0;
	while (--idx) {
		cn = cn-&amp;gt;next;
	}
	head = cn-&amp;gt;next;

	cn-&amp;gt;prev-&amp;gt;next = cn-&amp;gt;next;
	cn-&amp;gt;next-&amp;gt;prev = cn-&amp;gt;prev;
	return cn-&amp;gt;data;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;메인-함수&quot;&gt;메인 함수&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;#include &amp;lt;stdio.h&amp;gt;

int main() {
	int N, M;
	scanf(&quot;%d %d&quot;, &amp;amp;N, &amp;amp;M);

	for (int i = 1; i &amp;lt;= N; i++) {
		addNode(i);
	}

	printf(&quot;&amp;lt;&quot;);
	for (int i = 1; i &amp;lt; N; i++) {
		printf(&quot;%d, &quot;, delNode(M));
	}
	printf(&quot;%d&quot;, delNode(M));
	printf(&quot;&amp;gt;&quot;);
}
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>BOJ - 카드 정렬(1715)</title>
   <link href="http://localhost:4000/algorithm/2018/12/15/boj-card-merge-1715/"/>
   <updated>2018-12-15T00:00:00+09:00</updated>
   <id>http://localhost:4000/algorithm/2018/12/15/boj-card-merge-1715</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;최대 힙(MaxHeap)&lt;/strong&gt;을 연습하기 위해 풀어본 문제입니다. 문제를 풀기 위해서는 가지고 있는 카드 묶음 중 가장 작은 두 묶음을 찾아서 더해주어야 했기 때문에 &lt;strong&gt;최소 힙(MinHeap)&lt;/strong&gt;을 사용했습니다. 가장 작은 묶음 두 개를 꺼내서 더해준 후 다시 최소 힙에 넣어줍니다. 힙 안에 한 개만 남을 때까지 반복합니다. 힙만 구현하면 메인 함수는 매우 간단합니다.&lt;/p&gt;

&lt;div class=&quot;message&quot;&gt;
최대 힙에서 최소 힙으로 변경할 때 부등호만 변경해 주었더니 정상적으로 동작하지 않았습니다. 최대 힙에서는 모든 노드를 0으로 초기화 했었는데요, 최소 힙에서는 값이 마지막 자리를 찾아가는 과정에서 문제가 발생했습니다. 이를 해결하기 위해 마지막 노드의 Index 사용하여 비교할 자식 노드가 있는지 확인했습니다.
&lt;/div&gt;

&lt;h2 id=&quot;최소-힙minheap&quot;&gt;최소 힙(MinHeap)&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;// 초기화
int minHeap[200005];
int lastIdx = 0;

// 자식 노드 Index 
int getLeftChild(int idx) {
	return idx * 2;
}

int getRightChild(int idx) {
	return idx * 2 + 1;
}

// 부모 노드 Index 
int getParent(int idx) {
	return idx / 2;
}

// 교환하는 함수
void swap(int idx1, int idx2, int* arr) {
	int tmp = arr[idx1];
	arr[idx1] = arr[idx2];
	arr[idx2] = tmp;
}

// dequeue 함수
int dequeue() {

	if (lastIdx == 0) return -1;

	int maxVal = minHeap[1];
	minHeap[1] = minHeap[lastIdx];
	minHeap[lastIdx] = 0;
	lastIdx--;

	int curIdx = 1;
	while (true) {
		int left = getLeftChild(curIdx);
		int right = getRightChild(curIdx);

		int curVal = minHeap[curIdx];

		int leftVal = minHeap[left];
		int rightVal = minHeap[right];

		int nextIdx;
		int nextVal;

		// 자식 노드가 없는 경우
		if (lastIdx &amp;lt; left) {
			break;

		// 왼쪽 자식 노드만 있는 경우 
		} else if (lastIdx == left) {
			nextIdx = left;
			nextVal = minHeap[nextIdx];

		// 둘 다 있는 경우 비교
		} else if (lastIdx &amp;gt;= right) {
			nextIdx = leftVal &amp;lt; rightVal ? left : right;
			nextVal = minHeap[nextIdx];
		}

		if (curVal &amp;gt; nextVal) {
			swap(curIdx, nextIdx, minHeap);
			curIdx = nextIdx;
		} else {
			break;
		}
	}

	return maxVal;
}

// enqueue 함수
void enqueue(int val) {
	lastIdx++;
	minHeap[lastIdx] = val;

	int curIdx = lastIdx;
	while (true) {
		if (curIdx == 1) break;

		int nextIdx = getParent(curIdx);

		int curVal = minHeap[curIdx];
		int nextVal = minHeap[nextIdx];

		if (curVal &amp;lt; nextVal) {
			swap(curIdx, nextIdx, minHeap);
			curIdx = nextIdx;
		} else {
			break;
		}
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;메인-함수&quot;&gt;메인 함수&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;#include &amp;lt;stdio.h&amp;gt;

int main() {

	int N;
	scanf(&quot;%d&quot;, &amp;amp;N);

	int ans = 0;

	// enqueue
	while (N--) {
		int val;
		scanf(&quot;%d&quot;, &amp;amp;val);
		enqueue(val);
	}

	// 비교 횟수 계산
	while (lastIdx &amp;gt; 1) {
		int val1 = dequeue();
		int val2 = dequeue();
		
		ans += (val1 + val2);

		enqueue(val1 + val2);
	}

	printf(&quot;%d&quot;, ans);

}
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>빅스비 개발 포스팅을 시작하면서</title>
   <link href="http://localhost:4000/bixby/2018/12/13/intro/"/>
   <updated>2018-12-13T00:00:00+09:00</updated>
   <id>http://localhost:4000/bixby/2018/12/13/intro</id>
   <content type="html">&lt;h2 id=&quot;빅스비-개발-포스팅을-시작하면서&quot;&gt;빅스비 개발 포스팅을 시작하면서&lt;/h2&gt;

&lt;p&gt;안녕하세요. 블로그를 시작하면서 어떤 내용을 정리하면 좋을지 고민해봤습니다. 그러다가 현재 개발하고 있으면서, 동시에 자료가 없어서 고생중인 빅스비 개발에 대해 정리하면 좋겠다는 생각을 했습니다. 공부도 하면서 (개발하는 사람은 적더라도)공유하는 공간을 만들면 의미가 있을 것이라고 생각합니다.&lt;/p&gt;

&lt;h2 id=&quot;인공지능--딥러닝&quot;&gt;[인공지능 == 딥러닝]?&lt;/h2&gt;

&lt;p&gt;개인적으로 딥러닝에 관심을 가지며 조금씩 공부하고 있지만 딥러닝이 인공지능의 전부는 아니라고 생각합니다. 이미 많은 사람들이 
&lt;a href=&quot;https://translate.google.co.kr/&quot;&gt;번역&lt;/a&gt;, 
화질 개선, 얼굴 인식 
등 (매우 놀라운)딥러닝 기술을 다양한 곳에서 접하고 있지만 우리가 어릴때부터 상상해왔던 인공지능과는 조금 달라 보입니다. 우리가 상상했던 인공지능은 &lt;a href=&quot;https://www.youtube.com/watch?v=LikxFZZO2sk&quot;&gt;Parkour Atlas&lt;/a&gt;와 같은 &lt;strong&gt;로봇&lt;/strong&gt;이나 &lt;a href=&quot;https://assistant.google.com/&quot;&gt;Google Assistance&lt;/a&gt;, &lt;a href=&quot;https://developer.amazon.com/alexa&quot;&gt;Alexa&lt;/a&gt; 혹은 &lt;a href=&quot;https://kakao.ai/&quot;&gt;카카오 미니&lt;/a&gt;와 같이 사람과 대화를 주고 받는 &lt;strong&gt;음성 인식 비서&lt;/strong&gt;에 더 가깝지 않았나요? 저는 딥러닝과 마찬가지로 사람과 상호작용하는 서비스도 매우 중요하다고 생각합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2018-12-13-intro/robot.jpeg&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;인공지능을-modeling&quot;&gt;인공지능을 Modeling&lt;/h2&gt;

&lt;p&gt;일 년 전 쯤에 회사 동료 한분과 함께 서울대 연구소에서 진행한 ‘NLP 딥러닝’ 세미나를 다녀온 적이 있었는데요, 그 당시 인공지능이라고 말하는 챗봇은 [&lt;em&gt;switch case&lt;/em&gt;] 가 90% 이상이라는 얘기를 들었습니다. 하지만, 요즘 챗봇 Builder 와 마찬가지로 &lt;strong&gt;빅스비도 독립적인 개발 방식&lt;/strong&gt;을 보입니다. 모든 실행 Flow을 개발자가 직접적으로 프로그래밍하여 제어하는 것이 아닌, 개발자는 일부 코드와 &lt;strong&gt;Modeling&lt;/strong&gt;을 제공하면 빅스비가 내부 규칙에 따라서 Flow를 구성해 실행합니다. 사실 이 전통적인 프로그래밍과 다른 방식 때문에 개발하다보면 답답하기도 하고 화나기도 하고 심지어 전체 구조를 뒤집을 때도 있습니다. 하지만 조금씩 적응(이해)하다보면 매력을 느낄 수 있을 것이라고 생각합니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Developing for Bixby is different than traditional software development because you’re not writing the program; the Artificial Intelligence (AI) is. You perform modeling, which is how you teach Bixby about the domain you’re implementing. Using your models, and those of other developers, Bixby constructs a program that satisfies the user’s specific request in milliseconds the moment the request is made. As a developer, your job is to teach Bixby how to write these programs.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;앞으로&quot;&gt;앞으로&lt;/h2&gt;
&lt;p&gt;많이 공부하고, 도움을 드릴 수 있는 기회가 되었으면 좋겠습니다. 감사합니다.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>BOJ - 최대 힙(11279)</title>
   <link href="http://localhost:4000/algorithm/2018/12/11/boj-max-heap-11279/"/>
   <updated>2018-12-11T00:00:00+09:00</updated>
   <id>http://localhost:4000/algorithm/2018/12/11/boj-max-heap-11279</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;배열&lt;/strong&gt;을 사용해서 간단하게 구현한 &lt;strong&gt;최대 힙(MaxHeap)&lt;/strong&gt;입니다. 코드의 길이를 줄이기 보다는 가독성을 높이는 방향으로 작성했습니다.&lt;/p&gt;

&lt;div class=&quot;message&quot;&gt;
힙이 가득찬 상태에서 자식 노드의 위치를 가져올 때 배열의 Index를 넘어서 런타임 에러가 발생했습니다. 따로 로직을 추가해서 처리해도 되지만 메모리가 충분하기 때문에 배열의 크기를 (최대 힙 크기 * 2 + 1) 이상으로 잡았습니다.
&lt;/div&gt;

&lt;h2 id=&quot;최대-힙&quot;&gt;최대 힙&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;// 최대힙 초기화
int maxHeap[200005];
int lastIdx = 0;

// 자식 노드 Index 
int getLeftChild(int idx) {
	return idx * 2;
}

int getRightChild(int idx) {
	return idx * 2 + 1;
}

// 부모 노드 Index 
int getParent(int idx) {
	return idx / 2;
}

// 교환하는 함수
void swap(int idx1, int idx2, int* arr) {
	int tmp = arr[idx1];
	arr[idx1] = arr[idx2];
	arr[idx2] = tmp;
}

// dequeue 함수
int dequeue() {
	
	if (lastIdx == 0) return 0;

	int head = maxHeap[1];
	maxHeap[1] = maxHeap[lastIdx];
	maxHeap[lastIdx] = 0;
	lastIdx--;

	int curIdx = 1;
	while (true) {
		int rIdx = getRightChild(curIdx);
		int lIdx = getLeftChild(curIdx);

		int curVal = maxHeap[curIdx];
		int rVal = maxHeap[rIdx];
		int lVal = maxHeap[lIdx];
		
		int nextIdx = rVal &amp;gt; lVal ? rIdx : lIdx;
		int nextVal = maxHeap[nextIdx];
		if (curVal &amp;lt; nextVal) {
			swap(curIdx, nextIdx, maxHeap);
			curIdx = nextIdx;
		} else {
			break;
		}
	}

	return head;
}

// enqueue 함수
void enqueue(int val) {
	lastIdx++;
	maxHeap[lastIdx] = val;

	int curIdx = lastIdx;
	while (true) {
		if (curIdx == 1) break;

		int pIdx = getParent(curIdx);
		
		int curVal = maxHeap[curIdx];
		int pVal = maxHeap[pIdx];

		int nextIdx = pIdx;
		int nextVal = maxHeap[nextIdx];
		if (curVal &amp;gt; nextVal) {
			swap(curIdx, nextIdx, maxHeap);
			curIdx = nextIdx;
		} else {
			break;
		}
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;메인-함수&quot;&gt;메인 함수&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;#include &amp;lt;stdio.h&amp;gt;

int main() {
	
	int N;
	scanf(&quot;%d&quot;, &amp;amp;N);

	while (N--) {
		int x;
		scanf(&quot;%d&quot;, &amp;amp;x);

		if (x &amp;gt; 0) {
			enqueue(x);
		} else if (x == 0) {
			int val = dequeue();
			printf(&quot;%d\n&quot;, val);
		}
	}

	return 0;
}
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>BOJ - 스택 수열(1874)</title>
   <link href="http://localhost:4000/algorithm/2018/12/10/boj-stack-series-1874/"/>
   <updated>2018-12-10T00:00:00+09:00</updated>
   <id>http://localhost:4000/algorithm/2018/12/10/boj-stack-series-1874</id>
   <content type="html">&lt;p&gt;구조체 배열이 아닌 단순 &lt;strong&gt;배열&lt;/strong&gt;을 사용해서 구현한 스택 수열 알고리즘 코드입니다.&lt;/p&gt;

&lt;h2 id=&quot;배열과-헤드위치-선언&quot;&gt;배열과 헤드위치 선언&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;int stack[100005];
int head = 0;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;전체-코드&quot;&gt;전체 코드&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;#include &amp;lt;stdio.h&amp;gt;

int stack[100005];
int head = 0;

int main() {

	int n;
	scanf(&quot;%d&quot;, &amp;amp;n);

	int inc = 1;

	int outputIdx = 0;
	char output[500005];
	bool isFailed = false;

	while(n--) {

		int number;
		scanf(&quot;%d&quot;, &amp;amp;number);

		// 수열이 형성되지 못하는 경우 체크
		if (head &amp;lt; 0 || number &amp;lt; stack[head]) {
			isFailed = true;
			break;;
		}

		// 출력(POP)되어야 할 숫자(number)가 스택에 들어갈 때까지 PUSH
		while (stack[head] &amp;lt; number) {
			output[outputIdx++] = '+';
			stack[++head] = inc;
			inc++;
		}
		
		// 숫자(number)를 출력(POP)
		while (stack[head] &amp;gt;= number) {
			output[outputIdx++] = '-';
			head--;
		}

	}

	if (isFailed) {
		printf(&quot;NO\n&quot;);
	} else {
		for (int i = 0; i &amp;lt; outputIdx; i++ ) {
			printf(&quot;%c\n&quot;, output[i]);
		}
	}
	return 0;
}
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>BOJ - 스택(10828)</title>
   <link href="http://localhost:4000/algorithm/2018/12/10/boj-stack-10828/"/>
   <updated>2018-12-10T00:00:00+09:00</updated>
   <id>http://localhost:4000/algorithm/2018/12/10/boj-stack-10828</id>
   <content type="html">&lt;p&gt;간단한 &lt;strong&gt;링크드 리스트&lt;/strong&gt;를 이용해서 구현한 &lt;strong&gt;스택&lt;/strong&gt;입니다. 배열을 사용해서 더 간단하게도 구현 가능하지만 &lt;strong&gt;구조체 배열&lt;/strong&gt;을 연습하고자 링크드 리스트를 사용했습니다.&lt;/p&gt;

&lt;h2 id=&quot;구조체-배열&quot;&gt;구조체 배열&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;// 정적이지만 동적할당처럼 사용하는 구조체 배열
typedef struct node {
	int data;
	node* next;
} Node;

int nodeIndex = 0;
Node nodeArray[10000];
Node* allocNode() {
	return &amp;amp;nodeArray[nodeIndex++];
};

// 아래와 같이 사용할 수 있습니다.
Node* node = allocNode();
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;전체-코드&quot;&gt;전체 코드&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;#include &amp;lt;cstring&amp;gt;
#include &amp;lt;stdio.h&amp;gt;

// 구조체 배열
typedef struct node {
	int data;
	node* next;
} Node;

int nodeIndex = 0;
Node nodeArray[10000];
Node* allocNode() {
	return &amp;amp;nodeArray[nodeIndex++];
};

// 메인 함수
int main() {

	int N;
	scanf(&quot;%d&quot;, &amp;amp;N);


	Node* head = '\0';
	char order[10];
	int count = 0;
	while (N--) {
		scanf(&quot;%s&quot;, order);

		if (order[1] == 'u') {
			int data;
			scanf(&quot;%d&quot;, &amp;amp;data);
			Node* node = allocNode();
			node-&amp;gt;data = data;

			if (count == 0) {
				head = node;
			} else {
				node-&amp;gt;next = head;
				head = node;
			}
			count++;
		} else if (order[0] == 'p') {
			if (head == '\0') {
				printf(&quot;%d\n&quot;, -1);
			} else {
				printf(&quot;%d\n&quot;, head-&amp;gt;data);
				head = head-&amp;gt;next;
				count--;
			}
		} else if (order[0] == 's') {
			printf(&quot;%d\n&quot;, count);
		} else if (order[0] == 'e') {
			if (count == 0) {
				printf(&quot;%d\n&quot;, 1);
			} else {
				printf(&quot;%d\n&quot;, 0);
			}
		} else if (order[0] == 't') {
			if (count == 0) {
				printf(&quot;%d\n&quot;, -1);
			} else {
				printf(&quot;%d\n&quot;, head-&amp;gt;data);
			}
		}
	};

	return 0;
}
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>BOJ - 큐(10845)</title>
   <link href="http://localhost:4000/algorithm/2018/12/10/boj-queue-10845/"/>
   <updated>2018-12-10T00:00:00+09:00</updated>
   <id>http://localhost:4000/algorithm/2018/12/10/boj-queue-10845</id>
   <content type="html">&lt;p&gt;&lt;a href=&quot;../backjoon-stack-10828/&quot;&gt;스택(10828)&lt;/a&gt;과 마찬가지로 간단한 &lt;strong&gt;링크드 리스트&lt;/strong&gt;를 이용해서 구현한 &lt;strong&gt;큐&lt;/strong&gt;입니다. 배열을 사용해서 더 간단하게도 구현 가능하지만 &lt;strong&gt;구조체 배열&lt;/strong&gt;을 연습하고자 링크드 리스트를 사용했습니다.&lt;/p&gt;

&lt;h2 id=&quot;구조체-배열&quot;&gt;구조체 배열&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;// 정적이지만 동적할당처럼 사용하는 구조체 배열
typedef struct node {
	int data;
	node* next;
} Node;

int nodeIndex = 0;
Node nodeArray[10000];
Node* allocNode() {
	return &amp;amp;nodeArray[nodeIndex++];
};

// 아래와 같이 사용할 수 있습니다.
Node* node = allocNode();
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;전체-코드&quot;&gt;전체 코드&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;#include &amp;lt;stdio.h&amp;gt;

// 구조체 배열
typedef struct node {
	int data;
	node* next;
} Node;

int nodeidx = 0;
Node nodearray[10000];
Node* allocNode() {
	return &amp;amp;nodearray[nodeidx++];
};

// 메인 함수
int main() {

	int N;
	scanf(&quot;%d&quot;, &amp;amp;N);

	Node* head = '\0';
	Node* tail = '\0';

	char order[6];
	int count = 0;
	while (N--) {
		scanf(&quot;%s&quot;, order);

		// 새로운 노드 생성해서 큐에 PUSH
		if (order[1] == 'u') {
			int data;
			scanf(&quot;%d&quot;, &amp;amp;data);
			Node* node = allocNode();
			node-&amp;gt;data = data;
			if (count == 0) {
				head = node;
				tail = node;
			} else {
				tail-&amp;gt;next = node;
				tail = node;
			}
			count++;

		// 상위 노드(head) POP
		} else if (order[0] == 'p') {
			if (count == 0) printf(&quot;-1\n&quot;);
			else {
				printf(&quot;%d\n&quot;, head-&amp;gt;data);
				head = head-&amp;gt;next;
				count--;
			}

		// 큐에 있는 노드 갯수 출력
		} else if (order[0] == 's') {
			printf(&quot;%d\n&quot;, count);

		// 큐가 비어 있는지 확인
		} else if (order[0] == 'e') {
			if (count == 0) printf(&quot;1\n&quot;);
			else printf(&quot;0\n&quot;);

		// 상위 노드(head) 값 출력
		} else if (order[0] == 'f') {
			if (count == 0) printf(&quot;-1\n&quot;);
			else {
				printf(&quot;%d\n&quot;, head-&amp;gt;data);
			}

		// 마지막 노드(tail) 값 출력
		} else if (order[0] == 'b') {
			if (count == 0) printf(&quot;-1\n&quot;);
			else {
				printf(&quot;%d\n&quot;, tail-&amp;gt;data);
			}
		}
	};
	return 0;
}
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 

</feed>
